{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF4zYMmXULP7"
   },
   "source": [
    "# **CellTracksColab - Track clustering analysis**\n",
    "---\n",
    "<font size = 4>Explore Spatial Clustering in Track Data with CellTracksColab: This Colab Notebook is designed to help you determine whether tracks exhibit spatial clustering. Before beginning, ensure that your data is properly loaded in the CellTracksColab format for optimal analysis.\n",
    "\n",
    "---\n",
    "\n",
    "# **Before getting started**\n",
    "---\n",
    "\n",
    "<font size = 5>**Important notes**\n",
    "\n",
    "---\n",
    "\n",
    "<font size = 5>**Data Requirements for Analysis**\n",
    "\n",
    "<font size = 4>Be advised of one significant limitation inherent to this notebook.\n",
    "\n",
    "<font size = 4 color=\"red\">**This notebook only supports 2D + t datasets**</font>.\n",
    "\n",
    "---\n",
    "<font size = 5>**Prerequisites for Using This Notebook**\n",
    "\n",
    "<font size = 4>To effectively utilize this notebook the following prerequisite is essential:\n",
    "<font size = 4>\n",
    "1. **DataFrames from CellTrackColab**:\n",
    "   - Ensure you have the `spots` and `tracks` DataFrames compiled by CellTrackColab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JrkfFr7mgZmA"
   },
   "outputs": [],
   "source": [
    "# @title #MIT License\n",
    "\n",
    "print(\"\"\"\n",
    "**MIT License**\n",
    "\n",
    "Copyright (c) 2023 Guillaume Jacquemet\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rls134tXHCUL"
   },
   "source": [
    "--------------------------------------------------------\n",
    "# **Part 0. Prepare the Google Colab session**\n",
    "--------------------------------------------------------\n",
    "<font size = 4>skip this section when using a local installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h0prdayn0qG"
   },
   "source": [
    "## **0.1. Install key dependencies**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rAP0ahCzn1V6"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Play to install\n",
    "\n",
    "print(\"In progress....\")\n",
    "\n",
    "!git clone https://github.com/CellMigrationLab/CellTracksColab.git\n",
    "\n",
    "%pip -q install pandas scikit-learn\n",
    "%pip -q install plotly\n",
    "%pip -q install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Kzd_8GUnpbw"
   },
   "source": [
    "## **0.2. Mount your Google Drive**\n",
    "---\n",
    "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
    "\n",
    "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
    "\n",
    "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GA1wCrkoV4i5"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/Gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woCcctRqHCUM"
   },
   "source": [
    "--------------------------------------------------------\n",
    "# **Part 1. Prepare the session and load the data**\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzr5VjpTHCUM"
   },
   "source": [
    "## **1.1 Load key dependencies**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uiuJFVIsXOsl"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Play to load the dependancies\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import requests\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "import gzip\n",
    "\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from ipywidgets import Dropdown, interact,Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial.distance import cosine, pdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import zscore, ks_2samp\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from multiprocessing import Pool\n",
    "from matplotlib.ticker import FixedLocator\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"CellTracksColab/\")\n",
    "\n",
    "import celltracks\n",
    "from celltracks import *\n",
    "from celltracks.Track_Plots import *\n",
    "from celltracks.BoxPlots_Statistics import *\n",
    "from celltracks.Track_Metrics import *\n",
    "from celltracks.Distance_to_ROI import *\n",
    "from celltracks.Track_Clustering import *\n",
    "\n",
    "# Current version of the notebook the user is running\n",
    "current_version = \"1.0.2\"\n",
    "Notebook_name = 'Track_Clustering'\n",
    "\n",
    "# URL to the raw content of the version file in the repository\n",
    "version_url = \"https://raw.githubusercontent.com/guijacquemet/CellTracksColab/main/Notebook/latest_version.txt\"\n",
    "\n",
    "# Function to define colors for formatting messages\n",
    "class bcolors:\n",
    "    WARNING = '\\033[91m'  # Red color for warning messages\n",
    "    ENDC = '\\033[0m'      # Reset color to default\n",
    "\n",
    "# Check if this is the latest version of the notebook\n",
    "try:\n",
    "    All_notebook_versions = pd.read_csv(version_url, dtype=str)\n",
    "    print('Notebook version: ' + current_version)\n",
    "\n",
    "    # Check if 'Version' column exists in the DataFrame\n",
    "    if 'Version' in All_notebook_versions.columns:\n",
    "        Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Notebook_name]['Version'].iloc[0]\n",
    "        print('Latest notebook version: ' + Latest_Notebook_version)\n",
    "\n",
    "        if current_version == Latest_Notebook_version:\n",
    "            print(\"This notebook is up-to-date.\")\n",
    "        else:\n",
    "            print(bcolors.WARNING + \"A new version of this notebook has been released. We recommend that you download it at https://github.com/guijacquemet/CellTracksColab\" + bcolors.ENDC)\n",
    "    else:\n",
    "        print(\"The 'Version' column is not present in the version file.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Unable to fetch the latest version information. Please check your internet connection.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsDAwkSOo1gV"
   },
   "source": [
    "## **1.2. Load existing CellTracksColab dataframes**\n",
    "---\n",
    "\n",
    "<font size = 4> Please ensure that your data was properly processed using CellTracksColab. To use the Viewer Notebook, your data must be formatted in the CellTracksColab format. This involves compiling your tracking data into two main DataFrames:\n",
    "\n",
    "*   Your Track_table: `merged_tracks_df`\n",
    "\n",
    "*   Spot_table: `merged_spots_df`.\n",
    "\n",
    "<font size = 4>**Data_Dims**: Choose \"2D\" or \"3D\" for your data dimensions.\n",
    "\n",
    "<font size = 4>**Results_Folder**: The directory path where the analysis results will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "CQKXq3giI3nX"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Provide the path to your CellTracksColab dataset:\n",
    "\n",
    "Data_Dims = \"2D\" #@param [\"2D\", \"3D\"]\n",
    "Data_Type = \"CellTracksColab\"\n",
    "\n",
    "Track_table = ''  # @param {type: \"string\"}\n",
    "Spot_table = ''  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "Use_test_dataset = False\n",
    "\n",
    "#@markdown ###Provide the path to your Result folder\n",
    "\n",
    "Results_Folder = \"\"  # @param {type: \"string\"}\n",
    "\n",
    "# Update the parameters to load the data\n",
    "CellTracks = celltracks.TrackingData()\n",
    "if Use_test_dataset:\n",
    "    # Download the test dataset\n",
    "    test_celltrackscolab = \"https://zenodo.org/record/8420011/files/T_Cells_spots_only.zip?download=1\"\n",
    "    CellTracks.DownloadTestData(test_celltrackscolab)\n",
    "    File_Format = \"csv\"\n",
    "else:\n",
    "\n",
    "    CellTracks.Spot_table = Spot_table\n",
    "    CellTracks.Track_table = Track_table\n",
    "\n",
    "CellTracks.Results_Folder = Results_Folder\n",
    "CellTracks.skiprows = None\n",
    "CellTracks.data_type = Data_Type\n",
    "CellTracks.data_dims = Data_Dims\n",
    "\n",
    "# Load data\n",
    "CellTracks.LoadTrackingData()\n",
    "\n",
    "merged_spots_df = CellTracks.spots_data\n",
    "check_for_nans(merged_spots_df, \"merged_spots_df\")\n",
    "merged_tracks_df = CellTracks.tracks_data\n",
    "print(\"...Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52STmnv43d45"
   },
   "source": [
    "## **1.3. Visualise your tracks**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AE881uJW5ukQ"
   },
   "outputs": [],
   "source": [
    "# @title ##Run the cell and choose the file you want to inspect\n",
    "display_plots=True\n",
    "\n",
    "if not os.path.exists(Results_Folder+\"/Tracks\"):\n",
    "    os.makedirs(Results_Folder+\"/Tracks\")\n",
    "\n",
    "filenames = merged_spots_df['File_name'].unique()\n",
    "\n",
    "filename_dropdown = widgets.Dropdown(\n",
    "    options=filenames,\n",
    "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
    "    description='File Name:',\n",
    ")\n",
    "\n",
    "interact(lambda filename: plot_track_coordinates(filename, merged_spots_df, Results_Folder, display_plots=display_plots), filename=filename_dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eIekawNM9aS"
   },
   "source": [
    "# **Part 2: Assess spatial clustering using Ripley's L function**\n",
    "\n",
    "<font size = 4>In the specific spatial analysis being performed here, the choice of a single point within each track serves to focus on key moments or characteristics of object movement that are particularly relevant to the research objectives. For instance, when analyzing spatial distribution patterns of tracked objects within each field of view (FOV), different analysis points such as the beginning, end, middle, average, or median point of each track offer unique insights. Selecting the \"beginning\" point might help identify where objects enter an area, while the \"end\" point can indicate exit locations. Choosing the \"middle\" point provides insights into where objects spend a significant portion of their time. On the other hand, the \"average\" or \"median\" point offers a summary of the overall movement tendencies within each track. By accommodating these various analysis point options, researchers can tailor their spatial analysis to uncover specific aspects of object distribution that are most pertinent to their research questions, enhancing the depth and relevance of their findings.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC_pKRUuyn7A"
   },
   "source": [
    "## **2.1. Choose the point to use for each track**\n",
    "\n",
    "<font size = 4>This section offers users an interactive visualization tool to compare and select the most suitable analysis point within each track for spatial analysis. By providing a dynamic interface, users can assess the impact of different analysis points (e.g., \"beginning,\" \"end,\" \"middle,\" etc.) on spatial distribution patterns. This hands-on exploration empowers users to make informed decisions, ensuring that the chosen analysis point effectively captures the spatial characteristics of each track. Ultimately, this customization enhances the precision and relevance of spatial analysis results for a wide range of research objectives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OCcIC7ytzAzW"
   },
   "outputs": [],
   "source": [
    "# @title ##Run the cell and choose the analysis point you want to use\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "base_folder = f\"{Results_Folder}/Track_Clustering/Tracks\"\n",
    "\n",
    "# Check and create necessary directories\n",
    "if not os.path.exists(base_folder):\n",
    "    os.makedirs(base_folder)\n",
    "\n",
    "# Extract unique filenames from the dataframe\n",
    "filenames = merged_spots_df['File_name'].unique()\n",
    "\n",
    "# Create Dropdown widgets with labels and fixed width\n",
    "filename_dropdown = widgets.Dropdown(\n",
    "    options=filenames,\n",
    "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
    "    description='File Name:',\n",
    "    layout=widgets.Layout(width='300px'),  # Adjust width as needed\n",
    ")\n",
    "\n",
    "analysis_option_dropdown = widgets.Dropdown(\n",
    "    options=[\"beginning\", \"end\", \"middle\", \"average\", \"median\"],\n",
    "    value=\"beginning\",\n",
    "    description='Point:',\n",
    "    layout=widgets.Layout(width='300px'),  # Adjust width as needed\n",
    ")\n",
    "\n",
    "# Link both Dropdown widgets to the plotting function\n",
    "interact(lambda filename, analysis_option: plot_coordinates_Clustering(filename, merged_spots_df, analysis_option, base_folder),\n",
    "         filename=filename_dropdown, analysis_option=analysis_option_dropdown);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_Py1TcJwqwB"
   },
   "source": [
    "## **2.2. Compute Ripley's L function for each FOV**\n",
    "\n",
    "<font size = 4>This code aims to compute Ripley's L function for each Field of View (FOV) in a dataset of tracked objects. Ripley's L function is a spatial statistics tool used to analyze the spatial distribution of points or objects in a given area. In this analysis, we are interested in understanding how objects are distributed within each FOV.\n",
    "\n",
    "<font size = 4>**User Input Options**\n",
    "\n",
    "1. **Analysis Option**: This option allows you to choose the point within each track that will be used for analysis. You can select one of the following options:\n",
    "   - \"beginning\": Use the initial position of each track.\n",
    "   - \"end\": Use the final position of each track.\n",
    "   - \"middle\": Use the middle position of each track.\n",
    "   - \"average\": Use the average position of all points within each track.\n",
    "   - \"median\": Use the median position of all points within each track.\n",
    "\n",
    "2. **r_values Range**: Ripley's L function is computed for a range of spatial distances denoted by \"r.\" You can specify the range of r_values using the following parameters:\n",
    "   - **Start Value**: The starting value of \"r\" (minimum distance). This number should be greater than 0.\n",
    "   - **End Value**: The ending value of \"r\" (maximum distance).\n",
    "   - **Number of Points**: The number of points or steps within the specified range. The analysis will be performed at equidistant intervals between the start and end values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "5Fz9ShrlR5F1"
   },
   "outputs": [],
   "source": [
    "# @title ##Run check the settings you want to use to compute Ripley's L function\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define analysis parameters\n",
    "analysis_option = \"middle\" # @param [\"beginning\", \"end\", \"middle\", \"average\", \"median\"]\n",
    "\n",
    "# Prompt the user for the desired r_values range\n",
    "r_values_start = 0.1 # @param {type: \"number\"}\n",
    "r_values_end = 100 # @param {type: \"number\"}\n",
    "r_values_count = 50 # @param {type: \"number\"}\n",
    "\n",
    "r_values = np.linspace(r_values_start, r_values_end, r_values_count)\n",
    "\n",
    "# Get the dimensions with fixed minimum values\n",
    "max_dims = get_dimensions(merged_spots_df)\n",
    "max_x, max_y = max_dims['max_x'], max_dims['max_y']\n",
    "\n",
    "# Check that r_values_end is not larger than the field of view\n",
    "if r_values_end > max_x or r_values_end > max_y:\n",
    "    raise ValueError(\"r_values_end is larger than the field of view dimensions.\")\n",
    "\n",
    "# Initialize a counter for edge points\n",
    "edge_points_count = 0\n",
    "total_points_count = 0\n",
    "fov_count = merged_spots_df['File_name'].nunique()\n",
    "\n",
    "for file_name, group in tqdm(merged_spots_df.groupby('File_name'), desc=\"Testing the parameters\"):\n",
    "    # Sort each track by POSITION_T\n",
    "    group = group.sort_values(by=['TRACK_ID', 'POSITION_T'])\n",
    "\n",
    "    representative_points = group.groupby('TRACK_ID').apply(lambda track: select_analysis_point(track, analysis_option)).dropna()\n",
    "    total_points_count += len(representative_points)\n",
    "\n",
    "    # Identify points too close to the edge of the image\n",
    "    edge_points = representative_points[\n",
    "        (representative_points['POSITION_X'] <= r_values_end) |\n",
    "        (representative_points['POSITION_X'] >= (max_x - r_values_end)) |\n",
    "        (representative_points['POSITION_Y'] <= r_values_end) |\n",
    "        (representative_points['POSITION_Y'] >= (max_y - r_values_end))\n",
    "    ]\n",
    "\n",
    "    # Count edge points\n",
    "    edge_points_count += len(edge_points)\n",
    "\n",
    "print(f\"Total number of points: {total_points_count}\")\n",
    "print(f\"Total number of fields of view: {fov_count}\")\n",
    "print(f\"Total points closer to the edge than r_values_end: {edge_points_count}. These points could contribute to edge effects and impact your analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Y0Ikco9nMopc"
   },
   "outputs": [],
   "source": [
    "# @title ##Run to compute Ripley's L function for each FOV\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check and create necessary directories\n",
    "results_dir = f\"{Results_Folder}/Track_Clustering/RipleyL\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Define area based on your dataset's extent\n",
    "area = (max_x - merged_spots_df['POSITION_X'].min()) * (max_y - merged_spots_df['POSITION_Y'].min())\n",
    "\n",
    "# Compute Ripley's L function for each FOV\n",
    "l_values_per_fov_slow = {}\n",
    "for file_name, group in tqdm(merged_spots_df.groupby('File_name'), desc=\"Processing FOVs\"):\n",
    "    # Sort each track by POSITION_T\n",
    "    group = group.sort_values(by=['TRACK_ID', 'POSITION_T'])\n",
    "\n",
    "    # Select representative points for each track\n",
    "    representative_points = group.groupby('TRACK_ID').apply(lambda track: select_analysis_point(track, analysis_option)).dropna()\n",
    "    l_values = [ripley_l(representative_points[['POSITION_X', 'POSITION_Y']].values, r, area) for r in r_values]\n",
    "    l_values_per_fov_slow[file_name] = l_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlGW9UIqvqh6"
   },
   "source": [
    "## **2.3. Compute Monte Carlo Simulations for Each FOV**\n",
    "\n",
    "This code section performs Monte Carlo simulations to assess the significance of the observed spatial distribution patterns within each Field of View (FOV) in a dataset of tracked objects. The simulations help establish confidence envelopes for Ripley's L function, allowing for statistical testing.\n",
    "\n",
    "\n",
    "**Number of Simulations (Nb_simulation)**: You can specify the number of Monte Carlo simulations to run for each FOV. This parameter determines the level of statistical confidence and computational resources used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "INwTjl0JyM70"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# @title ##Run to compute Monte Carlo simulations for each FOV\n",
    "\n",
    "Nb_simulation = 10 # @param {type: \"number\"}\n",
    "\n",
    "simulated_l_values_dict_slow = {}\n",
    "\n",
    "confidence_envelopes_slow = {}\n",
    "\n",
    "# Compute the area once\n",
    "area = (merged_spots_df['POSITION_X'].max() - merged_spots_df['POSITION_X'].min()) * \\\n",
    "       (merged_spots_df['POSITION_Y'].max() - merged_spots_df['POSITION_Y'].min())\n",
    "\n",
    "for file_name, group in tqdm(merged_spots_df.groupby('File_name'), desc='Processing FOVs'):\n",
    "\n",
    "    group = group.sort_values(by=['TRACK_ID', 'POSITION_T'])\n",
    "    representative_points = group.groupby('TRACK_ID').apply(lambda track: select_analysis_point(track, analysis_option)).dropna()\n",
    "\n",
    "    if not representative_points.empty:\n",
    "        simulations = [\n",
    "            simulate_random_points(\n",
    "                len(representative_points),\n",
    "                (merged_spots_df['POSITION_X'].min(), merged_spots_df['POSITION_X'].max()),\n",
    "                (merged_spots_df['POSITION_Y'].min(), merged_spots_df['POSITION_Y'].max())\n",
    "            )\n",
    "            for _ in tqdm(range(Nb_simulation), desc=f'Simulating for {file_name}', leave=False)\n",
    "        ]\n",
    "\n",
    "        simulated_l_values = [\n",
    "            [ripley_l(points, r, area) for r in r_values]\n",
    "            for points in simulations\n",
    "        ]\n",
    "        simulated_l_values_dict_slow[file_name] = simulated_l_values\n",
    "\n",
    "        lower_bound = np.percentile(simulated_l_values, 2.5, axis=0)\n",
    "        upper_bound = np.percentile(simulated_l_values, 97.5, axis=0)\n",
    "        confidence_envelopes_slow[file_name] = (lower_bound, upper_bound)\n",
    "\n",
    "print(f\"Monte Carlo simulations completed for all fields of view.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcTrPDFNLF58"
   },
   "source": [
    "## **2.4. Plots the results for each FOV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FYkImji3TPOg"
   },
   "outputs": [],
   "source": [
    "# @title ##Plots the results for each FOV\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "show_plots = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# Visualization of Ripley's L function with confidence envelopes\n",
    "for file_name, l_values in l_values_per_fov_slow.items():\n",
    "    # Retrieve the confidence envelope for the current file\n",
    "    lower_bound, upper_bound = confidence_envelopes_slow.get(file_name, (None, None))\n",
    "\n",
    "    # Only proceed if the confidence envelope exists\n",
    "    if lower_bound is not None and upper_bound is not None:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(r_values, l_values, label=f'L(r) for {file_name}')\n",
    "        plt.fill_between(r_values, lower_bound, upper_bound, color='gray', alpha=0.5)\n",
    "        plt.xlabel('Radius (r)')\n",
    "        plt.ylabel(\"Ripley's L Function\")\n",
    "        plt.title(f\"Ripley's L Function - {file_name}_{analysis_option}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot as a PDF in the specified folder\n",
    "        pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/RipleyL/{file_name}_{analysis_option}.pdf\")\n",
    "        plt.savefig(pdf_path,bbox_inches='tight')\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.close()\n",
    "          # Close the plot to free memory\n",
    "    else:\n",
    "        print(f\"No confidence envelope data available for {file_name}_{analysis_option}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsL11TsbLEIY"
   },
   "source": [
    "## **2.5. Chose a specific radius and plot the results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hQlY8X-nJqYb"
   },
   "outputs": [],
   "source": [
    "# @title ##Define a specific radius and run\n",
    "\n",
    "# Define the specific radius for comparison\n",
    "specific_radius = 50 # @param {type: \"number\"}\n",
    "\n",
    "# Extract L values at the specific radius\n",
    "specific_radius_index = np.argmin(np.abs(r_values - specific_radius))  # Find the index of the closest radius value\n",
    "l_values_at_specific_radius_slow = {fov: l_values[specific_radius_index] for fov, l_values in l_values_per_fov_slow.items()}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(l_values_at_specific_radius_slow.keys(), l_values_at_specific_radius_slow.values())\n",
    "plt.xlabel('Field of View')\n",
    "plt.ylabel(f\"Ripley's L at radius {specific_radius}\")\n",
    "plt.title(f\"Comparison of Ripley's L Function at Radius {specific_radius} Across Different FOVs\")\n",
    "plt.xticks(rotation=45)\n",
    "# Save the plot as a PDF in the specified folder\n",
    "pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/RipleyL/l_values_at_specific_radius_{specific_radius}_{analysis_option}.pdf\")\n",
    "plt.savefig(pdf_path, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create DataFrame with confidence envelopes, median, and L values at the specific radius\n",
    "rows = []\n",
    "for fov, (lower_bound, upper_bound) in confidence_envelopes_slow.items():\n",
    "    l_value = l_values_per_fov_slow[fov][specific_radius_index]\n",
    "    lower = lower_bound[specific_radius_index]\n",
    "    upper = upper_bound[specific_radius_index]\n",
    "\n",
    "    # Retrieve simulated L values for the FOV\n",
    "    simulated_l_values_for_fov_slow = simulated_l_values_dict_slow.get(fov, [])\n",
    "\n",
    "    # Calculate median if simulated L values are available for the FOV\n",
    "    if simulated_l_values_for_fov_slow:\n",
    "        median_vals = [l_vals[specific_radius_index] for l_vals in simulated_l_values_for_fov_slow]\n",
    "        median = np.median(median_vals) if median_vals else np.nan\n",
    "    else:\n",
    "        median = np.nan\n",
    "\n",
    "    rows.append([fov, l_value, lower, upper, median])\n",
    "\n",
    "confidence_df = pd.DataFrame(rows, columns=['File_name', 'Ripley_L_at_Specific_Radius', 'Lower_Bound', 'Upper_Bound', 'Median'])\n",
    "\n",
    "# Merge with additional information\n",
    "additional_info_df = merged_tracks_df[['File_name', 'Condition', 'experiment_nb', 'Repeat']].drop_duplicates('File_name')\n",
    "merged_df = pd.merge(confidence_df, additional_info_df, left_on='File_name', right_on='File_name')\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(f\"{Results_Folder}/Track_Clustering/RipleyL/ripleys_l_values__{specific_radius}_{analysis_option}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJJBmMXVLDPb"
   },
   "source": [
    "## **2.6. Comparison of Ripley's L Values Across Conditions**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iRdk_OfxafE9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# @title ##Comparison of Ripley\\'s L Values Across Conditions\n",
    "\n",
    "# Convert 'Condition' to string if it's not already\n",
    "merged_df['Condition'] = merged_df['Condition'].astype(str)\n",
    "\n",
    "# Create the box plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=merged_df, x='Condition', y='Ripley_L_at_Specific_Radius')\n",
    "\n",
    "# Overlay the Monte Carlo simulation results\n",
    "for condition in merged_df['Condition'].unique():\n",
    "    condition_data = merged_df[merged_df['Condition'] == condition]\n",
    "\n",
    "    # Plot median values\n",
    "    medians = condition_data['Median']\n",
    "    plt.scatter([condition] * len(medians), medians, color='red', alpha=0.5)  # Median\n",
    "\n",
    "    # Handle NaN values and calculate mean and error only for non-NaN values\n",
    "    valid_data = condition_data.dropna(subset=['Median', 'Lower_Bound', 'Upper_Bound'])\n",
    "    if not valid_data.empty:\n",
    "        median_mean = valid_data['Median'].mean()\n",
    "        lower_mean = valid_data['Lower_Bound'].mean()\n",
    "        upper_mean = valid_data['Upper_Bound'].mean()\n",
    "        yerr = [[median_mean - lower_mean], [upper_mean - median_mean]]\n",
    "\n",
    "        # Check if yerr contains valid data before plotting\n",
    "        if not any(np.isnan(yerr)):\n",
    "            plt.errorbar(condition, median_mean, yerr=yerr, fmt='o', color='black', alpha=0.5)  # Confidence interval\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel(f\"Ripley's L at radius {specific_radius}\")\n",
    "plt.title('Comparison of Ripley\\'s L Values Across Conditions with Monte Carlo Simulation Results')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the figure before showing it\n",
    "pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/RipleyL/l_values_Conditions_radius_{specific_radius}_{analysis_option}.pdf\")\n",
    "plt.savefig(pdf_path, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiY-Iq5HcwA0"
   },
   "source": [
    "## **2.7. Plot the analysis point for each FOV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Fn-E8ZoIQG2m"
   },
   "outputs": [],
   "source": [
    "# @title ##Run the cell to plot the coordinates used for the spatial analysis for all FOV\n",
    "\n",
    "# Ensure the directory for saving plots exists\n",
    "if not os.path.exists(f\"{Results_Folder}/Track_Clustering/Coordinates\"):\n",
    "    os.makedirs(f\"{Results_Folder}/Track_Clustering/Coordinates\")\n",
    "\n",
    "show_plots = False  # @param {type:\"boolean\"}\n",
    "\n",
    "filenames = merged_spots_df['File_name'].unique()\n",
    "\n",
    "for filename in filenames:\n",
    "    analysis_option = \"beginning\"  # You can set your preferred analysis option here\n",
    "    plot_clustering_coordinates(filename, merged_spots_df, analysis_option, Results_Folder, r_values_end, max_x, max_y, show_plots)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMfsr-DFsVN4"
   },
   "source": [
    "# **Part 3: Version log**\n",
    "---\n",
    "<font size = 4>While we strive to provide accurate and helpful information, please be aware that:\n",
    "  - This notebook may contain bugs.\n",
    "  - Features are currently limited and will be expanded in future releases.\n",
    "\n",
    "<font size = 4>We encourage users to report any issues or suggestions for improvement.\n",
    "\n",
    "<font size = 4>**Version 1.0.1**\n",
    "  - Includes a general data reader\n",
    "  - Plotting functions are imported from the main code\n",
    "    \n",
    "<font size = 4>**Version 0.9.1**\n",
    "  - Improved documentation\n",
    "  - Improved saving strategy\n",
    "\n",
    "<font size = 4>**Version 0.8**\n",
    "  - First release of this notebook\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
