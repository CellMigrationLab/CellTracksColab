{"cells":[{"cell_type":"markdown","metadata":{"id":"xF4zYMmXULP7"},"source":["# **CellTracksColab**\n","---\n","\n","<font size = 4>Colab Notebook for compiling and analyzing Tracks\n","\n","\n","<font size = 4>Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aR2U8v9YoJcW"},"source":["# **Part 0. Before getting started**\n","---\n","\n","<font size = 5>**Important notes**\n","\n","---\n","## Data Requirements for Analysis\n","\n","<font size = 4>For a successful analysis using this notebook, ensure your data meets the following criteria:\n","\n","1. **CSV Format**: The dataset should be in CSV format.\n","2. **Essential Columns**:\n","   - **Track ID**: A unique identifier for each track. Each track should have a distinct ID.\n","   - **X Coordinate**: Represents the X position of the tracked object.\n","   - **Y Coordinate**: Represents the Y position of the tracked object.\n","   - **Time Point**: Indicates the time or frame at which the tracking data was recorded.\n","3. **Optional Columns**:\n","   - **Z Coordinate**: If available, this represents the Z position of the tracked object.\n","\n","**Note**: The naming and the sequence of columns in your CSV file aren't critical. You'll have the flexibility to map them during the analysis process in this notebook.</font>\n","\n","<font size = 4>Be advised of one significant limitation inherent to this notebook.\n","\n","<font size = 4 color=\"red\">**This notebook does not support Track splitting**</font>.\n","\n","---\n","\n","## Folder Hierarchy\n","\n","- üìÅ **Experiments** `[Folder_path]`\n","  - üåø **Condition_1** `[‚Äòcondition‚Äô is derived from this folder name]`\n","    - üîÑ **R1** `[‚Äòrepeat‚Äô is derived from this folder name]`\n","      - üìÑ `FOV1.csv`\n","      - üìÑ `FOV2.csv`\n","    - üîÑ **R2**\n","      - üìÑ `FOV1.csv`\n","      - üìÑ `FOV2.csv`\n","  - üåø **Condition_2**\n","    - üîÑ **R1**\n","    - üîÑ **R2**\n","\n","<font size = 4>In this representation, different symbols are used to represent folders and files clearly:\n","\n","üìÅ represents the main folder or directory.\n","üåø represents the condition folders.\n","üîÑ represents the repeat folders.\n","üìÑ represents the individual CSV files.\n","\n","---\n","## Test dataset\n","\n","A test dataset can be downloaded directly in this notebook or is available here:\n","\n","https://zenodo.org/record/8413510\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JrkfFr7mgZmA"},"outputs":[],"source":["# @title #MIT License\n","\n","print(\"\"\"\n","**MIT License**\n","\n","Copyright (c) 2023 Guillaume Jacquemet\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE.\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"Y4-Ft-yNRVCc"},"source":["--------------------------------------------------------\n","# **Part 1. Prepare the session and load your data**\n","--------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"9h0prdayn0qG"},"source":["## **1.1. Install key dependencies**\n","---\n","<font size = 4>"]},{"cell_type":"code","source":["#@markdown ##Play to install\n","!pip -q install pandas scikit-learn\n","!pip -q install hdbscan\n","!pip -q install umap-learn\n","!pip -q install plotly\n","!pip -q install tqdm"],"metadata":{"cellView":"form","id":"S_BZuYOQGo1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAP0ahCzn1V6","cellView":"form"},"outputs":[],"source":["#@markdown ##Play to load the dependancies\n","\n","import ipywidgets as widgets\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import numpy as np\n","import itertools\n","from matplotlib.gridspec import GridSpec\n","import requests\n","\n","# Current version of the notebook the user is running\n","current_version = \"0.8\"\n","Notebook_name = 'Main'\n","\n","# URL to the raw content of the version file in the repository\n","version_url = \"https://raw.githubusercontent.com/guijacquemet/CellTracksColab/main/Notebook/latest_version.txt\"\n","\n","# Function to define colors for formatting messages\n","class bcolors:\n","    WARNING = '\\033[91m'  # Red color for warning messages\n","    ENDC = '\\033[0m'      # Reset color to default\n","\n","# Check if this is the latest version of the notebook\n","try:\n","    All_notebook_versions = pd.read_csv(version_url, dtype=str)\n","    print('Notebook version: ' + current_version)\n","\n","    # Check if 'Version' column exists in the DataFrame\n","    if 'Version' in All_notebook_versions.columns:\n","        Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Notebook_name]['Version'].iloc[0]\n","        print('Latest notebook version: ' + Latest_Notebook_version)\n","\n","        if current_version == Latest_Notebook_version:\n","            print(\"This notebook is up-to-date.\")\n","        else:\n","            print(bcolors.WARNING + \"A new version of this notebook has been released. We recommend that you download it at https://github.com/guijacquemet/CellTracksColab\" + bcolors.ENDC)\n","    else:\n","        print(\"The 'Version' column is not present in the version file.\")\n","except requests.exceptions.RequestException as e:\n","    print(\"Unable to fetch the latest version information. Please check your internet connection.\")\n","except Exception as e:\n","    print(\"An error occurred:\", str(e))\n","\n","#----------------------- Key functions -----------------------------#\n","\n","# Function to calculate Cohen's d\n","def cohen_d(group1, group2):\n","    diff = group1.mean() - group2.mean()\n","    n1, n2 = len(group1), len(group2)\n","    var1 = group1.var()\n","    var2 = group2.var()\n","    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n","    d = diff / np.sqrt(pooled_var)\n","    return d\n","\n","def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n","    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n","\n","    # Estimating the number of chunks based on the provided chunk size\n","    num_chunks = int(len(df) / chunk_size) + 1\n","\n","    # Create a tqdm instance for progress tracking\n","    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n","        # Open the file for writing\n","        with open(path, \"w\") as f:\n","            # Write the header once at the beginning\n","            df.head(0).to_csv(f, index=False)\n","\n","            for chunk in np.array_split(df, num_chunks):\n","                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n","                pbar.update(len(chunk))\n","\n","def check_for_nans(df, df_name):\n","    \"\"\"\n","    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n","\n","    Args:\n","    df (pd.DataFrame): DataFrame to be checked for NaN values.\n","    df_name (str): The name of the DataFrame as a string, used for printing.\n","    \"\"\"\n","    # Check if the DataFrame has any NaN values and print a warning if it does.\n","    nan_columns = df.columns[df.isna().any()].tolist()\n","\n","    if nan_columns:\n","        for col in nan_columns:\n","            nan_count = df[col].isna().sum()\n","            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n","    else:\n","        print(f\"No NaN values found in {df_name}.\")\n","\n","\n","import pandas as pd\n","import os\n","\n","def save_parameters(params, file_path, param_type):\n","    # Convert params dictionary to a DataFrame for human readability\n","    new_params_df = pd.DataFrame(list(params.items()), columns=['Parameter', 'Value'])\n","    new_params_df['Type'] = param_type\n","\n","    if os.path.exists(file_path):\n","        # Read existing file\n","        existing_params_df = pd.read_csv(file_path)\n","\n","        # Merge the new parameters with the existing ones\n","        # Update existing parameters or append new ones\n","        updated_params_df = pd.merge(existing_params_df, new_params_df,\n","                                     on=['Type', 'Parameter'],\n","                                     how='outer',\n","                                     suffixes=('', '_new'))\n","\n","        # If there's a new value, update it, otherwise keep the old value\n","        updated_params_df['Value'] = updated_params_df['Value_new'].combine_first(updated_params_df['Value'])\n","\n","        # Drop the temporary new value column\n","        updated_params_df.drop(columns='Value_new', inplace=True)\n","    else:\n","        # Use new parameters DataFrame directly if file doesn't exist\n","        updated_params_df = new_params_df\n","\n","    # Save the updated DataFrame to CSV\n","    updated_params_df.to_csv(file_path, index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"3Kzd_8GUnpbw"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","id":"GA1wCrkoV4i5","executionInfo":{"status":"ok","timestamp":1703072225292,"user_tz":-120,"elapsed":19730,"user":{"displayName":"Guillaume Jacquemet","userId":"15907180801735728934"}},"outputId":"d9fefb21-3014-4d3d-f39f-736e850fe589","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive\n"]}],"source":["#@markdown ##Play the cell to connect your Google Drive to Colab\n","\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bsDAwkSOo1gV"},"source":["## **1.3. Compile your data or load existing dataframes**\n","---\n","\n","<font size = 4> Please ensure that your data is properly organised (see above)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWqpUp6BjcSR","cellView":"form"},"outputs":[],"source":["#@markdown ##Provide the path to your dataset:\n","\n","#@markdown ###You have multiple tracking files you want to compile, provide the path to your:\n","\n","import os\n","import re\n","import glob\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import requests\n","import zipfile\n","\n","Folder_path = ''  # @param {type: \"string\"}\n","\n","#@markdown ###Or use a test dataset (up to 10 min download)\n","Use_test_dataset = False #@param {type:\"boolean\"}\n","\n","#@markdown ###Provide the path to your Result folder\n","\n","Results_Folder = \"\"  # @param {type: \"string\"}\n","\n","if not Results_Folder:\n","    Results_Folder = '/content/Results'  # Default Results_Folder path if not defined\n","\n","if not os.path.exists(Results_Folder):\n","    os.makedirs(Results_Folder)  # Create Results_Folder if it doesn't exist\n","\n","# Print the location of the result folder\n","print(f\"Result folder is located at: {Results_Folder}\")\n","\n","def populate_columns(df, filepath):\n","    # Extract the parts of the file path\n","    path_parts = os.path.normpath(filepath).split(os.sep)\n","\n","    if len(path_parts) < 3:\n","        # if there are not enough parts in the path to extract folder and parent folder\n","        print(f\"Error: Cannot extract parent folder and folder from the filepath: {filepath}\")\n","        return df\n","\n","    # Assuming that the file is located at least two levels deep in the directory structure\n","    folder_name = path_parts[-2]  # The folder name is the second last part of the path\n","    parent_folder_name = path_parts[-3]  # The parent folder name is the third last part of the path\n","\n","    filename_without_extension = os.path.splitext(os.path.basename(filepath))[0]\n","    df['File_name'] = filename_without_extension\n","    df['Condition'] = parent_folder_name  # Populate 'Condition' with the parent folder name\n","    df['experiment_nb'] = folder_name  # Populate 'Repeat' with the folder name\n","\n","    return df\n","\n","def load_and_populate(file_pattern, usecols=None):\n","    df_list = []\n","    pattern = re.compile(file_pattern)  # Compile the file pattern to a regex object\n","    files_to_process = []\n","\n","    # First, list all the files we'll be processing\n","    for dirpath, dirnames, filenames in os.walk(Folder_path):\n","        for filename in filenames:\n","            if pattern.match(filename):  # Check if the filename matches the file pattern\n","                filepath = os.path.join(dirpath, filename)\n","                files_to_process.append(filepath)\n","\n","    # Create a tqdm instance for progress tracking\n","    for filepath in tqdm(files_to_process, desc=\"Processing Files\"):\n","        df = pd.read_csv(filepath, skiprows=[1, 2, 3], usecols=usecols)\n","        df_list.append(populate_columns(df, filepath))\n","\n","    if not df_list:  # if df_list is empty, return an empty DataFrame\n","        print(f\"No files found with pattern: {file_pattern}\")\n","        return pd.DataFrame()\n","\n","    merged_df = pd.concat(df_list, ignore_index=True)\n","    return merged_df\n","\n","def sort_and_generate_repeat(merged_df):\n","    merged_df.sort_values(['Condition', 'experiment_nb'], inplace=True)\n","    merged_df = merged_df.groupby('Condition', group_keys=False).apply(generate_repeat)\n","    return merged_df\n","\n","def generate_repeat(group):\n","    unique_experiment_nbs = sorted(group['experiment_nb'].unique())\n","    experiment_nb_to_repeat = {experiment_nb: i+1 for i, experiment_nb in enumerate(unique_experiment_nbs)}\n","    group['Repeat'] = group['experiment_nb'].map(experiment_nb_to_repeat)\n","    return group\n","\n","def check_unique_id_match(df1, df2):\n","    df1_ids = set(df1['Unique_ID'])\n","    df2_ids = set(df2['Unique_ID'])\n","\n","    # Check if the IDs in the two dataframes match\n","    if df1_ids == df2_ids:\n","        print(\"The Unique_ID values in both dataframes match perfectly!\")\n","    else:\n","        missing_in_df1 = df2_ids - df1_ids\n","        missing_in_df2 = df1_ids - df2_ids\n","\n","        if missing_in_df1:\n","            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n","\n","        if missing_in_df2:\n","            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n","\n","\n","# URL of the file you want to download\n","url = \"https://zenodo.org/record/8420011/files/T_Cells_spots_only.zip?download=1\"\n","\n","# Specify the local directory where you want to extract the downloaded files\n","extracted_dir = \"/content/Tracks\"\n","\n","if Use_test_dataset:\n","    # Define the local file path for the downloaded zip file\n","    Folder_path = extracted_dir\n","    local_zip_file = os.path.join(extracted_dir, \"T_cell_dataset.zip\")\n","\n","    # Check if the extracted directory exists\n","    if os.path.exists(extracted_dir):\n","        print(\"Dataset already downloaded.\")\n","\n","        Folder_path = '/content/Tracks/Tracks'\n","\n","    # Check if the ZIP file already exists\n","    if not os.path.exists(local_zip_file):\n","        print(\"Downloading test dataset\")\n","        response = requests.get(url, stream=True)\n","\n","        if response.status_code == 200:\n","            # Create the extracted directory if it doesn't exist\n","            os.makedirs(extracted_dir, exist_ok=True)\n","\n","            # Calculate the total file size for the progress bar\n","            total_size = int(response.headers.get('content-length', 0))\n","\n","            # Create a tqdm progress bar\n","            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n","                # Download and save the content with progress tracking\n","                with open(local_zip_file, \"wb\") as file:\n","                    for data in response.iter_content(chunk_size=1024):\n","                        pbar.update(len(data))\n","                        file.write(data)\n","\n","            print(\"Test dataset downloaded successfully.\")\n","                    # Extract the contents of the zip file to the specified directory\n","            with zipfile.ZipFile(local_zip_file, 'r') as zip_ref:\n","              zip_ref.extractall(extracted_dir)\n","            print(\"Test dataset extracted successfully.\")\n","            Folder_path = '/content/Tracks/Tracks'\n","\n","\n","merged_tracks_df = pd.DataFrame()\n","\n","if Folder_path:\n","    merged_spots_df = load_and_populate(r'.*\\.csv$')\n","    merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n","    save_dataframe_with_progress(merged_spots_df, Results_Folder + '/' + 'merged_Spots.csv', desc=\"Saving Spots\")\n","\n","    print(\"...Done\")\n"]},{"cell_type":"markdown","source":["## **1.4. Map your data**\n","---\n","\n","## Required Columns for Track Parameter Calculation\n","\n","<font size = 4>To calculate track parameters, we need to ensure the presence of specific columns in the dataset. Here's a breakdown of the required columns:\n","\n","- **`TRACK_ID`**: Uniquely identifies each individual track.\n","\n","- **`POSITION_X`**: Represents the x-coordinate of a position in the track.\n","\n","- **`POSITION_Y`**: Represents the y-coordinate of a position in the track.\n","\n","- **`POSITION_Z`**: Represents the z-coordinate, providing depth information.\n","\n","- **`POSITION_T`**: Denotes the time point for each position in the track.\n"],"metadata":{"id":"dr9Wm3BIHnuI"}},{"cell_type":"code","source":["#@markdown ###Map your data\n","\n","\n","import ipywidgets as widgets  # Ensure that we have the required widgets module imported\n","\n","merged_tracks_df = pd.DataFrame()\n","\n","def two_stage_column_mapping(merged_spots_df):\n","\n","    # Stage 1: Ask if the data is 2D or 3D\n","    data_dimensionality = widgets.RadioButtons(\n","        options=['2D', '3D'],\n","        description='Data Dimensionality:',\n","        disabled=False\n","    )\n","\n","    stage_1_button = widgets.Button(description=\"Proceed\")\n","\n","    display(data_dimensionality, stage_1_button)\n","\n","    def proceed_to_stage_2(button):\n","        global merged_spots_df  # Declare merged_spots_df as nonlocal\n","\n","        # Clear current output to move to the next stage\n","        from IPython.display import clear_output\n","        clear_output()\n","\n","        # If 2D is selected and POSITION_Z exists, delete it\n","        if data_dimensionality.value == '2D' and 'POSITION_Z' in merged_spots_df.columns:\n","            merged_spots_df = merged_spots_df.drop('POSITION_Z', axis=1)\n","\n","        # Stage 2: Column Mapping\n","        dropdowns = {}\n","        required_columns = ['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T']\n","\n","        # If 3D is chosen, add POSITION_Z to the required columns\n","        if data_dimensionality.value == '3D':\n","            required_columns.append('POSITION_Z')\n","\n","        for col in required_columns:\n","            description_label = widgets.Label(f\"Map {col} to:\")\n","            dropdowns[col] = widgets.Dropdown(options=merged_spots_df.columns, layout=widgets.Layout(width='250px'))\n","\n","            # Use HBox to display the description label next to the dropdown\n","            hbox = widgets.HBox([description_label, dropdowns[col]])\n","            display(hbox)\n","\n","        stage_2_button = widgets.Button(description=\"Rename Columns\")\n","\n","        def rename_columns(button):\n","            global merged_spots_df\n","            global merged_tracks_df\n","            column_mapping = {dropdown.value: col for col, dropdown in dropdowns.items()}\n","            merged_spots_df = merged_spots_df.rename(columns=column_mapping)\n","\n","            # If 2D was chosen, add back the POSITION_Z column filled with 0s at the end\n","            if data_dimensionality.value == '2D':\n","                merged_spots_df['POSITION_Z'] = 0\n","\n","            print(\"Columns Renamed!\")\n","            merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n","            merged_spots_df['Unique_ID'] = merged_spots_df['File_name'] + \"_\" + merged_spots_df['TRACK_ID'].astype(str)\n","\n","            # Extracting unique Unique_ID values from merged_spots_df\n","            unique_ids = merged_spots_df['Unique_ID'].drop_duplicates().reset_index(drop=True)\n","\n","            # Creating merged_tracks_df with only the unique Unique_ID values\n","            merged_tracks_df = pd.DataFrame(unique_ids, columns=['Unique_ID'])\n","            print(\"Create a the merged_tracks_df to store track parameters\")\n","            # Specify the columns you want to merge\n","            columns_to_merge = ['Unique_ID', 'File_name', 'Condition', 'experiment_nb', 'Repeat']\n","\n","            # Filter to only include the desired columns\n","            filtered_df = merged_spots_df[columns_to_merge].drop_duplicates(subset='Unique_ID')\n","\n","            # Find the overlapping columns between the two DataFrames, excluding the merging key\n","            overlapping_columns = merged_tracks_df.columns.intersection(filtered_df.columns).drop('Unique_ID')\n","\n","            # Drop the overlapping columns from the left DataFrame\n","            merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","            # Merge the filtered df_directionality back into the original DataFrame\n","            merged_tracks_df = pd.merge(merged_tracks_df, filtered_df, on='Unique_ID', how='left')\n","\n","            # Save the DataFrame with the selected columns merged\n","            save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv', desc=\"Saving Tracks\")\n","            check_unique_id_match(merged_spots_df, merged_tracks_df)\n","            save_dataframe_with_progress(merged_spots_df, Results_Folder + '/' + 'merged_Spots.csv', desc=\"Saving Spots\")\n","\n","            print(\"...Done\")\n","\n","            return merged_spots_df, merged_tracks_df\n","\n","        stage_2_button.on_click(rename_columns)\n","        display(stage_2_button)\n","\n","    stage_1_button.on_click(proceed_to_stage_2)\n","\n","two_stage_column_mapping(merged_spots_df)\n","\n"],"metadata":{"cellView":"form","id":"5VE9oqIEHrG-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **1.5. Spatial and Temporal Calibration for your data (optional)**\n","---\n","<font size = 4>If your tracking data are currently in pixel units and you wish to recalibrate them to reflect actual physical units (e.g., micrometers), you can use the provided widget to input your calibration values.\n","\n","### **Instructions**:\n","\n","1. **Spatial Calibration**:\n","    - **X Calibration**: Input the real-world length (in your desired unit, e.g., micrometers) corresponding to a single pixel's width in your images.\n","    - **Y Calibration**: Input the real-world length (in your desired unit) corresponding to a single pixel's height in your images.\n","    - **Z Calibration**: Input the real-world depth (in your desired unit) corresponding to a single pixel's depth in z-stacked images. If you have a 2D dataset, leave to 1.\n","    - **Spatial Unit**: Specify the unit of measurement for your spatial calibration (e.g., \"micron\", \"mm\").\n","\n","2. **Temporal Calibration**:\n","    - **Temporal Calibration**: Input the real-world time (in your desired time unit, e.g., seconds) corresponding to the interval between frames in your tracking data.\n","    - **Time Unit**: Specify the unit of measurement for your temporal calibration (e.g., \"second\", \"minute\").\n","\n","3. After inputting your calibration values, click the **\"Save Calibration\"** button. This action will recalibrate the `POSITION_X`, `POSITION_Y`, `POSITION_Z`, and `POSITION_T` columns of your `merged_spots_df` DataFrame to reflect the real-world units.\n","\n","### **Note**:\n","    - The test dataset is already calibrated.\n"],"metadata":{"id":"N-Z6_wGYHv-e"}},{"cell_type":"code","source":["# @title ##Run the cell to calibrate your data\n","\n","# Import necessary libraries\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","# Duplicate merged_spots_df and create merged_spots_calibrated_df\n","merged_spots_calibrated_df = merged_spots_df.copy()\n","\n","# Define a layout for the widgets\n","widget_layout = widgets.Layout(width='200px')\n","description_layout = widgets.Layout(width='200px')\n","\n","# Create function to display widgets with wide descriptions\n","def display_with_description(widget, description):\n","    display(widgets.HBox([widgets.Label(description, layout=description_layout), widget]))\n","\n","# Create widgets for spatial calibration input with the specified layout\n","x_calibration = widgets.FloatText(value=1.0, layout=widget_layout)\n","y_calibration = widgets.FloatText(value=1.0, layout=widget_layout)\n","z_calibration = widgets.FloatText(value=1.0, layout=widget_layout)\n","spatial_unit_of_measurement = widgets.Text(value='pixel', layout=widget_layout)\n","\n","# Create widgets for temporal calibration\n","temporal_calibration = widgets.FloatText(value=1.0, layout=widget_layout)\n","temporal_unit_of_measurement = widgets.Text(value='second', layout=widget_layout)\n","\n","# Function to save calibration when the button is pressed\n","def save_calibration(button):\n","    global spots_df_to_use\n","    # Logic to save or use the calibration values\n","    x_cal_value = x_calibration.value\n","    y_cal_value = y_calibration.value\n","    z_cal_value = z_calibration.value\n","    spatial_calibration_unit = spatial_unit_of_measurement.value\n","    time_cal_value = temporal_calibration.value\n","    time_unit = temporal_unit_of_measurement.value\n","\n","    # Update the merged_spots_df columns with calibration values\n","    merged_spots_calibrated_df['POSITION_X'] = merged_spots_df['POSITION_X'] * x_calibration.value\n","    merged_spots_calibrated_df['POSITION_Y'] = merged_spots_df['POSITION_Y'] * y_calibration.value\n","    merged_spots_calibrated_df['POSITION_Z'] = merged_spots_df['POSITION_Z'] * z_calibration.value\n","\n","# Update the temporal column with calibration value\n","    merged_spots_calibrated_df['POSITION_T'] = merged_spots_df['POSITION_T'] * temporal_calibration.value\n","\n","    save_dataframe_with_progress(merged_spots_calibrated_df, Results_Folder + '/' + 'merged_Spots_calibrated.csv', desc=\"Saving Spots\")\n","\n","    print(f\"Spatial Calibration saved: X={x_cal_value}, Y={y_cal_value}, Z={z_cal_value} in {spatial_calibration_unit}\")\n","    print(f\"Temporal Calibration saved: {time_cal_value} per frame in {time_unit}\")\n","\n","\n","# Create a button widget\n","save_button = widgets.Button(description=\"Save Calibration\", layout=widget_layout)\n","save_button.on_click(save_calibration)  # Set the action for the button\n","\n","# Display widgets with wide descriptions\n","display_with_description(x_calibration, 'X Calibration:')\n","display_with_description(y_calibration, 'Y Calibration:')\n","display_with_description(z_calibration, 'Z Calibration:')\n","display_with_description(spatial_unit_of_measurement, 'Spatial Unit:')\n","display_with_description(temporal_calibration, 'Temporal Calibration:')\n","display_with_description(temporal_unit_of_measurement, 'Time Unit:')\n","display(save_button)\n"],"metadata":{"cellView":"form","id":"NWEkrZeCH1G6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52STmnv43d45"},"source":["## **1.6. Visualise your tracks**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AE881uJW5ukQ"},"outputs":[],"source":["# @title ##Run the cell and choose the file you want to inspect\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","import matplotlib.pyplot as plt\n","\n","if not os.path.exists(Results_Folder+\"/Tracks\"):\n","    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = merged_spots_df['File_name'].unique()\n","\n","# Create a Dropdown widget with the filenames\n","filename_dropdown = widgets.Dropdown(\n","    options=filenames,\n","    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n","    description='File Name:',\n",")\n","\n","def plot_coordinates(filename):\n","    if filename:\n","        # Filter the DataFrame based on the selected filename\n","        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n","\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Coordinates for {filename}')\n","        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n","        plt.show()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","# Link the Dropdown widget to the plotting function\n","interact(plot_coordinates, filename=filename_dropdown)\n"]},{"cell_type":"markdown","metadata":{"id":"9nl8H75zbh15"},"source":["## **1.7. Filter and smooth your tracks (Optional)**\n","---"]},{"cell_type":"markdown","metadata":{"id":"DSwE2j8598Tg"},"source":["\n","<font size = 4>The following section provides an interactive way to refine your tracking data. Here's what it's designed to achieve:\n","\n","1. <font size = 4>**Filter Tracks**:\n","    - <font size = 4>This feature allows you to define a range for the track lengths you're interested in. By adjusting the `Min Length` and `Max Length` sliders, you can ignore very short or very long tracks that might be artifacts or noise in your data.\n","\n","2. <font size = 4>**Smooth Tracks**:\n","    - <font size = 4>The positional data in your tracks can be smoothed using a moving average technique. By adjusting the `Smoothing` slider, you can control the degree of smoothing applied to the tracks. A higher value will average over more points, producing smoother tracks. This can be beneficial if your raw data has a lot of jitter or minor positional fluctuations.\n","\n","<font size = 4>**How to Use**:\n","\n","- <font size = 4>**Min Length**: Use the slider to set the minimum length of the tracks you're interested in.\n","- <font size = 4>**Max Length**: Use the slider to set the maximum length of the tracks you're interested in.\n","- <font size = 4>**Smoothing**: Adjust this slider to control the degree of smoothing you'd like to apply to your tracks.\n","- <font size = 4>**Apply Filters**: After adjusting the sliders to your preference, click this button. This will process the data based on your choices and prepare it for downstream analyses.\n","\n"]},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","\n","# @title ##Run to filter and smooth your tracks (slow when the dataset is large)\n","\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","import pandas as pd\n","import numpy as np\n","\n","def save_filter_smoothing_params(file_path, min_length, max_length, smoothing_neighbors):\n","    params = {\n","        'Min Length': min_length,\n","        'Max Length': max_length,\n","        'Smoothing Neighbors': smoothing_neighbors\n","    }\n","    params_df = pd.DataFrame([params])\n","    params_df.to_csv(file_path, index=False)\n","\n","def optimized_filter_and_smooth_tracks(dataframe, min_length=5, max_length=100, smoothing_neighbors=3):\n","    track_lengths = dataframe.groupby('Unique_ID').size()\n","    valid_tracks = track_lengths[(track_lengths >= min_length) & (track_lengths <= max_length)].index\n","    filtered_df = dataframe[dataframe['Unique_ID'].isin(valid_tracks)]\n","\n","    def smooth_track(track_data):\n","        track_data = track_data.sort_values(by='POSITION_T')\n","        smoothed_track = track_data.copy()\n","\n","        if len(track_data) >= smoothing_neighbors:\n","            smoothed_X = track_data['POSITION_X'].rolling(window=smoothing_neighbors, center=True).mean()\n","            smoothed_Y = track_data['POSITION_Y'].rolling(window=smoothing_neighbors, center=True).mean()\n","            smoothed_Z = track_data['POSITION_Z'].rolling(window=smoothing_neighbors, center=True).mean()\n","\n","            smoothed_track['POSITION_X'] = smoothed_X\n","            smoothed_track['POSITION_Y'] = smoothed_Y\n","            smoothed_track['POSITION_Z'] = smoothed_Z\n","\n","        return smoothed_track\n","\n","    # Use tqdm for progress bar\n","    tqdm.pandas(desc=\"Processing Tracks\")\n","    smoothed_df = filtered_df.groupby('Unique_ID').progress_apply(smooth_track).reset_index(drop=True)\n","\n","    return smoothed_df\n","\n","\n","def on_button_click(button):\n","    global filtered_and_smoothed_df\n","    with output:\n","        clear_output(wait=True)\n","\n","        filtered_and_smoothed_df = optimized_filter_and_smooth_tracks(\n","            merged_spots_df,\n","            min_length=min_length_slider.value,\n","            max_length=max_length_slider.value,\n","            smoothing_neighbors=smoothing_slider.value\n","        )\n","\n","        save_dataframe_with_progress(filtered_and_smoothed_df, Results_Folder + '/' + 'filtered_and_smoothed_Spots.csv')\n","\n","        # Save the parameters\n","        params_file_path = os.path.join(Results_Folder, \"filter_smoothing_parameters.csv\")\n","        save_filter_smoothing_params(params_file_path, min_length_slider.value, max_length_slider.value, smoothing_slider.value)\n","\n","        print(\"Done\")\n","\n","\n","\n","\n","# Calculate the maximum track length from the data\n","max_track_length = merged_spots_df.groupby('Unique_ID').size().max()\n","\n","# Define widgets for user input\n","min_length_slider = widgets.IntSlider(\n","    value=1,\n","    min=1,\n","    max=max_track_length,\n","    step=1,\n","    description='Min Length:',\n","    continuous_update=False\n",")\n","\n","max_length_slider = widgets.IntSlider(\n","    value=max_track_length,\n","    min=1,\n","    max=max_track_length,\n","    step=1,\n","    description='Max Length:',\n","    continuous_update=False\n",")\n","\n","smoothing_slider = widgets.IntSlider(\n","    value=3,\n","    min=1,\n","    max=10,\n","    step=1,\n","    description='Smoothing:',\n","    continuous_update=False\n",")\n","\n","\n","apply_button = widgets.Button(description=\"Apply Filters\", button_style='info')\n","output = widgets.Output()\n","\n","apply_button.on_click(on_button_click)\n","\n","# Display the widgets\n","display(widgets.VBox([min_length_slider, max_length_slider, smoothing_slider, apply_button, output]))\n","\n","\n"],"metadata":{"cellView":"form","id":"Uo8B54BnkYDT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"c37ZpK82a3Hl"},"outputs":[],"source":["# @title ##Compare Raw vs Filtered tracks\n","\n","from ipywidgets import interactive\n","from IPython.display import display\n","from IPython.display import clear_output\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","import matplotlib.pyplot as plt\n","\n","if not os.path.exists(Results_Folder+\"/Tracks\"):\n","    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = merged_spots_df['File_name'].unique()\n","\n","# Create a Dropdown widget with the filenames\n","filename_dropdown = widgets.Dropdown(\n","    options=filenames,\n","    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n","    description='File Name:',\n",")\n","\n","\n","def plot_coordinates_side_by_side(filename):\n","    if filename:\n","        # Filter the DataFrames based on the selected filename\n","        raw_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n","        processed_df = filtered_and_smoothed_df[filtered_and_smoothed_df['File_name'] == filename]\n","\n","        # Create subplots\n","        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n","\n","        # Create a colormap to ensure consistent colors across tracks\n","        unique_ids = raw_df['Unique_ID'].unique()\n","        colormap = plt.get_cmap('tab20')\n","\n","        # Plot Raw Data\n","        for idx, unique_id in enumerate(unique_ids):\n","            unique_data = raw_df[raw_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            color_val = colormap(idx % 20 / 20)  # Ensure colors are within colormap range\n","            axes[0].plot(unique_data['POSITION_X'], unique_data['POSITION_Y'],\n","                         color=color_val, marker='o', linestyle='-', markersize=2)\n","        axes[0].set_title(f'Raw Coordinates for {filename}')\n","        axes[0].set_xlabel('POSITION_X')\n","        axes[0].set_ylabel('POSITION_Y')\n","\n","        # Plot Filtered & Smoothed Data\n","        for idx, unique_id in enumerate(unique_ids):\n","            unique_data = processed_df[processed_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            color_val = colormap(idx % 20 / 20)  # Ensure colors are within colormap range\n","            axes[1].plot(unique_data['POSITION_X'], unique_data['POSITION_Y'],\n","                         color=color_val, marker='o', linestyle='-', markersize=2)\n","        axes[1].set_title(f'Filtered & Smoothed Coordinates for {filename}')\n","        axes[1].set_xlabel('POSITION_X')\n","        axes[1].set_ylabel('POSITION_Y')\n","\n","        plt.tight_layout()\n","        plt.savefig(f\"{Results_Folder}/Tracks/Filtered_tracks_{filename}.pdf\")\n","        plt.show()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","\n","def on_filename_change(change):\n","    # Clear the previous plots\n","    clear_output(wait=True)\n","\n","    # Redisplay the dropdown\n","    display(filename_dropdown)\n","\n","    # Generate and display the new plots\n","    plot_coordinates_side_by_side(change['new'])\n","\n","# Define a button-click callback\n","def on_plot_button_click(button):\n","    # Clear previous plots\n","    clear_output(wait=True)\n","\n","    # Redisplay the dropdown and button\n","    display(filename_dropdown)\n","    display(plot_button)\n","\n","    # Generate and display the new plots\n","    plot_coordinates_side_by_side(filename_dropdown.value)\n","\n","# Create the plot button and attach the callback\n","plot_button = widgets.Button(\n","    description=\"Plot\",\n","    button_style='info'\n",")\n","plot_button.on_click(on_plot_button_click)\n","\n","# Initial display of the dropdown widget and the plot button\n","display(filename_dropdown)\n","display(plot_button)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"eYR59Kw-uhFv"},"outputs":[],"source":["# @title ##Run to choose which data you want to use for further analysis\n","\n","import ipywidgets as widgets\n","\n","# Layout for the widgets\n","widget_layout = widgets.Layout(width='500px')\n","\n","# Create a RadioButtons widget to allow users to choose the DataFrame\n","data_choice = widgets.RadioButtons(\n","    options=[('Raw data', 'raw'), ('Smooth and filtered data', 'smoothed')],\n","    description='Use:',\n","    value='raw',\n","    disabled=False,\n","    layout=widget_layout\n",")\n","\n","# Create a global variable to store the chosen dataframe\n","spots_df_to_use = None\n","\n","# Create a button for analysis\n","analyze_button = widgets.Button(\n","    description=\"Analyze\",\n","    button_style='info',\n","    layout=widget_layout\n",")\n","\n","# Define the button click callback\n","def on_analyze_button_click(button):\n","    global spots_df_to_use\n","\n","    # Check the selected value and update the global dataframe accordingly\n","    if data_choice.value == 'raw':\n","        spots_df_to_use = merged_spots_df\n","    elif data_choice.value == 'smoothed':\n","        spots_df_to_use = filtered_and_smoothed_df\n","    else:\n","        print(\"Invalid choice!\")\n","        return\n","\n","    print(f\"Analysis will be performed using: {data_choice.label}\")\n","\n","\n","# Assign button callback\n","analyze_button.on_click(on_analyze_button_click)\n","\n","# Initial display of the widgets\n","display(data_choice)\n","display(analyze_button)\n"]},{"cell_type":"markdown","metadata":{"id":"UlUCXg5QUC4f"},"source":["--------------------------------------------------------\n","# **Part 2. Compute track metrics**\n","--------------------------------------------------------\n","<font size = 4 color=\"red\">Part2 does not support Track splitting</font>.\n"]},{"cell_type":"markdown","source":["## **2.1. Duration and speed metrics**\n","---\n","<font size = 4>When this cell is executed, it calculates various metrics for each unique track. Specifically, for each track, it determines the duration of the track, the average, maximum, minimum, and standard deviation of speeds, as well as the total distance traveled by the tracked object."],"metadata":{"id":"hWL9haZ4IA_u"}},{"cell_type":"code","source":["# @title ##Calculate track metrics\n","\n","from tqdm.notebook import tqdm\n","\n","print(\"Calculating track metrics...\")\n","\n","# Check if merged_spots_calibrated_df is None or empty; if so, set it to merged_spots_df\n","if 'merged_spots_calibrated_df' not in globals() or merged_spots_calibrated_df is None or merged_spots_calibrated_df.empty:\n","    merged_spots_calibrated_df = merged_spots_df\n","\n","# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n","if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n","    spots_df_to_use = merged_spots_calibrated_df\n","\n","spots_df_to_use.dropna(subset=['POSITION_X', 'POSITION_Y', 'POSITION_Z'], inplace=True)\n","\n","def calculate_track_metrics(group):\n","    group = group.sort_values('POSITION_T')\n","    positions = group[['POSITION_X', 'POSITION_Y', 'POSITION_Z']].values\n","    times = group['POSITION_T'].values\n","\n","    # Track Duration\n","    duration = times[-1] - times[0]\n","\n","    # Speeds calculation\n","    deltas = np.linalg.norm(np.diff(positions, axis=0), axis=1)\n","    time_diffs = np.diff(times)\n","    time_diffs[time_diffs == 0] = 1e-10\n","    speeds = deltas / time_diffs\n","\n","    return pd.Series({\n","        'Track Duration': duration,\n","        'Mean Speed': speeds.mean(),\n","        'Max Speed': speeds.max(),\n","        'Min Speed': speeds.min(),\n","        'Speed Standard Deviation': speeds.std(),\n","        'Total Distance Traveled': deltas.sum()\n","    })\n","\n","tqdm.pandas(desc=\"Calculating Track Metrics\")\n","\n","# Sort and then calculate metrics\n","spots_df_to_use.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","df_track_metrics = spots_df_to_use.groupby('Unique_ID').progress_apply(calculate_track_metrics).reset_index()\n","\n","# Find overlapping columns, drop them, then merge new metrics into merged_tracks_df\n","overlapping_columns = merged_tracks_df.columns.intersection(df_track_metrics.columns).drop('Unique_ID')\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","merged_tracks_df = pd.merge(merged_tracks_df, df_track_metrics, on='Unique_ID', how='left')\n","\n","# Save the DataFrame (assuming you have a function called save_dataframe_with_progress)\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n","\n","# Safeguard: check for NaN\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","#check_for_nans(merged_spots_df, \"merged_spots_df\")\n","\n","print(\"...Done\")\n"],"metadata":{"cellView":"form","id":"ffpX3SvWID4s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8TqsbEAya-U6"},"source":["## **2.2. Directionality**\n","---\n","<font size = 4>To calculate the directionality of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The directionality, denoted as \\(D\\), is calculated using the formula:\n","\n","$$ D = \\frac{d_{\\text{euclidean}}}{d_{\\text{total path}}} $$\n","\n","where \\($d_{\\text{euclidean}}$\\) is the Euclidean distance between the first and the last points of the track, calculated as:\n","\n","$$ d_{\\text{euclidean}} = \\sqrt{(x_{\\text{end}} - x_{\\text{start}})^2 + (y_{\\text{end}} - y_{\\text{start}})^2 + (z_{\\text{end}} - z_{\\text{start}})^2} $$\n","\n","and \\($d_{\\text{total path}}$\\) is the sum of the Euclidean distances between all consecutive points in the track, representing the total path length traveled. If the total path length is zero, the directionality is defined to be zero. This measure provides insight into the straightness of the path taken, with a value of 1 indicating a straight path between the start and end points, and values approaching 0 indicating more circuitous paths.</font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ijl_DXjhTvnn"},"outputs":[],"source":["# @title ##Calculate directionality\n","import pandas as pd\n","import numpy as np\n","\n","print(\"In progress...\")\n","\n","# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n","if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n","    spots_df_to_use = merged_spots_df\n","\n","spots_df_to_use.dropna(subset=['POSITION_X', 'POSITION_Y', 'POSITION_Z'], inplace=True)\n","\n","# Function to calculate Directionality\n","def calculate_directionality(group):\n","\n","    group = group.sort_values('POSITION_T')\n","    start_point = group.iloc[0][['POSITION_X', 'POSITION_Y', 'POSITION_Z']].to_numpy()\n","    end_point = group.iloc[-1][['POSITION_X', 'POSITION_Y', 'POSITION_Z']].to_numpy()\n","\n","    # Calculating Euclidean distance in 3D between start and end points\n","    euclidean_distance = np.linalg.norm(end_point - start_point)\n","\n","    # Calculating the total path length in 3D\n","    deltas = np.linalg.norm(np.diff(group[['POSITION_X', 'POSITION_Y', 'POSITION_Z']].values, axis=0), axis=1)\n","    total_path_length = deltas.sum()\n","\n","    # Calculating Directionality\n","    D = euclidean_distance / total_path_length if total_path_length != 0 else 0\n","\n","    return pd.Series({'Directionality': D})\n","\n","\n","# Create a tqdm object for the groupby apply\n","tqdm.pandas(desc=\"Calculating Directionality\")\n","\n","# Assuming spots_df_to_use is your DataFrame\n","spots_df_to_use.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","\n","# Calculate directionality for each track\n","df_directionality = spots_df_to_use.groupby('Unique_ID').progress_apply(calculate_directionality).reset_index()\n","\n","# Find the overlapping columns between the two DataFrames, excluding the merging key\n","overlapping_columns = merged_tracks_df.columns.intersection(df_directionality.columns).drop('Unique_ID')\n","\n","# Drop the overlapping columns from the left DataFrame\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","# Merge the directionality back into the original DataFrame\n","merged_tracks_df = pd.merge(merged_tracks_df, df_directionality, on='Unique_ID', how='left')\n","\n","# Save the DataFrame with the calculated directionality\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n","\n","# Safeguard: check for NaN\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","#check_for_nans(merged_spots_df, \"merged_spots_df\")\n","\n","print(\"...Done\")\n"]},{"cell_type":"markdown","metadata":{"id":"Gb_W3oSleJOj"},"source":["## **2.3. Tortuosity**\n","---\n","<font size = 4>This measure provides insight into the curvature and complexity of the path taken, with a value of 1 indicating a straight path between the start and end points, and values greater than 1 indicating paths with more twists and turns.\n","To calculate the tortuosity of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The tortuosity, denoted as \\(T\\), is calculated using the formula:\n","\n","$$ T = \\frac{d_{\\text{total path}}}{d_{\\text{euclidean}}} $$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"uLiAYGb2eKbC"},"outputs":[],"source":["# @title ##Calculate tortuosity\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","print(\"In progress...\")\n","\n","# Initialize tqdm with pandas\n","tqdm.pandas(desc=\"Calculating Tortuosity\")\n","\n","# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n","if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n","    spots_df_to_use = merged_spots_df\n","\n","def calculate_tortuosity(group):\n","    group = group.sort_values('POSITION_T')\n","\n","    # Apply spatial calibration to the coordinates\n","    calibrated_coords = group[['POSITION_X', 'POSITION_Y', 'POSITION_Z']].values\n","\n","    start_point = calibrated_coords[0]\n","    end_point = calibrated_coords[-1]\n","\n","    # Calculating Euclidean distance in 3D between start and end points\n","    euclidean_distance = np.linalg.norm(end_point - start_point)\n","\n","    # Calculating the total path length in 3D\n","    deltas = np.linalg.norm(np.diff(calibrated_coords, axis=0), axis=1)\n","    total_path_length = deltas.sum()\n","\n","    # Calculating Tortuosity\n","    T = total_path_length / euclidean_distance if euclidean_distance != 0 else 0\n","\n","    return pd.Series({'Tortuosity': T})\n","\n","# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n","spots_df_to_use.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","\n","# Calculate tortuosity for each track using progress_apply\n","df_tortuosity = spots_df_to_use.groupby('Unique_ID').progress_apply(calculate_tortuosity).reset_index()\n","\n","# Find the overlapping columns between the two DataFrames, excluding the merging key\n","overlapping_columns = merged_tracks_df.columns.intersection(df_tortuosity.columns).drop('Unique_ID')\n","\n","# Drop the overlapping columns from the left DataFrame\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","# Merge the tortuosity back into the original DataFrame\n","merged_tracks_df = pd.merge(merged_tracks_df, df_tortuosity, on='Unique_ID', how='left')\n","\n","# Save the DataFrame with the calculated tortuosity\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n","\n","# Safeguard: check for NaN\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","#check_for_nans(merged_spots_df, \"merged_spots_df\")\n","\n","print(\"...Done\")\n"]},{"cell_type":"markdown","metadata":{"id":"IoZC4xJVhkpe"},"source":["## **2.4. Calculate the total turning angle**\n","---\n","\n","<font size = 4>This measure provides insight into the cumulative amount of turning along the path, with a value of 0 indicating a straight path with no turning, and higher values indicating paths with more turning.\n","\n","<font size = 4>To calculate the Total Turning Angle of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The Total Turning Angle, denoted as \\(A\\), is the sum of the angles between each pair of consecutive direction vectors along the track, representing the cumulative amount of turning along the path.\n","\n","<font size = 4>For each pair of consecutive segments in the track, we calculate the direction vectors \\( $\\vec{v_1}$ \\) and \\($ \\vec{v_2}$ \\), and the angle \\($ \\theta$ \\) between them is calculated using the formula:\n","\n","$$ \\cos(\\theta) = \\frac{\\vec{v_1} \\cdot \\vec{v_2}}{||\\vec{v_1}|| \\cdot ||\\vec{v_2}||} $$\n","\n","<font size = 4>where \\( $\\vec{v_1} \\cdot$ $\\vec{v_2}$ \\) is the dot product of the direction vectors, and \\( $||\\vec{v_1}||$ \\) and \\( $||\\vec{v_2}||$ \\) are the magnitudes of the direction vectors. The Total Turning Angle \\( $A$ \\) is then the sum of all the angles \\( \\$theta$ \\) calculated between each pair of consecutive direction vectors along the track:\n","\n","$$ A = \\sum \\theta $$\n","<font size = 4>\n","If either of the direction vectors is a zero vector, the angle between them is undefined, and such cases are skipped in the calculation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1Sbn-9ephHlO"},"outputs":[],"source":["# @title ##Calculate the total turning angle\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","print(\"In progress...\")\n","\n","# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n","if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n","    spots_df_to_use = merged_spots_df\n","\n","# Initialize tqdm with pandas\n","tqdm.pandas(desc=\"Calculating Total Turning Angle\")\n","\n","# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n","if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n","    spots_df_to_use = merged_spots_df\n","\n","def calculate_total_turning_angle(group):\n","    group = group.sort_values('POSITION_T')\n","    directions = group[['POSITION_X', 'POSITION_Y', 'POSITION_Z']].diff().dropna()\n","    total_turning_angle = 0\n","\n","    for i in range(1, len(directions)):\n","        dir1 = directions.iloc[i - 1]\n","        dir2 = directions.iloc[i]\n","\n","        if np.linalg.norm(dir1) == 0 or np.linalg.norm(dir2) == 0:\n","            continue\n","\n","        cos_angle = np.dot(dir1, dir2) / (np.linalg.norm(dir1) * np.linalg.norm(dir2))\n","        cos_angle = np.clip(cos_angle, -1, 1)\n","        angle = np.degrees(np.arccos(cos_angle))\n","        total_turning_angle += angle\n","\n","    return pd.Series({'Total_Turning_Angle': total_turning_angle})\n","\n","# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n","spots_df_to_use.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","\n","# Calculate total turning angle for each track using progress_apply instead of apply\n","df_turning_angle = spots_df_to_use.groupby('Unique_ID').progress_apply(calculate_total_turning_angle).reset_index()\n","\n","# Check if 'Total_Turning_Angle' is in the columns of df_turning_angle\n","if 'Total_Turning_Angle' not in df_turning_angle.columns:\n","    print(\"Error: 'Total_Turning_Angle' not in df_turning_angle columns\")\n","\n","# Find the overlapping columns between the two DataFrames, excluding the merging key\n","overlapping_columns = merged_tracks_df.columns.intersection(df_turning_angle.columns).drop('Unique_ID')\n","\n","# Drop the overlapping columns from the left DataFrame\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","# Merge the total turning angle back into the original DataFrame\n","merged_tracks_df = pd.merge(merged_tracks_df, df_turning_angle, on='Unique_ID', how='left')\n","\n","# Save the DataFrame with the calculated total turning angle\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n","\n","# Safeguard: check for NaN\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","#check_for_nans(merged_spots_df, \"merged_spots_df\")\n","\n","print(\"...Done\")\n"]},{"cell_type":"markdown","metadata":{"id":"sYCKq5wWljf_"},"source":["## **2.5. Calculate the Spatial Coverage**\n","---\n","\n","<font size = 4>Spatial coverage provides insight into the spatial extent covered by the object's movement, with higher values indicating that the object has covered a larger area or volume during its movement.\n","\n","\n","<font size = 4>To calculate the spatial coverage of a track in 2D or 3D space, we consider a series of points each with \\(x\\), \\(y\\), and optionally \\(z\\) coordinates, sorted by time. The spatial coverage, denoted as \\(S\\), represents the area (in 2D) or volume (in 3D) enclosed by the convex hull formed by the points in the track. It provides insight into the spatial extent covered by the moving object.\n","\n","#### In the implementation below we:\n","1. <font size = 4>**Check Dimensionality**:\n","   <font size = 4>- If the variance of the \\(z\\) coordinates is zero, implying all \\(z\\) coordinates are the same, the spatial coverage is calculated in 2D using only the \\(x\\) and \\(y\\) coordinates.\n","  <font size = 4> - If the \\(z\\) coordinates vary, the spatial coverage is calculated in 3D using the \\(x\\), \\(y\\), and \\(z\\) coordinates.\n","\n","2. <font size = 4>**Form Convex Hull**:\n","   <font size = 4>- In 2D, a minimum of 3 non-collinear points is required to form a convex hull.\n","   <font size = 4>- In 3D, a minimum of 4 non-coplanar points is required to form a convex hull.\n","   <font size = 4>- If the required minimum points are not available, the spatial coverage is defined to be zero.\n","\n","3. <font size = 4>**Calculate Spatial Coverage**:\n","   <font size = 4>- In 2D, the spatial coverage \\(S\\) is the area of the convex hull formed by the points in the track.\n","   <font size = 4>- In 3D, the spatial coverage \\(S\\) is the volume of the convex hull formed by the points in the track.\n","\n","#### Formula:\n","- For 2D Spatial Coverage (Area of Convex Hull), if points are \\(P_1(x_1, y_1), P_2(x_2, y_2), \\ldots, P_n(x_n, y_n)\\):\n","  $$ S_{2D} = \\text{Area of Convex Hull formed by } P_1, P_2, \\ldots, P_n $$\n","\n","- For 3D Spatial Coverage (Volume of Convex Hull), if points are \\(P_1(x_1, y_1, z_1), P_2(x_2, y_2, z_2), \\ldots, P_n(x_n, y_n, z_n)\\):\n","  $$ S_{3D} = \\text{Volume of Convex Hull formed by } P_1, P_2, \\ldots, P_n $$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"A-46mr2inFhg"},"outputs":[],"source":["# @title ##Calculate the Spatial Coverage\n","\n","import pandas as pd\n","import numpy as np\n","from scipy.spatial import ConvexHull\n","from tqdm.notebook import tqdm\n","\n","print(\"In progress...\")\n","\n","# Initialize tqdm with pandas\n","tqdm.pandas(desc=\"Calculating Spatial Coverage\")\n","\n","# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n","if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n","    spots_df_to_use = merged_spots_df\n","\n","def calculate_spatial_coverage_with_calibration(group):\n","    group = group.sort_values('POSITION_T')\n","\n","    # Apply spatial calibration to the coordinates\n","    calibrated_coords = group[['POSITION_X', 'POSITION_Y', 'POSITION_Z']].values  # Ensure .values is added here\n","\n","    # Drop rows with NaN values in the coordinates\n","    calibrated_coords = calibrated_coords[~np.isnan(calibrated_coords).any(axis=1)]\n","\n","    # Check the variance of Z coordinates\n","    z_variance = np.var(calibrated_coords[:, 2])\n","\n","    if z_variance == 0:  # If variance of Z is 0, calculate 2D spatial coverage\n","        if len(calibrated_coords) < 3:  # Need at least 3 points for a 2D convex hull\n","            return pd.Series({'Spatial_Coverage': 0})\n","\n","        try:\n","            coords_2d = calibrated_coords[:, :2]  # Use only X and Y coordinates\n","            hull_2d = ConvexHull(coords_2d, qhull_options='QJ')  # 'QJ' joggles the input to avoid precision errors\n","            spatial_coverage = hull_2d.volume  # Area of the convex hull in 2D\n","        except Exception as e:\n","            print(f\"Error calculating 2D spatial coverage: {e}\")\n","            spatial_coverage = 0\n","    else:  # If variance of Z is not 0, calculate 3D spatial coverage\n","        if len(calibrated_coords) < 4:  # Need at least 4 points for a 3D convex hull\n","            return pd.Series({'Spatial_Coverage': 0})\n","\n","        try:\n","            hull = ConvexHull(calibrated_coords, qhull_options='QJ')  # 'QJ' joggles the input to avoid precision errors\n","            spatial_coverage = hull.volume  # Volume of the convex hull in 3D\n","        except Exception as e:\n","            print(f\"Error calculating 3D spatial coverage: {e}\")\n","            spatial_coverage = 0\n","\n","    return pd.Series({'Spatial_Coverage': spatial_coverage})\n","\n","\n","# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n","spots_df_to_use.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","\n","# Calculate spatial coverage for each track using progress_apply instead of apply\n","df_spatial_coverage = spots_df_to_use.groupby('Unique_ID').progress_apply(calculate_spatial_coverage_with_calibration).reset_index()\n","\n","# Find the overlapping columns between the two DataFrames, excluding the merging key\n","overlapping_columns = merged_tracks_df.columns.intersection(df_spatial_coverage.columns).drop('Unique_ID')\n","\n","# Drop the overlapping columns from the left DataFrame\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","# Merge the spatial coverage back into the original DataFrame\n","merged_tracks_df = pd.merge(merged_tracks_df, df_spatial_coverage, on='Unique_ID', how='left')\n","\n","# Save the DataFrame with the calculated spatial coverage\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n","\n","# Safeguard: check for NaN\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","#check_for_nans(merged_spots_df, \"merged_spots_df\")\n","\n","print(\"...Done\")\n"]},{"cell_type":"markdown","metadata":{"id":"FJHi3-zp3JTd"},"source":["--------\n","# **Part 3. Quality Control**\n","--------\n","\n","      \n","\n"]},{"cell_type":"markdown","source":["## **3.1. Assess if your dataset is balanced**\n","---\n","\n","In cell tracking and similar biological analyses, the balance of the dataset is important, particularly in ensuring that each biological repeat carries equal weight. Here's why this balance is essential:\n","\n","### Accurate Representation of Biological Variability\n","\n","- **Capturing True Biological Variation**: Biological repeats are crucial for capturing the natural variability inherent in biological systems. Equal weighting ensures that this variability is accurately represented.\n","- **Reducing Sampling Bias**: By balancing the dataset, we avoid overemphasizing the characteristics of any single repeat, which might not be representative of the broader biological context.\n","\n","If your data is too imbalanced, it may be useful to ensure that this does not shift your results.\n","\n"],"metadata":{"id":"dqu5RWoh8TNc"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# @title ##Check the number of track per condition per repeats\n","\n","\n","if not os.path.exists(f\"{Results_Folder}/QC\"):\n","    os.makedirs(f\"{Results_Folder}/QC\")\n","\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import os\n","\n","def count_tracks_by_condition_and_repeat(df, Results_Folder, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID'):\n","    \"\"\"\n","    Counts the number of unique tracks for each combination of condition and repeat in the given DataFrame and\n","    saves a stacked histogram plot as a PDF in the QC folder with annotations for each stack.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    Results_Folder (str): The base folder where the results will be saved.\n","    condition_col (str): The name of the column representing the condition. Default is 'Condition'.\n","    repeat_col (str): The name of the column representing the repeat. Default is 'Repeat'.\n","    track_id_col (str): The name of the column representing the track ID. Default is 'Unique_ID'.\n","    \"\"\"\n","    track_counts = df.groupby([condition_col, repeat_col])[track_id_col].nunique()\n","    track_counts_df = track_counts.reset_index()\n","    track_counts_df.rename(columns={track_id_col: 'Number_of_Tracks'}, inplace=True)\n","\n","    # Pivot the data for plotting\n","    pivot_df = track_counts_df.pivot(index=condition_col, columns=repeat_col, values='Number_of_Tracks').fillna(0)\n","\n","    # Plotting\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel('Number of Tracks')\n","    ax.set_title('Stacked Histogram of Track Counts per Condition and Repeat')\n","    ax.legend(title=repeat_col)\n","    ax.grid(axis='y', linestyle='--')\n","\n","    # Hide horizontal grid lines\n","    ax.yaxis.grid(False)\n","\n","    # Add number annotations on each stack\n","    for bar in bars.patches:\n","        ax.text(bar.get_x() + bar.get_width() / 2,\n","                bar.get_y() + bar.get_height() / 2,\n","                int(bar.get_height()),\n","                ha='center', va='center', color='black', fontweight='bold', fontsize=8)\n","\n","    # Save the plot as a PDF\n","    pdf_file = os.path.join(Results_Folder, 'Track_Counts_Histogram.pdf')\n","    plt.savefig(pdf_file, bbox_inches='tight')\n","    print(f\"Saved histogram to {pdf_file}\")\n","\n","    plt.show()\n","\n","    return track_counts_df\n","\n","result_df = count_tracks_by_condition_and_repeat(merged_tracks_df, f\"{Results_Folder}/QC\")\n","\n","\n"],"metadata":{"cellView":"form","id":"dlqqoWZlzoeL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3.2. Compute Similarity Metrics between Field of Views (FOV) and between Conditions and Repeats**\n","---\n","\n","<font size = 4>**Purpose**:\n","\n","<font size = 4>This section provides a set of tools to compute and visualize similarities between different field of views (FOV) based on selected track parameters. By leveraging hierarchical clustering, the resulting dendrogram offers a clear visualization of how different FOV, conditions, or repeats relate to one another. This tool is essential for:\n","\n","<font size = 4>1. **Quality Control**:\n","    - Ensuring that FOVs from the same condition or experimental setup are more similar to each other than to FOVs from different conditions.\n","    - Confirming that repeats of the same experiment yield consistent results and cluster together.\n","    \n","<font size = 4>2. **Data Integrity**:\n","    - Identifying potential outliers or anomalies in the dataset.\n","    - Assessing the overall consistency of the experiment and ensuring reproducibility.\n","\n","<font size = 4>**How to Use**:\n","\n","<font size = 4>1. **Track Parameters Selection**:\n","    - A list of checkboxes allows users to select which track parameters they want to consider for similarity calculations. By default, all parameters are selected. Users can deselect parameters that they believe might not contribute significantly to the similarity.\n","\n","<font size = 4>2. **Similarity Metric**:\n","    - Users can choose a similarity metric from a dropdown list. Options include cosine, euclidean, cityblock, jaccard, and correlation. The choice of similarity metric can influence the clustering results, so users might need to experiment with different metrics to see which one provides the most meaningful results.\n","\n","<font size = 4>3. **Linkage Method**:\n","    - Determines how the distance between clusters is calculated in the hierarchical clustering process. Different linkage methods can produce different dendrograms, so users might want to try various methods.\n","\n","<font size = 4>4. **Visualization**:\n","    - Once the parameters are selected, users can click on the \"Select the track parameters and visualize similarity\" button. This will compute the hierarchical clustering and display two dendrograms:\n","        - One dendrogram displays similarities between individual FOVs.\n","        - Another dendrogram aggregates the data based on conditions and repeats, providing a higher-level view of the similarities.\n","      \n"],"metadata":{"id":"oQbwNuLG737Q"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bojHIRXv5bnN"},"outputs":[],"source":["# @title ##Compute similarity metrics between FOV and between conditions and repeats\n","\n","import pandas as pd\n","import numpy as np\n","from scipy.spatial.distance import cosine\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import ipywidgets as widgets\n","from sklearn.metrics import pairwise_distances\n","from scipy.spatial.distance import pdist\n","\n","# Check and create \"QC\" folder\n","if not os.path.exists(f\"{Results_Folder}/QC\"):\n","    os.makedirs(f\"{Results_Folder}/QC\")\n","\n","# Columns to exclude\n","excluded_columns = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","\n","selected_df = pd.DataFrame()\n","\n","# Filter out non-numeric columns but keep 'File_name'\n","numeric_df = merged_tracks_df.select_dtypes(include=['float64', 'int64']).copy()\n","numeric_df['File_name'] = merged_tracks_df['File_name']\n","\n","# Create a list of column names excluding 'File_name'\n","column_names = [col for col in numeric_df.columns if col not in excluded_columns]\n","\n","# Create a checkbox for each column\n","checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in column_names]\n","\n","# Dropdown for similarity metrics\n","similarity_dropdown = widgets.Dropdown(\n","    options=['cosine', 'euclidean', 'cityblock', 'jaccard', 'correlation'],\n","    value='cosine',\n","    description='Similarity Metric:'\n",")\n","\n","# Dropdown for linkage methods\n","linkage_dropdown = widgets.Dropdown(\n","    options=['single', 'complete', 'average', 'ward'],\n","    value='single',\n","    description='Linkage Method:'\n",")\n","\n","# Arrange checkboxes in a 2x grid\n","grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n","\n","# Create a button to trigger the selection and visualization\n","button = widgets.Button(description=\"Select the track parameters and visualize similarity\", layout=widgets.Layout(width='400px'), button_style='info')\n","\n","# Define the button click event handler\n","def on_button_click(b):\n","    global selected_df  # Declare selected_df as global\n","\n","    # Get the selected columns from the checkboxes\n","    selected_columns = [box.description for box in checkboxes if box.value]\n","    selected_columns.append('File_name')  # Always include 'File_name'\n","\n","    # Extract the selected columns from the DataFrame\n","    selected_df = numeric_df[selected_columns]\n","\n","    # Check and print the percentage of NaNs for each selected column\n","    for column in selected_columns:\n","        if selected_df[column].isna().any():\n","            nan_percentage = selected_df[column].isna().mean() * 100\n","            print(\"Warning: NaN values found in the selected data.\")\n","            print(f\"{column}: {nan_percentage:.2f}%\")\n","            any_nan = True\n","            print(\"Proceeding to handle NaN values.\")\n","            selected_df = selected_df.dropna()\n","\n","\n","    # Aggregate the data by filename\n","    aggregated_by_filename = selected_df.groupby('File_name').mean(numeric_only=True)\n","\n","    # Aggregate the data by condition and repeat\n","    aggregated_by_condition_repeat = merged_tracks_df.groupby(['Condition', 'Repeat'])[selected_columns].mean(numeric_only=True)\n","\n","    # Compute condensed distance matrices\n","    distance_matrix_filename = pdist(aggregated_by_filename, metric=similarity_dropdown.value)\n","    distance_matrix_condition_repeat = pdist(aggregated_by_condition_repeat, metric=similarity_dropdown.value)\n","\n","    # Perform hierarchical clustering\n","    linked_filename = linkage(distance_matrix_filename, method=linkage_dropdown.value)\n","    linked_condition_repeat = linkage(distance_matrix_condition_repeat, method=linkage_dropdown.value)\n","\n","    annotation_text = f\"Similarity Method: {similarity_dropdown.value}, Linkage Method: {linkage_dropdown.value}\"\n","\n","        # Prepare the parameters dictionary\n","    similarity_params = {\n","        'Similarity Metric': similarity_dropdown.value,\n","        'Linkage Method': linkage_dropdown.value,\n","        'Selected Columns': ', '.join(selected_columns)\n","    }\n","\n","    # Save the parameters\n","    params_file_path = os.path.join(Results_Folder, \"QC/analysis_parameters.csv\")\n","    save_parameters(similarity_params, params_file_path, 'Similarity Metrics')\n","\n","    # Plot the dendrograms one under the other\n","    plt.figure(figsize=(10, 10))\n","\n","    # Dendrogram for individual filenames\n","    plt.subplot(2, 1, 1)\n","    dendrogram(linked_filename, labels=aggregated_by_filename.index, orientation='top', distance_sort='descending', leaf_rotation=90)\n","    plt.title(f'Dendrogram of Field of view Similarities\\n{annotation_text}')\n","\n","    # Dendrogram for aggregated data based on condition and repeat\n","    plt.subplot(2, 1, 2)\n","    dendrogram(linked_condition_repeat, labels=aggregated_by_condition_repeat.index, orientation='top', distance_sort='descending', leaf_rotation=90)\n","    plt.title(f'Dendrogram of Aggregated Similarities by Condition and Repeat\\n{annotation_text}')\n","\n","    plt.tight_layout()\n","\n","    # Save the dendrogram to a PDF\n","    pdf_pages = PdfPages(f\"{Results_Folder}/QC/Dendrogram_Similarities.pdf\")\n","\n","    # Save the current figure to the PDF\n","    pdf_pages.savefig()\n","\n","    # Close the PdfPages object to finalize the document\n","    pdf_pages.close()\n","\n","    plt.show()\n","\n","# Set the button click event handler\n","button.on_click(on_button_click)\n","\n","# Display the widgets\n","display(grid, similarity_dropdown, linkage_dropdown, button)\n"]},{"cell_type":"markdown","metadata":{"id":"11aD1AmQh7ST"},"source":["-------------------------------------------\n","\n","# **Part 4. Plot track parameters**\n","-------------------------------------------\n","\n","<font size = 4> In this section you can plot all the track parameters previously computed. Data and graphs are automatically saved in your result folder.\n","\n","<b>Note on Units:</b> The parameters plotted are in the unit of measurement you used when tracking your data.\n","</font>\n","\n","<font size=\"4\" color=\"red\">\n","<b>Results Storage:</b>\n","Results generated by in this section are saved  in the sub-folder named `track_parameters_plots` within your `Results_Folder`.\n"]},{"cell_type":"markdown","source":["##**Statistical analyses**\n","### Cohen's d (Effect Size):\n","<font size = 4>Cohen's d measures the size of the difference between two groups, normalized by their pooled standard deviation. Values can be interpreted as small (0 to 0.2), medium (0.2 to 0.5), or large (0.5 and above) effects. It helps quantify how significant the observed difference is, beyond just being statistically significant.\n","\n","### Randomization Test:\n","<font size = 4>This non-parametric test evaluates if observed differences between conditions could have arisen by random chance. It shuffles condition labels multiple times, recalculating the Cohen's d each time. The resulting p-value, which indicates the likelihood of observing the actual difference by chance, provides evidence against the null hypothesis: a smaller p-value implies stronger evidence against the null.\n","\n","### Bonferroni Correction:\n","<font size = 4>Given multiple comparisons, the Bonferroni Correction adjusts significance thresholds to mitigate the risk of false positives. By dividing the standard significance level (alpha) by the number of tests, it ensures that only robust findings are considered significant. However, it's worth noting that this method can be conservative, sometimes overlooking genuine effects."],"metadata":{"id":"hIa_MlR2Ktu-"}},{"cell_type":"markdown","metadata":{"id":"RdAeBwtVaRCv"},"source":["## **4.1. Plot your entire dataset**\n","--------"]},{"cell_type":"code","source":["# @title ##Plot track parameters (entire dataset)\n","\n","import ipywidgets as widgets\n","from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","import pandas as pd\n","import os\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/track_parameters_plots\"\n","Conditions = 'Condition'\n","df_to_plot = merged_tracks_df\n","\n","# Check and create necessary directories\n","folders = [\"pdf\", \"csv\"]\n","for folder in folders:\n","    dir_path = os.path.join(base_folder, folder)\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def create_condition_selector(df, column_name):\n","    conditions = df[column_name].unique()\n","    condition_selector = SelectMultiple(\n","        options=conditions,\n","        description='Conditions:',\n","        disabled=False,\n","        layout=Layout(width='100%')  # Adjusting the layout width\n","    )\n","    return condition_selector\n","\n","def display_condition_selection(df, column_name):\n","    condition_selector = create_condition_selector(df, column_name)\n","\n","    condition_accordion = Accordion(children=[VBox([condition_selector])])\n","    condition_accordion.set_title(0, 'Select Conditions')\n","    display(condition_accordion)\n","    return condition_selector\n","\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Conditions, Results_Folder, condition_selector):\n","\n","    plt.clf()  # Clear the current figure before creating a new plot\n","    print(\"Plotting in progress...\")\n","\n","  # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","  # Get selected conditions\n","    selected_conditions = condition_selector.value\n","    n_selected_conditions = len(selected_conditions)\n","\n","    if n_selected_conditions == 0:\n","        print(\"No conditions selected for plotting\")\n","        return\n","\n","# Use only selected and ordered conditions\n","    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n","\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df[Conditions].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = df[df[Conditions] == cond1][var]\n","        group2 = df[df[Conditions] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = abs(cohen_d(new_group1, new_group2))\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = df[[Conditions, var, 'Repeat', 'File_name' ]]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = df[var].quantile(0.25)\n","      Q3 = df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      ax_box.set_xticklabels(ax_box.get_xticklabels(), rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","condition_selector = display_condition_selection(df_to_plot, Conditions)\n","selectable_columns = get_selectable_columns(df_to_plot)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, base_folder, condition_selector))\n","display(button)"],"metadata":{"cellView":"form","id":"tklU69qJRndj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l9PtrXYM0mKY"},"source":["## **4.2. Plot a balanced dataset**\n","--------"]},{"cell_type":"markdown","metadata":{"id":"3S0qiuWGaYv4"},"source":["## **4.2.1. Downsample your dataset to ensure that it is balanced**\n","--------\n","\n","### Downsampling and Balancing Dataset\n","\n","This section of the notebook is dedicated to addressing imbalances in the dataset, which is crucial for ensuring the accuracy and reliability of the analysis. The cell bellow will downsample the dataset to balance the number of tracks across different conditions and repeats. It allows for reproducibility by including a `random_seed` parameter, which is set to 42 by default but can be adjusted as needed.\n","\n","All results from this section will be saved in the Balanced Dataset Directory created in your `Results_Folder`.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IVQAzHo6N8PG"},"outputs":[],"source":["# @title ##Run this cell to downsample and balance your dataset\n","\n","random_seed = 42  # @param {type: \"number\"}\n","\n","if not os.path.exists(f\"{Results_Folder}/Balanced_dataset\"):\n","    os.makedirs(f\"{Results_Folder}/Balanced_dataset\")\n","\n","\n","def count_tracks_by_condition_and_repeat(df, Results_Folder, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID'):\n","    \"\"\"\n","    Counts the number of unique tracks for each combination of condition and repeat in the given DataFrame and\n","    saves a stacked histogram plot as a PDF in the QC folder with annotations for each stack.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    Results_Folder (str): The base folder where the results will be saved.\n","    condition_col (str): The name of the column representing the condition. Default is 'Condition'.\n","    repeat_col (str): The name of the column representing the repeat. Default is 'Repeat'.\n","    track_id_col (str): The name of the column representing the track ID. Default is 'Unique_ID'.\n","    \"\"\"\n","    track_counts = df.groupby([condition_col, repeat_col])[track_id_col].nunique()\n","    track_counts_df = track_counts.reset_index()\n","    track_counts_df.rename(columns={track_id_col: 'Number_of_Tracks'}, inplace=True)\n","\n","    # Pivot the data for plotting\n","    pivot_df = track_counts_df.pivot(index=condition_col, columns=repeat_col, values='Number_of_Tracks').fillna(0)\n","\n","    # Plotting\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel('Number of Tracks')\n","    ax.set_title('Stacked Histogram of Track Counts per Condition and Repeat')\n","    ax.legend(title=repeat_col)\n","    ax.grid(axis='y', linestyle='--')\n","\n","    # Hide horizontal grid lines\n","    ax.yaxis.grid(False)\n","\n","    # Add number annotations on each stack\n","    for bar in bars.patches:\n","        ax.text(bar.get_x() + bar.get_width() / 2,\n","                bar.get_y() + bar.get_height() / 2,\n","                int(bar.get_height()),\n","                ha='center', va='center', color='black', fontweight='bold', fontsize=8)\n","\n","    # Save the plot as a PDF\n","    pdf_file = os.path.join(Results_Folder, 'Track_Counts_Histogram.pdf')\n","    plt.savefig(pdf_file, bbox_inches='tight')\n","    print(f\"Saved histogram to {pdf_file}\")\n","\n","    plt.show()\n","\n","    return track_counts_df\n","\n","def balance_dataset(df, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID', random_seed=None):\n","    \"\"\"\n","    Balances the dataset by downsampling tracks for each condition and repeat combination.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    condition_col (str): The name of the column representing the condition.\n","    repeat_col (str): The name of the column representing the repeat.\n","    track_id_col (str): The name of the column representing the track ID.\n","    random_seed (int, optional): The seed for the random number generator. Default is None.\n","\n","    Returns:\n","    pandas.DataFrame: A new DataFrame with balanced track counts.\n","    \"\"\"\n","    # Group by condition and repeat, and find the minimum track count\n","    min_track_count = df.groupby([condition_col, repeat_col])[track_id_col].nunique().min()\n","\n","    # Function to sample min_track_count tracks from each group\n","    def sample_tracks(group):\n","        return group.sample(n=min_track_count, random_state=random_seed)\n","\n","    # Apply sampling to each group and concatenate the results\n","    balanced_merged_tracks_df = df.groupby([condition_col, repeat_col]).apply(sample_tracks).reset_index(drop=True)\n","\n","    return balanced_merged_tracks_df\n","\n","balanced_merged_tracks_df = balance_dataset(merged_tracks_df, random_seed=random_seed)\n","result_df = count_tracks_by_condition_and_repeat(balanced_merged_tracks_df, f\"{Results_Folder}/Balanced_dataset\")\n","\n","check_for_nans(balanced_merged_tracks_df, \"balanced_merged_tracks_df\")\n","save_dataframe_with_progress(balanced_merged_tracks_df, Results_Folder + '/Balanced_dataset/merged_Tracks_balanced_dataset.csv')\n"]},{"cell_type":"markdown","metadata":{"id":"tzAsrJURz4E6"},"source":["## **4.2.2. Check if the downsampling has affected data distribution**\n","--------\n","\n","This section of the notebook generates a heatmap visualizing the Kolmogorov-Smirnov (KS) p-values for each numerical column in the dataset, comparing the distributions before and after downsampling. This heatmap serves as a tool for assessing the impact of downsampling on data quality, guiding decisions on whether the downsampled dataset is suitable for further analysis.\n","\n","#### Purpose of the Heatmap\n","- **KS Test:** The KS test is used to determine if two samples are drawn from the same distribution. In this context, it compares the distribution of each numerical column in the original dataset (`merged_tracks_df`) with its counterpart in the downsampled dataset (`balanced_merged_tracks_df`).\n","- **P-Value Interpretation:** The p-value indicates the probability that the two samples come from the same distribution. A higher p-value suggests a greater likelihood that the distributions are similar.\n","\n","#### Interpreting the Heatmap\n","- **Color Coding:** The heatmap uses a color gradient (from viridis) to represent the range of p-values. Darker colors indicate higher p-values.\n","- **P-Value Thresholds:**\n","  - **High P-Values (Lighter Areas):** Indicate that the downsampling process likely did not significantly alter the distribution of that numerical column for the specific condition-repeat group.\n","  - **Low P-Values (Darker Areas):** Suggest that the downsampling process may have affected the distribution significantly.\n","- **Varying P-Values:** Variations in color across different columns and rows help identify which specific numerical columns and condition-repeat groups are most affected by the downsampling.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"LUGDFw62QCbd"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy.stats import ks_2samp\n","\n","# @title ##Check if your downsampling has affected your data distribution\n","\n","def calculate_ks_p_value(df1, df2, column):\n","    \"\"\"\n","    Calculate the KS p-value for a given column between two dataframes.\n","\n","    Parameters:\n","    df1 (pandas.DataFrame): Original DataFrame.\n","    df2 (pandas.DataFrame): DataFrame after downsampling.\n","    column (str): Column name to compare.\n","\n","    Returns:\n","    float: KS p-value.\n","    \"\"\"\n","    return ks_2samp(df1[column].dropna(), df2[column].dropna())[1]\n","\n","# Identify numerical columns\n","numerical_columns = merged_tracks_df.select_dtypes(include=['int64', 'float64']).columns\n","\n","# Initialize a DataFrame to store KS p-values\n","ks_p_values = pd.DataFrame(columns=numerical_columns)\n","\n","# Iterate over each group and numerical column\n","for group, group_df in merged_tracks_df.groupby(['Condition', 'Repeat']):\n","    group_p_values = []\n","    balanced_group_df = balanced_merged_tracks_df[(balanced_merged_tracks_df['Condition'] == group[0]) & (balanced_merged_tracks_df['Repeat'] == group[1])]\n","    for column in numerical_columns:\n","        p_value = calculate_ks_p_value(group_df, balanced_group_df, column)\n","        group_p_values.append(p_value)\n","    ks_p_values.loc[f'Condition: {group[0]}, Repeat: {group[1]}'] = group_p_values\n","\n","# Maximum number of columns per heatmap\n","max_columns_per_heatmap = 20\n","\n","# Total number of columns\n","total_columns = len(ks_p_values.columns)\n","\n","# Calculate the number of heatmaps needed\n","num_heatmaps = -(-total_columns // max_columns_per_heatmap)  # Ceiling division\n","\n","# File path for the PDF\n","pdf_filepath = Results_Folder+'/Balanced_dataset/p-Value Heatmap.pdf'\n","\n","# Create a PDF file\n","with PdfPages(pdf_filepath) as pdf:\n","    # Loop through each subset of columns and create a heatmap\n","    for i in range(num_heatmaps):\n","        start_col = i * max_columns_per_heatmap\n","        end_col = min(start_col + max_columns_per_heatmap, total_columns)\n","\n","        # Subset of columns for this heatmap\n","        subset_columns = ks_p_values.columns[start_col:end_col]\n","\n","        # Create the heatmap for the subset of columns\n","        plt.figure(figsize=(12, 8))\n","        sns.heatmap(ks_p_values[subset_columns], cmap='viridis', vmax=0.5, vmin=0)\n","        plt.title(f'Kolmogorov-Smirnov P-Value Heatmap (Columns {start_col+1} to {end_col})')\n","        plt.xlabel('Numerical Columns')\n","        plt.ylabel('Condition-Repeat Groups')\n","        plt.tight_layout()\n","\n","        # Save the current figure to the PDF\n","        pdf.savefig()\n","        plt.show()\n","        plt.close()\n","\n","print(f\"Saved all heatmaps to {pdf_filepath}\")\n","\n","# Save the p-values to a CSV file\n","ks_p_values.to_csv(Results_Folder + '/Balanced_dataset/ks_p_values.csv')\n","print(\"Saved KS p-values to ks_p_values.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"kgoO61WY06ZK"},"source":["## **4.2.3. Plot your balanced dataset**\n","--------"]},{"cell_type":"code","source":["# @title ##Plot track parameters (balanced dataset)\n","\n","import ipywidgets as widgets\n","from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","import pandas as pd\n","import os\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/Balanced_dataset/track_parameters_plots\"\n","Conditions = 'Condition'\n","df_to_plot = balanced_merged_tracks_df\n","\n","# Check and create necessary directories\n","folders = [\"pdf\", \"csv\"]\n","for folder in folders:\n","    dir_path = os.path.join(base_folder, folder)\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def create_condition_selector(df, column_name):\n","    conditions = df[column_name].unique()\n","    condition_selector = SelectMultiple(\n","        options=conditions,\n","        description='Conditions:',\n","        disabled=False,\n","        layout=Layout(width='100%')  # Adjusting the layout width\n","    )\n","    return condition_selector\n","\n","def display_condition_selection(df, column_name):\n","    condition_selector = create_condition_selector(df, column_name)\n","\n","    condition_accordion = Accordion(children=[VBox([condition_selector])])\n","    condition_accordion.set_title(0, 'Select Conditions')\n","    display(condition_accordion)\n","    return condition_selector\n","\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Conditions, Results_Folder, condition_selector):\n","\n","    plt.clf()  # Clear the current figure before creating a new plot\n","    print(\"Plotting in progress...\")\n","\n","  # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","  # Get selected conditions\n","    selected_conditions = condition_selector.value\n","    n_selected_conditions = len(selected_conditions)\n","\n","    if n_selected_conditions == 0:\n","        print(\"No conditions selected for plotting\")\n","        return\n","\n","# Use only selected and ordered conditions\n","    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n","\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df[Conditions].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = df[df[Conditions] == cond1][var]\n","        group2 = df[df[Conditions] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = abs(cohen_d(new_group1, new_group2))\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = df[[Conditions, var, 'Repeat', 'File_name' ]]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = df[var].quantile(0.25)\n","      Q3 = df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      ax_box.set_xticklabels(ax_box.get_xticklabels(), rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","condition_selector = display_condition_selection(df_to_plot, Conditions)\n","selectable_columns = get_selectable_columns(df_to_plot)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, base_folder, condition_selector))\n","display(button)"],"metadata":{"cellView":"form","id":"1olzHCkHWJRy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6tFTER9rg2Id"},"source":["--------\n","# **Part 5. Explore your high-dimensional data using UMAP and HDBSCAN**\n","--------\n","\n","<font size = 4> The workflow provided below is inspired by [CellPlato](https://github.com/Michael-shannon/cellPLATO)"]},{"cell_type":"markdown","metadata":{"id":"_7bKh1LujZsP"},"source":["## **5.1. Choose the track metrics to use for clustering**\n","--------\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EP-kp5JkR3xh"},"outputs":[],"source":["# @title ##Choose the track metrics to use\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/Umap\"):\n","    os.makedirs(f\"{Results_Folder}/Umap\")\n","\n","\n","excluded_columns = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","\n","# Columns you want to always include\n","columns_to_include = ['File_name', 'Repeat', 'Condition', 'Unique_ID']\n","\n","selected_df = pd.DataFrame()\n","nan_columns = pd.DataFrame()\n","# Extract the columns you always want to include and ensure they exist in the original dataframe\n","saved_columns = {col: merged_tracks_df[col].copy() for col in columns_to_include if col in merged_tracks_df}\n","\n","# Filter out non-numeric columns\n","numeric_df = merged_tracks_df.select_dtypes(include=['float64', 'int64'])  # Selecting only numeric columns\n","\n","column_names = [col for col in numeric_df.columns if col not in excluded_columns]\n","\n","# Create a checkbox for each column\n","checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in column_names]\n","\n","# Arrange checkboxes in a 2x grid\n","grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n","\n","# Create a button to trigger the selection\n","button = widgets.Button(description=\"Select the track parameters\", layout=widgets.Layout(width='400px'), button_style='info')\n","\n","# Define the button click event handler\n","def on_button_click(b):\n","    global selected_df  # Declare selected_df as global\n","    global nan_columns\n","    # Get the selected columns from the checkboxes\n","    selected_columns = [box.description for box in checkboxes if box.value]\n","\n","    # Extract the selected columns from the DataFrame\n","    selected_df = numeric_df[selected_columns].copy()\n","\n","        # Prepare the parameters dictionary\n","    UMAP_params = {\n","        'Selected Columns': ', '.join(selected_columns)\n","    }\n","\n","    # Save the parameters\n","    params_file_path = os.path.join(Results_Folder, \"Umap/analysis_parameters.csv\")\n","    save_parameters(UMAP_params, params_file_path, 'UMAP')\n","\n","    # Add back the always-included columns to selected_df\n","    for col, data in saved_columns.items():\n","        selected_df.loc[:, col] = data\n","\n","    # Check if the DataFrame has any NaN values and print a warning if it does.\n","    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n","    if nan_columns:\n","        for col in nan_columns:\n","            selected_df = selected_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n","\n","    print(\"Done\")\n","\n","# Set the button click event handler\n","button.on_click(on_button_click)\n","\n","# Display the grid of checkboxes and the button\n","display(grid, button)\n"]},{"cell_type":"markdown","metadata":{"id":"GLlDuCKmlxvl"},"source":["## **5.2. UMAP**\n","---\n","\n","<font size = 4> The given code performs UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction on the merged tracks dataframe, focusing on its numeric columns, and visualizes the result. In the provided UMAP code, the parameters `n_neighbors`, `min_dist`, and `n_components` are crucial for determining the structure and appearance of the resulting low-dimensional representation of the data.\n","\n","<font size = 4>`n_neighbors`: This parameter controls how UMAP balances local versus global structure in the data. It determines the size of the local neighborhood UMAP will look at when learning the manifold structure of the data.\n","- A smaller value emphasizes the local structure of the data, potentially at the expense of the global structure.\n","- A larger value allows UMAP to consider more distant neighbors, emphasizing more on the global structure of the data.\n","- Typically, values in the range of 5 to 50 are chosen, depending on the density and scale of the data.\n","\n","<font size = 4>`min_dist`: This parameter controls how tightly UMAP is allowed to pack points together. It determines the minimum distance between points in the low-dimensional representation.\n","- Setting it to a low value will allow points to be packed more closely, potentially revealing clusters in the data.\n","- A higher value ensures that points are more spread out in the representation.\n","- Values usually range between 0 and 1.\n","\n","<font size = 4>`n_dimension`: This parameter determines the number of dimensions in the low-dimensional space that the data will be reduced to.\n","For visualization purposes, `n_dimension` is typically set to 2 or 3 to obtain 2D or 3D representations, respectively.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"h-HJIe9ug1bl"},"outputs":[],"source":["# @title ##Perform UMAP\n","import umap\n","import plotly.offline as pyo\n","\n","#@markdown ###UMAP parameters:\n","\n","n_neighbors = 20  # @param {type: \"number\"}\n","min_dist = 0  # @param {type: \"number\"}\n","n_dimension = 2  # @param {type: \"slider\", min: 1, max: 3}\n","\n","#@markdown ###Display parameters:\n","spot_size = 30 # @param {type: \"number\"}\n","\n","# Initialize UMAP object with the specified settings\n","reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_dimension, random_state=42)\n","# Exclude non-numeric columns when fitting UMAP\n","embedding = reducer.fit_transform(selected_df.drop(columns=columns_to_include))\n","# Create dynamic column names based on n_components\n","column_names = [f'UMAP dimension {i}' for i in range(1, n_dimension + 1)]\n","\n","# Extract the columns_to_include from selected_df\n","included_data = selected_df[columns_to_include].reset_index(drop=True)\n","\n","# Concatenate the UMAP embedding with the included columns\n","umap_df = pd.concat([pd.DataFrame(embedding, columns=column_names), included_data], axis=1)\n","\n","\n","# Check if the DataFrame has any NaN values and print a warning if it does.\n","nan_columns = umap_df.columns[umap_df.isna().any()].tolist()\n","\n","if nan_columns:\n","  warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n","  for col in nan_columns:\n","    umap_df = umap_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n","\n","  # Prepare the parameters dictionary\n","UMAP_params = {\n","        'n_neighbors': n_neighbors,\n","        'min_dist': min_dist,\n","        'n_dimension': n_dimension\n","    }\n","\n","    # Save the parameters\n","params_file_path = os.path.join(Results_Folder, \"Umap/analysis_parameters.csv\")\n","save_parameters(UMAP_params, params_file_path, 'UMAP')\n","\n","# Visualize the UMAP projection\n","plt.figure(figsize=(12, 10))\n","\n","# The plot will adjust automatically based on the n_components\n","if n_dimension == 2:\n","    sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=umap_df, palette='Set2', s=spot_size)\n","    plt.title('UMAP Projection of the Dataset')\n","    plt.savefig(f\"{Results_Folder}/Umap/umap_projection_2D.pdf\")  # Save 2D plot as PDF\n","    plt.show()\n","elif n_dimension == 1:\n","    sns.stripplot(x=column_names[0], hue='Condition', data=umap_df, palette='Set2', jitter=0.05, size=spot_size)\n","    plt.title('UMAP Projection of the Dataset')\n","    plt.savefig(f\"{Results_Folder}/Umap/umap_projection_1D.pdf\")  # Save 2D plot as PDF\n","    plt.show()\n","else:\n","    # umap_df should have columns like 'UMAP dimension 1', 'UMAP dimension 2', 'UMAP dimension 3', and 'condition'\n","    import plotly.express as px\n","    import pandas as pd\n","    import numpy as np\n","\n","    fig = px.scatter_3d(umap_df,\n","                    x='UMAP dimension 1',\n","                    y='UMAP dimension 2',\n","                    z='UMAP dimension 3',\n","                    color='Condition')\n","\n","    for trace in fig.data:\n","      trace.marker.size = spot_size  # You can set this to any desired value\n","\n","    fig.show()\n","    pyo.plot(fig, filename=f\"{Results_Folder}/Umap/umap_projection.html\", auto_open=False)"]},{"cell_type":"markdown","metadata":{"id":"hMx8DVRMmvKQ"},"source":["## **5.3. HDBSCAN**\n","---\n","\n","<font size = 4> The provided code employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify clusters within a dataset that has already undergone UMAP dimensionality reduction. HDBSCAN is utilized for its proficiency in determining the optimal number of clusters while managing varied densities within the data.\n","\n","<font size = 4>In the provided HDBSCAN code, the parameters `min_samples`, `min_cluster_size`, and `metric` are crucial for determining the structure and appearance of the resulting clusters in the data.\n","\n","<font size = 4>`min_samples`: This parameter primarily controls the degree to which the algorithm is willing to declare noise. It's the number of samples in a neighborhood for a point to be considered as a core point.\n","- A smaller value of `min_samples` makes the algorithm more prone to declaring points as part of a cluster, potentially leading to larger clusters and fewer noise points.\n","- A larger value makes the algorithm more conservative, resulting in more points declared as noise and smaller, more defined clusters.\n","- The choice of `min_samples` typically depends on the density of the data; denser datasets may require a larger value.\n","\n","<font size = 4>`min_cluster_size`: This parameter determines the smallest size grouping that you wish to consider a cluster.\n","- A smaller value will allow the formation of smaller clusters, whereas a larger value will prevent small isolated groups of points from being declared as clusters.\n","- The choice of `min_cluster_size` depends on the scale of the data and the desired level of granularity in the clustering.\n","\n","<font size = 4>`metric`: This parameter is the metric used for distance computation between data points, and it affects the shape of the clusters.\n","- The `euclidean` metric is a good starting point, and depending on the clustering results and the data type, it might be beneficial to experiment with different metrics.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PGdYdL7hnrpK"},"outputs":[],"source":["# @title ##Run to see more information about the available metrics\n","print(\"\"\"\n","Metric                   Description                                                               Suitable For\n","-------------------------------------------------------------------------------------------------------------------------------------------------------\n","Euclidean                Standard distance metric.                                                 Numerical data.\n","Manhattan                Sum of absolute differences.                                              Numerical/Categorical data.\n","Chebyshev                Maximum value of absolute differences.                                    Numerical data.\n","Minkowski                Generalization of Euclidean and Manhattan distance.                       Numerical data.\n","Bray-Curtis              Dissimilarity between sample sets.                                        Numerical data.\n","Canberra                 Weighted version of Manhattan distance.                                   Numerical data.\n","Mahalanobis              Distance between a point and a distribution.                              Numerical data.\n","\n","\"\"\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1VV75MUjixkQ"},"outputs":[],"source":["# @title ##Identify clusters using HDBSCAN\n","import hdbscan\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import pandas as pd\n","import numpy as np\n","\n","#@markdown ###HDBSCAN parameters:\n","clustering_data_source = 'umap'  # @param ['umap', 'raw']\n","min_samples = 20  # @param {type: \"number\"}\n","min_cluster_size = 50  # @param {type: \"number\"}\n","metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'braycurtis', 'canberra']\n","\n","#@markdown ###Display parameters:\n","spot_size = 30 # @param {type: \"number\"}\n","# Apply HDBSCAN\n","clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)  # You may need to tune these parameters\n","\n","if clustering_data_source == 'umap':\n","  if n_dimension == 1:\n","    clusterer.fit(umap_df[['UMAP dimension 1']])  # Use only one UMAP dimension for clustering\n","\n","  elif n_dimension == 2:\n","    clusterer.fit(umap_df[['UMAP dimension 1', 'UMAP dimension 2']])  # Use two UMAP dimensions for clustering\n","\n","  elif n_dimension == 3:\n","    clusterer.fit(umap_df[['UMAP dimension 1', 'UMAP dimension 2', 'UMAP dimension 3']])  # Use three UMAP dimensions for clustering\n","\n","else:\n","  clusterer.fit(selected_df.select_dtypes(include=['number']))\n","\n","# Add the cluster labels to your UMAP DataFrame\n","umap_df['Cluster_UMAP'] = clusterer.labels_\n","\n","# If the Cluster column already exists in merged_tracks_df, drop it to avoid duplications\n","if 'Cluster_UMAP' in merged_tracks_df.columns:\n","    merged_tracks_df.drop(columns='Cluster_UMAP', inplace=True)\n","\n","# Merge the Cluster column from umap_df to merged_tracks_df based on Unique_ID\n","merged_tracks_df = pd.merge(merged_tracks_df, umap_df[['Unique_ID', 'Cluster_UMAP']], on='Unique_ID', how='left')\n","\n","# Handle cases where some rows in merged_tracks_df might not have a corresponding cluster label\n","merged_tracks_df['Cluster_UMAP'].fillna(-1, inplace=True)  # Assigning -1 to cells that were not assigned to any cluster\n","\n","# Save the DataFrame with the identified clusters\n","merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n","\n","  # Prepare the parameters dictionary\n","UMAP_params = {\n","        'clustering_data_source': clustering_data_source,\n","        'min_samples': min_samples,\n","        'min_cluster_size': min_cluster_size,\n","        'metric': metric\n","    }\n","\n","    # Save the parameters\n","params_file_path = os.path.join(Results_Folder, \"Umap/analysis_parameters.csv\")\n","save_parameters(UMAP_params, params_file_path, 'HDBSCAN')\n","\n","# Plotting the results\n","if n_dimension == 1:\n","    plt.figure(figsize=(12, 6))\n","    sns.stripplot(data=umap_df, x='UMAP dimension 1', hue='Cluster_UMAP', palette='viridis', s=spot_size)\n","    plt.title('Clusters Identified by HDBSCAN (1D)')\n","    plt.xlabel('UMAP dimension 1')\n","    plt.ylabel('Count')\n","    plt.savefig(f\"{Results_Folder}/Umap/HDBSCAN_clusters_1D.pdf\")  # Save 1D histogram as PDF\n","    plt.show()\n","\n","if n_dimension == 2:\n","\n","  plt.figure(figsize=(12,10))\n","  sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', hue='Cluster_UMAP', palette='viridis', data=umap_df, s=spot_size)\n","  plt.title('Clusters Identified by HDBSCAN')\n","  plt.savefig(f\"{Results_Folder}/Umap/HDBSCAN_clusters_2D.pdf\")  # Save 2D plot as PDF\n","  plt.show()\n","\n","if n_dimension == 3:\n","\n","  fig = px.scatter_3d(umap_df,\n","                    x='UMAP dimension 1',\n","                    y='UMAP dimension 2',\n","                    z='UMAP dimension 3',\n","                    color='Cluster_UMAP')\n","\n","  for trace in fig.data:\n","    trace.marker.size = spot_size\n","\n","  fig.show()\n","  pyo.plot(fig, filename=f\"{Results_Folder}/Umap/HDBSCAN_clusters.html\", auto_open=False)"]},{"cell_type":"markdown","metadata":{"id":"eBFfmTo5e-Wv"},"source":["## **5.4. Fingerprint**\n","---\n","\n","<font size = 4>This section is designed to visualize the distribution of different clusters within each condition in a dataset, showing the 'fingerprint' of each cluster per condition."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"B8S0uwynjht1"},"outputs":[],"source":["# @title ##Plot the 'fingerprint' of each cluster per condition\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","# Group by 'Condition' and 'Cluster' and calculate the size of each group\n","cluster_counts = umap_df.groupby(['Condition', 'Cluster_UMAP']).size().reset_index(name='counts')\n","\n","# Calculate the total number of points per condition\n","total_counts = umap_df.groupby('Condition').size().reset_index(name='total_counts')\n","\n","# Merge the DataFrames on 'Condition' to calculate percentages\n","percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n","percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n","\n","# Save the percentage_df DataFrame as a CSV file\n","percentage_df.to_csv(Results_Folder+'/Umap/UMAP_percentage_results.csv', index=False)\n","\n","# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n","pivot_df = percentage_df.pivot(index='Condition', columns='Cluster_UMAP', values='percentage')\n","\n","# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n","pivot_df.fillna(0, inplace=True)\n","\n","# Initialize PDF\n","pdf_pages = PdfPages(Results_Folder+'/Umap/UMAP_Cluster_Fingerprint_Plot.pdf')\n","\n","# Plotting\n","fig, ax = plt.subplots(figsize=(10, 7))\n","pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n","plt.title('Percentage in each cluster per Condition')\n","plt.ylabel('Percentage')\n","plt.xlabel('Condition')\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","\n","# Save the figure to a PDF\n","pdf_pages.savefig(fig)\n","\n","# Close the PDF\n","pdf_pages.close()\n","\n","# Display the plot\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"huCM6iMWSd_j"},"source":["## **5.5. Understand your clusters using heatmaps**\n","--------\n","\n","<font size = 4>This section help visualize how different track parameters vary across the identified clusters. The approach is to display these variations using a heatmap, which offers a color-coded representation of the median values of each parameter for each cluster. This visualization technique can make it easier to spot differences or patterns among the clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"HGuLloitQN38"},"outputs":[],"source":["import os\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import pandas as pd\n","from scipy.stats import zscore\n","\n","# @title ##Plot track normalized track parameters based on clusters as an heatmap\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/Umap/Track_parameters\"):\n","    os.makedirs(f\"{Results_Folder}/Umap/Track_parameters\")\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","def heatmap_comparison(df, Results_Folder):\n","    # Get all the selectable columns\n","    variables_to_plot = get_selectable_columns(df)\n","\n","    # Drop rows where all elements are NaNs in the variables_to_plot columns\n","    df = df.dropna()\n","\n","    # Compute median for each variable across clusters\n","    median_values = df.groupby('Cluster_UMAP')[variables_to_plot].median().transpose()\n","\n","    # Normalize the median values using Z-score\n","    normalized_values = median_values.apply(zscore, axis=1)\n","\n","    # Plot the heatmap\n","    plt.figure(figsize=(16, 10))\n","    sns.heatmap(normalized_values, cmap='coolwarm', annot=True, linewidths=.5)\n","    plt.title(\"Z-score Normalized Median Values of Variables by Cluster\")\n","    plt.tight_layout()\n","\n","    # Save the heatmap\n","    plt.savefig(f\"{Results_Folder}/Umap/Track_parameters/Heatmap_Normalized_Median_Values_by_Cluster.pdf\")\n","    plt.show()\n","\n","    # Save the normalized median values data to CSV\n","    normalized_values.to_csv(f\"{Results_Folder}/Umap/Track_parameters/Normalized_Median_Values_by_Cluster.csv\")\n","\n","# Plot the heatmap directly\n","heatmap_comparison(merged_tracks_df, Results_Folder)\n"]},{"cell_type":"markdown","metadata":{"id":"5KWMzPMaR3OC"},"source":["## **5.6. Understand your clusters using box plots**\n","--------\n","\n","<font size = 4>The provided code aims to visually represent the distribution of different track parameters across the identified clusters. Specifically, for each parameter selected, a boxplot is generated to showcase the spread of its values across different clusters. This approach provides a comprehensive view of how each track parameter varies within and across the clusters.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"e2jMmapvz8Sh"},"outputs":[],"source":["import os\n","import itertools\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","from matplotlib.gridspec import GridSpec\n","import pandas as pd\n","import ipywidgets as widgets\n","\n","# @title ##Plot track parameters based on clusters\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/Umap/Track_parameters\"):\n","    os.makedirs(f\"{Results_Folder}/Umap/Track_parameters\")\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Results_Folder):\n","    print(\"Plotting in progress...\")\n","\n","    # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","    for var in variables_to_plot:\n","        # Extract data for the specific variable and cluster\n","        data_to_save = df[['Cluster_UMAP', var]]\n","\n","        # Save data for the plot to CSV\n","        data_to_save.to_csv(f\"{Results_Folder}/Umap/Track_parameters/{var}_data_by_Cluster.csv\", index=False)\n","\n","        plt.figure(figsize=(16, 10))\n","\n","        # Plotting\n","        sns.boxplot(x='Cluster_UMAP', y=var, data=df, color='lightgray')  # Boxplot by cluster\n","        sns.stripplot(x='Cluster_UMAP', y=var, data=df, jitter=True, alpha=0.2)  # Individual data points\n","\n","        plt.title(f\"{var} by Cluster\")\n","        plt.xlabel('Cluster_UMAP')\n","        plt.ylabel(var)\n","        plt.xticks(rotation=90)\n","        plt.tight_layout()\n","\n","        # Save the plot\n","        plt.savefig(f\"{Results_Folder}/Umap/Track_parameters/{var}_Boxplots_by_Cluster.pdf\")\n","        plt.show()\n","\n","selectable_columns = get_selectable_columns(merged_tracks_df)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","# Create and display the plot button\n","button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, merged_tracks_df, Results_Folder))\n","display(button)\n"]},{"cell_type":"markdown","source":["## **5.7. Plot track parameters for a selected cluster**\n","---"],"metadata":{"id":"dKU37w3jnuh_"}},{"cell_type":"code","source":["# @title ##Plot track parameters for a selected cluster\n","\n","\n","# Check and create \"cluster_plots\" folder\n","if not os.path.exists(f\"{Results_Folder}/Umap/cluster_plots\"):\n","    os.makedirs(f\"{Results_Folder}/Umap/cluster_plots\")\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/Umap/cluster_plots/pdf\"):\n","    os.makedirs(f\"{Results_Folder}/Umap/cluster_plots/pdf\")\n","\n","# Check and create \"csv\" folder\n","if not os.path.exists(f\"{Results_Folder}/Umap/cluster_plots/csv\"):\n","    os.makedirs(f\"{Results_Folder}/Umap/cluster_plots/csv\")\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","\n","def display_cluster_dropdown(df):\n","    # Extract unique clusters\n","    unique_clusters = df['Cluster_UMAP'].unique()\n","    cluster_dropdown = widgets.Dropdown(\n","        options=unique_clusters,\n","        description='Select Cluster:',\n","        disabled=False,\n","    )\n","    display(cluster_dropdown)\n","    return cluster_dropdown\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def plot_selected_vars(button, variable_checkboxes, cluster_dropdown, df, Results_Folder):\n","    selected_cluster = cluster_dropdown.value\n","    print(f\"Plotting in progress for Cluster {selected_cluster}...\")\n","\n","    # Attempt to filter the dataframe for the selected cluster\n","    filtered_df = df[df['Cluster_UMAP'] == selected_cluster]\n","\n","    # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df['Condition'].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/Umap/cluster_plots/pdf/Cluster_{selected_cluster}_{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = filtered_df[filtered_df['Condition'] == cond1][var]\n","        group2 = filtered_df[filtered_df['Condition'] == cond2][var]\n","\n","        original_d = cohen_d(group1, group2)\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = cohen_d(new_group1, new_group2)\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/Umap/cluster_plots/csv/Cluster_{selected_cluster}_{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = filtered_df[['Condition', var, 'Repeat', 'File_name', 'Cluster_UMAP']]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/Umap/cluster_plots/csv/Cluster_{selected_cluster}_{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = filtered_df[var].quantile(0.25)\n","      Q3 = filtered_df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x='Condition', y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x='Condition', y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var} for Cluster {selected_cluster}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      ax_box.set_xticklabels(ax_box.get_xticklabels(), rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"coolwarm\", cbar=True, square=True, ax=ax_d)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","selectable_columns = get_selectable_columns(merged_tracks_df)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","cluster_dropdown = display_cluster_dropdown(merged_tracks_df)\n","\n","#merged_tracks_df = merged_tracks_df.dropna\n","\n","# Create and display the plot button\n","button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, cluster_dropdown, merged_tracks_df, Results_Folder))\n","display(button)"],"metadata":{"cellView":"form","id":"2vWSaPCkoqws"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wI8bxErp4kAk"},"source":["## **5.8. Identify exemplar tracks from each clusters**\n","---\n","\n","<font size = 4>Exemplars, in the context of clustering analysis, refer to representative data points that are selected to encapsulate the essential characteristics of a cluster. They are often chosen because they are central or prototypical members of a cluster, making them valuable for summarizing the cluster's properties. In the provided code, exemplars are identified using the HDBSCAN clustering algorithm and marked within the dataset.\n","\n","<font size = 4>**Keep in mind that not all cluster will have examplar**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_4f4_I16ji5L"},"outputs":[],"source":["import plotly.express as px  # Importing plotly for 3D plots\n","\n","# @title ##Identify exemplar tracks using HDBSCAN\n","\n","#@markdown ###Display parameters:\n","spot_size = 30 # @param {type: \"number\"}\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/Umap/Examplar\"):\n","    os.makedirs(f\"{Results_Folder}/Umap/Examplar\")\n","\n","# Extracting exemplar points\n","exemplars = []\n","for exemplar in clusterer.exemplars_:\n","    exemplars.extend(exemplar)\n","\n","# Flatten the exemplars list of lists into a single list\n","flattened_exemplars = [index for sublist in exemplars for index in sublist]\n","\n","# Now pass the flattened list to iloc\n","exemplar_df = umap_df.iloc[flattened_exemplars]\n","\n","# Deduplicate exemplar_df based on 'Unique_ID'\n","exemplar_df = exemplar_df.drop_duplicates(subset='Unique_ID')\n","\n","# Create a new column in exemplar_df to indicate it's an exemplar\n","exemplar_df['Exemplar'] = 1\n","\n","# If the Exemplar column already exists in merged_tracks_df, drop it to avoid duplications\n","if 'Exemplar' in merged_tracks_df.columns:\n","    merged_tracks_df.drop(columns='Exemplar', inplace=True)\n","\n","# Merge the Exemplar column from exemplar_df to merged_tracks_df based on Unique_ID\n","merged_tracks_df = pd.merge(merged_tracks_df, exemplar_df[['Unique_ID', 'Exemplar']], on='Unique_ID', how='left')\n","\n","# Handle cases where some rows in merged_tracks_df might not have a corresponding Exemplar label\n","merged_tracks_df['Exemplar'].fillna(0, inplace=True)  # Assigning 0 to cells that were not identified as exemplars\n","\n","# Save the DataFrame with the identified clusters Exemplar label\n","merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n","\n","\n","# Plotting clusters and exemplar points\n","if n_dimension == 1:\n","    plt.figure(figsize=(12,10))\n","    sns.stripplot(x='UMAP dimension 1', hue='Cluster_UMAP', data=umap_df, palette='viridis', jitter=0.05, size=spot_size)\n","    sns.stripplot(x='UMAP dimension 1', color='red', label='Exemplars', data=exemplar_df, jitter=0.05, size=spot_size, marker='X')\n","    plt.title('Clusters and Exemplar tracks Identified by HDBSCAN')\n","    plt.show()\n","\n","elif n_dimension == 2:\n","    plt.figure(figsize=(12,10))\n","    sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', hue='Cluster_UMAP', palette='viridis', data=umap_df, s=spot_size)\n","    sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', color='red', label='Exemplars', data=exemplar_df, s=spot_size*2, marker='X')\n","    plt.title('Clusters and Exemplar tracks Identified by HDBSCAN')\n","    plt.show()\n","\n","elif n_dimension == 3:\n","    fig = px.scatter_3d(umap_df,\n","                        x='UMAP dimension 1',\n","                        y='UMAP dimension 2',\n","                        z='UMAP dimension 3',\n","                        color='Cluster_UMAP',\n","                        color_discrete_sequence=px.colors.qualitative.Vivid)\n","\n","    # Add a new column for coloring exemplars\n","    exemplar_df['ExemplarColor'] = 'Exemplar'\n","    exemplar_fig = px.scatter_3d(exemplar_df,\n","                                 x='UMAP dimension 1',\n","                                 y='UMAP dimension 2',\n","                                 z='UMAP dimension 3',\n","                                 color='ExemplarColor',\n","                                 color_discrete_map={'Exemplar':'red'})\n","\n","    for trace in fig.data:\n","        trace.marker.size = spot_size\n","\n","    for trace in exemplar_fig.data:\n","        trace.marker.size = spot_size\n","        trace.marker.symbol = 'x'\n","\n","    fig.add_trace(exemplar_fig.data[0])\n","    fig.show()\n","    pyo.plot(fig, filename=f\"{Results_Folder}/Umap/Examplar/HDBSCAN_examplar.html\", auto_open=False)\n"]},{"cell_type":"markdown","metadata":{"id":"QXIPUckHs5We"},"source":["## **5.9. See the exemplar tracks**\n","---"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","# @title ##Plot the examplar tracks for each cluster\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/Umap/Examplar\"):\n","    os.makedirs(f\"{Results_Folder}/Umap/Examplar\")\n","\n","\n","# Extracting actual indices for exemplar rows\n","exemplar_indices = umap_df.iloc[flattened_exemplars].index\n","\n","# Add a new column to umap_df to indicate if a point is an exemplar\n","umap_df['Exemplar'] = 0\n","\n","# Mark the rows corresponding to exemplars as 1\n","umap_df.loc[exemplar_indices, 'Exemplar'] = 1\n","\n","# Determine max and min coordinates from the DataFrame\n","min_x = merged_spots_df['POSITION_X'].min()\n","max_x = merged_spots_df['POSITION_X'].max()\n","min_y = merged_spots_df['POSITION_Y'].min()\n","max_y = merged_spots_df['POSITION_Y'].max()\n","\n","# Extract exemplars from the umap_df\n","exemplar_info = umap_df[umap_df['Exemplar'] == 1]\n","\n","# Determine the unique clusters from exemplar_info\n","clusters = exemplar_info['Cluster_UMAP'].unique()\n","\n","# Create a PDF object to save the plots\n","with PdfPages(f\"{Results_Folder}/Umap/Examplar/Examplar_tracks_Clusters.pdf\") as pdf:\n","\n","    # Iterate over each cluster\n","    for cluster in clusters:\n","\n","        # Start a new figure for this cluster\n","        plt.figure(figsize=(7, 7))\n","\n","        # Extract unique IDs for the current cluster from exemplar_info\n","        cluster_unique_ids = exemplar_info[exemplar_info['Cluster_UMAP'] == cluster]['Unique_ID'].tolist()\n","\n","        # For each unique ID in the cluster, plot the track\n","        for unique_id in cluster_unique_ids:\n","\n","            # Filter dataframe based on the unique ID\n","            unique_df = merged_spots_df[merged_spots_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","\n","            # Color code tracks based on 'Condition' using seaborn's color palette\n","            color = sns.color_palette('husl', n_colors=merged_spots_df['Condition'].nunique())[merged_spots_df['Condition'].unique().tolist().index(unique_df['Condition'].iloc[0])]\n","\n","            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2, color=color, label=unique_df['Condition'].iloc[0])\n","\n","            # Set title for the subplot\n","            plt.title(f'Coordinates for Cluster {cluster}')\n","\n","            # Limit the plot dimensions based on your data's extent\n","            plt.xlim(min_x - 1, max_x + 1)\n","            plt.ylim(min_y - 1, max_y + 1)\n","\n","            # Add legend to differentiate tracks based on condition\n","            plt.legend(loc='best')\n","\n","            plt.xlabel('POSITION_X')\n","            plt.ylabel('POSITION_Y')\n","\n","        # Save the figure in the PDF\n","        pdf.savefig()\n","\n","# Adjust layout to avoid overlap\n","        plt.tight_layout()\n","\n","        plt.show()\n"],"metadata":{"cellView":"form","id":"nYmx0-9pyPi_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqcLQtFHsu7m"},"source":["## **5.10. Find the exemplar on your raw images**\n","--------\n"]},{"cell_type":"markdown","source":["<font size = 4>This Python script serves as a user-friendly tool for visualizing exemplar tracks within your microscopy video.\n","\n","<font size = 4>To utilize it effectively, **users should provide the path to the directory containing the raw stacks of their data**.\n","<font size = 4>It's essential to ensure that these stack files have the same name as the\n","*  Use .tif or .tiff files only\n","*  It's essential to ensure that these tif files have the same name as the corresponding CSV file used in the analysis.\n","*   The Tif files can be in the same folder as your csv file\n","\n","<font size = 4>Additionally, users are required to **specify the pixel calibration** value to accurately scale the visualization.\n","*   Use the same calibration as the one used in TrackMate\n","\n","\n","<font size = 4>With these inputs, the script automates the retrieval of matching TIFF files, adjusts for pixel calibration, and overlays vital information on video frames. Users can interactively select a specific cluster and initiate the visualization process with a single click."],"metadata":{"id":"BwdT6nNOgEOc"}},{"cell_type":"code","source":["import requests\n","import zipfile\n","import os\n","from tqdm import tqdm\n","from tifffile import imread\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from ipywidgets import interact, interactive, widgets, Button, Output\n","from IPython.display import clear_output\n","\n","# @title ##Find your examplar tracks\n","\n","#@markdown ###Or provide the path to the folder containing your .tif files\n","\n","Video_path = ''  # @param {type: \"string\"}\n","\n","#@markdown ###Provide the pixel calibration\n","\n","Pixel_calibration = None # @param {type: \"number\"}\n","\n","# Error message widget\n","error_output = Output()\n","\n","# Function to display an error message\n","def display_error_message(message):\n","    with error_output:\n","        print(message)\n","\n","def refined_find_matching_tiff_file(directory, filename):\n","    \"\"\"Refined function to find a TIFF file that matches the given filename in the directory or its subdirectories.\"\"\"\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            # Use exact string match for more accurate matching\n","            if file == filename + '.tif' or file == filename + '.tiff':\n","                return os.path.join(root, file)\n","    return None\n","\n","def overlay_square_on_frame(frame, x, y, square_size=50, border_width=3):\n","    \"\"\"Overlay a red square on a single frame.\"\"\"\n","    overlaid_frame = frame.copy()\n","\n","    half_size = square_size // 2\n","\n","    # Define the coordinates for the top-left and bottom-right corners of the square\n","    top_left_x = max(0, x - half_size)\n","    top_left_y = max(0, y - half_size)\n","    bottom_right_x = min(frame.shape[1] - 1, x + half_size)\n","    bottom_right_y = min(frame.shape[0] - 1, y + half_size)\n","\n","    # Overlay the red border on the frame\n","    # Horizontal lines\n","    overlaid_frame[top_left_y:top_left_y+border_width, top_left_x:bottom_right_x] = np.max(frame)\n","    overlaid_frame[bottom_right_y-border_width:bottom_right_y, top_left_x:bottom_right_x] = np.max(frame)\n","\n","    # Vertical lines\n","    overlaid_frame[top_left_y:bottom_right_y, top_left_x:top_left_x+border_width] = np.max(frame)\n","    overlaid_frame[top_left_y:bottom_right_y, bottom_right_x-border_width:bottom_right_x] = np.max(frame)\n","\n","    return overlaid_frame\n","\n","# Function to visualize a track for a cluster\n","def visualize_track_for_cluster(cluster_number):\n","    # Filter merged_tracks_df for exemplars and the selected cluster\n","    exemplar_tracks = merged_tracks_df[(merged_tracks_df['Cluster_UMAP'] == cluster_number) & (merged_tracks_df['Exemplar'] == 1)]\n","\n","    if exemplar_tracks.empty:\n","        display_error_message(\"No exemplar found for this cluster.\")\n","        return\n","\n","    for idx, track in exemplar_tracks.iterrows():\n","        # Get the filename\n","        filename = track['File_name']\n","\n","        # Find the corresponding tiff file\n","        full_path = refined_find_matching_tiff_file(Video_path, filename)\n","\n","        if not full_path:\n","            display_error_message(f\"No matching .tif or .tiff file found for filename: {filename}\")\n","            continue\n","\n","        # Load the movie\n","        movie = imread(full_path)\n","\n","        if len(movie.shape) != 3:\n","            display_error_message(f\"Warning: The loaded movie from file '{filename}' is not 2D over time.\")\n","            continue\n","\n","        # Fetch the track coordinates from merged_spots_df and adjust for calibration\n","        track_id = track['Unique_ID']\n","        track_coordinates = merged_spots_df[merged_spots_df['Unique_ID'] == track_id][['POSITION_T', 'POSITION_X', 'POSITION_Y']].copy()\n","        track_coordinates['POSITION_X'] = track_coordinates['POSITION_X'] / Pixel_calibration\n","        track_coordinates['POSITION_Y'] = track_coordinates['POSITION_Y'] / Pixel_calibration\n","\n","        # Define a function to update the display based on the frame slider\n","        def update_display(frame_number):\n","            plt.figure(figsize=(10, 10))\n","            frame_with_square = movie[frame_number, :, :].copy()\n","            coords_for_frame = track_coordinates[track_coordinates['POSITION_T'] == frame_number]\n","            if not coords_for_frame.empty:\n","                x, y = int(coords_for_frame['POSITION_X'].values[0]), int(coords_for_frame['POSITION_Y'].values[0])\n","                frame_with_square = overlay_square_on_frame(frame_with_square, x, y)\n","            plt.imshow(frame_with_square, cmap='gray')\n","            plt.title(f\"Frame {frame_number} for Exemplar in Cluster {cluster_number} from file {filename}\")\n","            plt.show()\n","\n","        # Create a slider for frame navigation\n","        frame_slider = widgets.IntSlider(min=0, max=len(movie) - 1, description='Frame')\n","\n","        # Display the visualization with interactive for more reactive updates\n","        w = interactive(update_display, frame_number=frame_slider)\n","        display(w)  # This line explicitly displays the widget\n","        break  # Only display for the first matching exemplar for the sake of demonstration\n","\n","# Dropdown widget for cluster selection\n","clusters = merged_tracks_df['Cluster_UMAP'].unique()\n","cluster_dropdown = widgets.Dropdown(\n","    options=clusters,\n","    description='Select Cluster:',\n","    disabled=False,\n",")\n","\n","# Button to trigger visualization\n","plot_button = Button(description=\"Plot\")\n","\n","# Function to handle button click\n","def on_plot_button_click(b):\n","    cluster_number = cluster_dropdown.value\n","        # Clear the previous output\n","    clear_output()\n","\n","    display(cluster_dropdown)\n","    display(plot_button)\n","    display(error_output)\n","    visualize_track_for_cluster(cluster_number)\n","\n","# Bind the function to the button click event\n","plot_button.on_click(on_plot_button_click)\n","\n","# Display the widgets\n","display(cluster_dropdown)\n","display(plot_button)\n","display(error_output)\n"],"metadata":{"cellView":"form","id":"7znpsOefcFce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6zGL9f-tKA2"},"source":["## **5.11. Export movies of the exemplar tracks**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_cIPhYbGkCLM"},"outputs":[],"source":["# @title ##Export movies with the examplar tracks labelled\n","\n","import os\n","import numpy as np\n","from tifffile import imwrite\n","from tqdm.notebook import tqdm\n","import imageio\n","\n","def percentile_normalize_and_convert_uint8(image_sequence, low_percentile=1, high_percentile=99):\n","    \"\"\"\n","    Normalize the image sequence to 0-255 based on percentiles and convert to uint8.\n","\n","    Parameters:\n","    - image_sequence: The sequence of images to be normalized.\n","    - low_percentile: Lower percentile value used for normalization.\n","    - high_percentile: Higher percentile value used for normalization.\n","\n","    Returns:\n","    - Normalized image sequence in uint8 format.\n","    \"\"\"\n","    # Compute the percentiles\n","    min_val = np.percentile(image_sequence, low_percentile)\n","    max_val = np.percentile(image_sequence, high_percentile)\n","\n","    # Clip the values outside the percentiles and normalize\n","    normalized = 255 * (np.clip(image_sequence, min_val, max_val) - min_val) / (max_val - min_val)\n","\n","    return normalized.astype(np.uint8)\n","\n","# Function to find a TIFF file that matches the given filename in the directory or its subdirectories\n","def find_matching_tiff_file(directory, filename):\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            if file.startswith(filename) and (file.endswith('.tif') or file.endswith('.tiff')):\n","                return os.path.join(root, file)\n","    return None\n","\n","def overlay_square_on_frame(frame, x, y, square_size=50, border_width=3):\n","    \"\"\"Overlay a red square on a single frame.\"\"\"\n","    overlaid_frame = frame.copy()\n","\n","    half_size = square_size // 2\n","\n","    # Define the coordinates for the top-left and bottom-right corners of the square\n","    top_left_x = max(0, x - half_size)\n","    top_left_y = max(0, y - half_size)\n","    bottom_right_x = min(frame.shape[1] - 1, x + half_size)\n","    bottom_right_y = min(frame.shape[0] - 1, y + half_size)\n","\n","    # Overlay the red border on the frame\n","    # Horizontal lines\n","    overlaid_frame[top_left_y:top_left_y+border_width, top_left_x:bottom_right_x] = np.max(frame)\n","    overlaid_frame[bottom_right_y-border_width:bottom_right_y, top_left_x:bottom_right_x] = np.max(frame)\n","\n","    # Vertical lines\n","    overlaid_frame[top_left_y:bottom_right_y, top_left_x:top_left_x+border_width] = np.max(frame)\n","    overlaid_frame[top_left_y:bottom_right_y, bottom_right_x-border_width:bottom_right_x] = np.max(frame)\n","\n","    return overlaid_frame\n","\n","# Create a directory to store the exported videos\n","video_export_folder = Results_Folder + \"/Umap/Examplar/Exported_Videos\"\n","if not os.path.exists(video_export_folder):\n","    os.makedirs(video_export_folder)\n","\n","# Iterate over all exemplar tracks\n","# Iterate over all exemplar tracks\n","for idx, track in tqdm(merged_tracks_df[merged_tracks_df['Exemplar'] == 1].iterrows(), total=merged_tracks_df[merged_tracks_df['Exemplar'] == 1].shape[0]):\n","    # Get the filename and cluster number\n","    filename = track['File_name']\n","    cluster_num = track['Cluster_UMAP']\n","\n","    # Find the corresponding tiff file\n","    full_path = find_matching_tiff_file(Folder_path, filename)\n","\n","    if not full_path:\n","        print(f\"No matching .tif or .tiff file found for filename: {filename}\")\n","        continue\n","\n","    # Load the movie\n","    movie = imread(full_path)\n","\n","    # Check dimensions to ensure 2D video\n","    if len(movie.shape) != 3:\n","        print(f\"Skipping {filename} as it is not a 2D video.\")\n","        continue\n","\n","    # Fetch the track coordinates from merged_spots_df\n","    track_id = track['Unique_ID']\n","    track_coordinates = merged_spots_df[merged_spots_df['Unique_ID'] == track_id][['POSITION_T', 'POSITION_X', 'POSITION_Y']]\n","\n","    # Overlay the track on the video using the overlay_square_on_frame function\n","    for _, coord in track_coordinates.iterrows():\n","        frame_number = int(coord['POSITION_T'])\n","        x = int(coord['POSITION_X'] / Pixel_calibration)\n","        y = int(coord['POSITION_Y'] / Pixel_calibration)\n","        movie[frame_number] = overlay_square_on_frame(movie[frame_number], x, y)\n","\n","    # Incorporate the cluster number in the output filenames\n","    output_video_path_tiff = os.path.join(video_export_folder, f\"{filename}_Cluster_{cluster_num}_with_track.tiff\")\n","    output_video_path_mp4 = os.path.join(video_export_folder, f\"{filename}_Cluster_{cluster_num}_with_track.mp4\")\n","\n","    # Save the video with overlaid track as TIFF\n","    imwrite(output_video_path_tiff, movie)\n","\n","    # Normalize and convert the movie to uint8\n","    movie_uint8 = percentile_normalize_and_convert_uint8(movie)\n","\n","    # Convert and save as MP4\n","    imageio.mimwrite(output_video_path_mp4, movie_uint8, fps=10)\n","\n","print(\"Video export completed.\")\n","\n"]},{"cell_type":"markdown","source":["--------\n","# **Part 6. Explore your high-dimensional data using t-SNE and HDBSCAN**\n","--------"],"metadata":{"id":"42rnP6i0EBh5"}},{"cell_type":"markdown","source":["## **6.1. Choose the track metrics to use for clustering**\n","--------"],"metadata":{"id":"hY8yEB30ISY_"}},{"cell_type":"code","source":["# @title ##Choose the track metrics to use\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/Tsne\"):\n","    os.makedirs(f\"{Results_Folder}/Tsne\")\n","\n","\n","excluded_columns = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","\n","# Columns you want to always include\n","columns_to_include = ['File_name', 'Repeat', 'Condition', 'Unique_ID']\n","\n","selected_df = pd.DataFrame()\n","nan_columns = pd.DataFrame()\n","# Extract the columns you always want to include and ensure they exist in the original dataframe\n","saved_columns = {col: merged_tracks_df[col].copy() for col in columns_to_include if col in merged_tracks_df}\n","\n","# Filter out non-numeric columns\n","numeric_df = merged_tracks_df.select_dtypes(include=['float64', 'int64'])  # Selecting only numeric columns\n","\n","column_names = [col for col in numeric_df.columns if col not in excluded_columns]\n","\n","# Create a checkbox for each column\n","checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in column_names]\n","\n","# Arrange checkboxes in a 2x grid\n","grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n","\n","# Create a button to trigger the selection\n","button = widgets.Button(description=\"Select the track parameters\", layout=widgets.Layout(width='400px'), button_style='info')\n","\n","# Define the button click event handler\n","def on_button_click(b):\n","    global selected_df  # Declare selected_df as global\n","    global nan_columns\n","    # Get the selected columns from the checkboxes\n","    selected_columns = [box.description for box in checkboxes if box.value]\n","\n","    # Extract the selected columns from the DataFrame\n","    selected_df = numeric_df[selected_columns].copy()\n","\n","            # Prepare the parameters dictionary\n","    tsne_params = {\n","        'Selected Columns': ', '.join(selected_columns)\n","    }\n","\n","    # Save the parameters\n","    params_file_path = os.path.join(Results_Folder, \"Tsne/analysis_parameters.csv\")\n","    save_parameters(tsne_params, params_file_path, 'tsne')\n","\n","    # Add back the always-included columns to selected_df\n","    for col, data in saved_columns.items():\n","        selected_df.loc[:, col] = data\n","\n","    # Check if the DataFrame has any NaN values and print a warning if it does.\n","    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n","\n","    if nan_columns:\n","\n","        for col in nan_columns:\n","            selected_df = selected_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n","\n","    print(\"Done\")\n","\n","# Set the button click event handler\n","button.on_click(on_button_click)\n","\n","# Display the grid of checkboxes and the button\n","display(grid, button)\n","\n"],"metadata":{"cellView":"form","id":"w_J1ckCnES2Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6.2. t-SNE**\n","--------\n","\n","The code snippet provided performs **t-Distributed Stochastic Neighbor Embedding (t-SNE)**, a powerful technique for dimensionality reduction, particularly suited for the visualization of high-dimensional datasets. The process is applied to the merged tracks dataframe, focusing on its numeric columns, with the goal of visualizing the data in a lower-dimensional space.\n","\n","### Key Parameters of t-SNE:\n","\n","- **Perplexity (`perplexity`):**\n","  - This parameter is a measure of the effective number of local neighbors each point has.\n","  - Perplexity influences the t-SNE algorithm's ability to capture local versus global aspects of the data.\n","  - Typical values for perplexity range between 5 and 50, with the choice depending on dataset size and density.\n","\n","- **Learning Rate (`learning_rate`):**\n","  - This parameter controls the step size in the optimization process.\n","  - A suitable learning rate helps t-SNE to converge to a meaningful low-dimensional representation.\n","  - Values too high might cause the algorithm to converge to a suboptimal solution, while too low values can slow down the convergence.\n","\n","- **Number of Iterations (`n_iter`):**\n","  - This parameter defines the number of optimization iterations t-SNE will run.\n","  - A higher number of iterations allows the algorithm more time to find a stable configuration.\n","  - Generally, a value of 1000 iterations is sufficient for most datasets.\n","\n","- **Number of Dimensions (`n_dimension`):**\n","  - The target dimensionality for the lower-dimensional space.\n","  - For visualization purposes, this is commonly set to 2, allowing the data to be plotted in a 2D scatter plot.\n"],"metadata":{"id":"OBPMuhoYIV7A"}},{"cell_type":"code","source":["# @title ##Perform t-SNE\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import warnings\n","import pandas as pd\n","\n","# Check and create necessary directories\n","tsne_folder_path = f\"{Results_Folder}/Tsne/\"\n","if not os.path.exists(tsne_folder_path):\n","    os.makedirs(tsne_folder_path)\n","\n","#@markdown ###t-SNE parameters:\n","\n","perplexity = 50  # @param {type: \"number\"}\n","learning_rate = 200  # @param {type: \"number\"}\n","n_iter = 1000  # @param {type: \"number\"}\n","n_dimension = 2  # The number of dimensions is set to 2 for t-SNE as standard practice\n","\n","#@markdown ###Display parameters:\n","spot_size = 30  # @param {type: \"number\"}\n","\n","# Initialize t-SNE object with the specified settings\n","tsne = TSNE(n_components=n_dimension, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter, random_state=42)\n","\n","# Exclude non-numeric columns when fitting t-SNE\n","numeric_columns = selected_df._get_numeric_data()\n","embedding = tsne.fit_transform(numeric_columns)\n","\n","  # Prepare the parameters dictionary\n","tsne_params = {\n","        'perplexity': perplexity,\n","        'learning_rate': learning_rate,\n","        'n_iter': n_iter,\n","        'n_dimension': n_dimension,\n","        'spot_size': spot_size\n","    }\n","\n","    # Save the parameters\n","params_file_path = os.path.join(Results_Folder, \"Tsne/analysis_parameters.csv\")\n","save_parameters(tsne_params, params_file_path, 'tsne')\n","\n","# Create dynamic column names based on n_components\n","column_names = [f't-SNE dimension {i+1}' for i in range(n_dimension)]\n","\n","# Extract the columns_to_include from selected_df\n","included_data = selected_df[columns_to_include].reset_index(drop=True)\n","\n","# Concatenate the t-SNE embedding with the included columns\n","tsne_df = pd.concat([pd.DataFrame(embedding, columns=column_names), included_data], axis=1)\n","\n","# Check if the DataFrame has any NaN values and print a warning if it does.\n","nan_columns = tsne_df.columns[tsne_df.isna().any()].tolist()\n","if nan_columns:\n","  warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n","  tsne_df.dropna(subset=nan_columns, inplace=True)  # Drop NaN values only from columns containing them\n","\n","# Visualize the t-SNE projection\n","plt.figure(figsize=(12, 10))\n","sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=tsne_df, palette='Set2', s=spot_size)\n","plt.title('t-SNE Projection of the Dataset')\n","tsne_output_path = os.path.join(tsne_folder_path, 'tsne_projection_2D.pdf')\n","plt.savefig(tsne_output_path)  # Save 2D plot as PDF\n","plt.show()\n"],"metadata":{"cellView":"form","id":"p9vuULJXE1XG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6.3. HDBSCAN**\n","---\n","\n","<font size = 4> The provided code employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify clusters within a dataset that has already undergone UMAP dimensionality reduction. HDBSCAN is utilized for its proficiency in determining the optimal number of clusters while managing varied densities within the data.\n","\n","<font size = 4>In the provided HDBSCAN code, the parameters `min_samples`, `min_cluster_size`, and `metric` are crucial for determining the structure and appearance of the resulting clusters in the data.\n","\n","<font size = 4>`min_samples`: This parameter primarily controls the degree to which the algorithm is willing to declare noise. It's the number of samples in a neighborhood for a point to be considered as a core point.\n","- A smaller value of `min_samples` makes the algorithm more prone to declaring points as part of a cluster, potentially leading to larger clusters and fewer noise points.\n","- A larger value makes the algorithm more conservative, resulting in more points declared as noise and smaller, more defined clusters.\n","- The choice of `min_samples` typically depends on the density of the data; denser datasets may require a larger value.\n","\n","<font size = 4>`min_cluster_size`: This parameter determines the smallest size grouping that you wish to consider a cluster.\n","- A smaller value will allow the formation of smaller clusters, whereas a larger value will prevent small isolated groups of points from being declared as clusters.\n","- The choice of `min_cluster_size` depends on the scale of the data and the desired level of granularity in the clustering.\n","\n","<font size = 4>`metric`: This parameter is the metric used for distance computation between data points, and it affects the shape of the clusters.\n","- The `euclidean` metric is a good starting point, and depending on the clustering results and the data type, it might be beneficial to experiment with different metrics.\n"],"metadata":{"id":"-Cs7TLJAKiGR"}},{"cell_type":"code","source":["# @title ##Identify clusters using HDBSCAN\n","import hdbscan\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","#@markdown ###HDBSCAN parameters:\n","clustering_data_source = 'tsne'  # @param ['tsne', 'raw']\n","min_samples = 20  # @param {type: \"number\"}\n","min_cluster_size = 200  # @param {type: \"number\"}\n","metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'braycurtis', 'canberra']\n","\n","#@markdown ###Display parameters:\n","spot_size = 30 # @param {type: \"number\"}\n","\n","# Apply HDBSCAN\n","clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)\n","\n","\n","  # Prepare the parameters dictionary\n","tsne_params = {\n","        'clustering_data_source': clustering_data_source,\n","        'min_samples': min_samples,\n","        'min_cluster_size': min_cluster_size,\n","        'metric': metric\n","    }\n","\n","    # Save the parameters\n","params_file_path = os.path.join(Results_Folder, \"Tsne/analysis_parameters.csv\")\n","save_parameters(tsne_params, params_file_path, 'tsne')\n","\n","# Depending on the data source, we fit HDBSCAN to the t-SNE dimensions or the raw data\n","if clustering_data_source == 'tsne':\n","    # We only have two t-SNE dimensions based on the previous t-SNE code provided\n","    clusterer.fit(tsne_df[['t-SNE dimension 1', 't-SNE dimension 2']])\n","else:\n","    # If raw data is selected, we use all the numerical columns for clustering\n","    clusterer.fit(selected_df.select_dtypes(include=['number']))\n","\n","# Add the cluster labels to your t-SNE DataFrame\n","tsne_df['Cluster_tsne'] = clusterer.labels_\n","\n","# If the Cluster column already exists in merged_tracks_df, drop it to avoid duplications\n","if 'Cluster_tsne' in merged_tracks_df.columns:\n","    merged_tracks_df.drop(columns='Cluster_tsne', inplace=True)\n","\n","# Merge the Cluster column from tsne_df to merged_tracks_df based on Unique_ID\n","merged_tracks_df = pd.merge(merged_tracks_df, tsne_df[['Unique_ID', 'Cluster_tsne']], on='Unique_ID', how='left')\n","\n","# Handle cases where some rows in merged_tracks_df might not have a corresponding cluster label\n","merged_tracks_df['Cluster_tsne'].fillna(-1, inplace=True)  # Assigning -1 to cells that were not assigned to any cluster\n","\n","# Save the DataFrame with the identified clusters\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n","\n","# Plotting the results\n","plt.figure(figsize=(12, 10))\n","sns.scatterplot(x='t-SNE dimension 1', y='t-SNE dimension 2', hue='Cluster_tsne', palette='viridis', data=tsne_df, s=spot_size)\n","plt.title('Clusters Identified by HDBSCAN')\n","plt.savefig(os.path.join(Results_Folder, 'Tsne', 'HDBSCAN_clusters_2D.pdf'))  # Save 2D plot as PDF\n","plt.show()\n"],"metadata":{"cellView":"form","id":"HR_gR91rFW0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6.4 Fingerprint**\n","---\n","\n","<font size = 4>This section is designed to visualize the distribution of different clusters within each condition in a dataset, showing the 'fingerprint' of each cluster per condition."],"metadata":{"id":"2IVQnjaWK7sG"}},{"cell_type":"code","source":["# @title ##Plot the 'fingerprint' of each cluster per condition\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","# Group by 'Condition' and 'Cluster' and calculate the size of each group\n","cluster_counts = tsne_df.groupby(['Condition', 'Cluster_tsne']).size().reset_index(name='counts')\n","\n","# Calculate the total number of points per condition\n","total_counts = tsne_df.groupby('Condition').size().reset_index(name='total_counts')\n","\n","# Merge the DataFrames on 'Condition' to calculate percentages\n","percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n","percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n","\n","# Save the percentage_df DataFrame as a CSV file\n","percentage_df.to_csv(os.path.join(Results_Folder, 'Tsne', 'TSNE_percentage_results.csv'), index=False)\n","\n","# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n","pivot_df = percentage_df.pivot(index='Condition', columns='Cluster_tsne', values='percentage')\n","\n","# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n","pivot_df.fillna(0, inplace=True)\n","\n","# Initialize PDF\n","pdf_path = os.path.join(Results_Folder, 'Tsne', 'TSNE_Cluster_Fingerprint_Plot.pdf')\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Plotting\n","fig, ax = plt.subplots(figsize=(10, 7))\n","pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n","plt.title('Percentage in each cluster per Condition')\n","plt.ylabel('Percentage')\n","plt.xlabel('Condition')\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","\n","# Save the figure to a PDF\n","pdf_pages.savefig(fig)\n","\n","# Close the PDF\n","pdf_pages.close()\n","\n","# Display the plot\n","plt.show()\n"],"metadata":{"cellView":"form","id":"stDecXFJF4wu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6.5. Understand your clusters using heatmaps**\n","--------\n","<font size = 4>This section help visualize how different track parameters vary across the identified clusters. The approach is to display these variations using a heatmap, which offers a color-coded representation of the median values of each parameter for each cluster. This visualization technique can make it easier to spot differences or patterns among the clusters.\n"],"metadata":{"id":"DF1GADu0KuG4"}},{"cell_type":"code","source":["import os\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy.stats import zscore\n","import pandas as pd\n","\n","# @title ##Plot track normalized track parameters based on clusters as a heatmap\n","\n","# Create \"Tsne/Track_parameters\" directory if it doesn't exist\n","tsne_track_parameters_path = os.path.join(Results_Folder, 'Tsne', 'Track_parameters')\n","os.makedirs(tsne_track_parameters_path, exist_ok=True)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","def heatmap_comparison(df, Results_Folder):\n","    # Get all the selectable columns\n","    variables_to_plot = get_selectable_columns(df)\n","\n","    # Drop rows where all elements are NaNs in the variables_to_plot columns\n","    df = df.dropna()\n","\n","    # Compute median for each variable across clusters\n","    median_values = df.groupby('Cluster_tsne')[variables_to_plot].median().transpose()\n","\n","    # Normalize the median values using Z-score\n","    normalized_values = median_values.apply(zscore, axis=1)\n","\n","    # Plot the heatmap\n","    plt.figure(figsize=(16, 10))\n","    sns.heatmap(normalized_values, cmap='coolwarm', annot=True, linewidths=.5)\n","    plt.title(\"Z-score Normalized Median Values of Variables by Cluster\")\n","    plt.tight_layout()\n","\n","    # Save the heatmap to PDF\n","    heatmap_pdf_path = os.path.join(tsne_track_parameters_path, 'Heatmap_Normalized_Median_Values_by_Cluster.pdf')\n","    plt.savefig(heatmap_pdf_path)\n","    plt.show()\n","\n","    # Save the normalized median values data to CSV\n","    normalized_values_csv_path = os.path.join(tsne_track_parameters_path, 'Normalized_Median_Values_by_Cluster.csv')\n","    normalized_values.to_csv(normalized_values_csv_path)\n","\n","# Plot the heatmap directly\n","heatmap_comparison(merged_tracks_df, Results_Folder)\n"],"metadata":{"cellView":"form","id":"SbYuVPGyFzb4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6.6. Understand your clusters using box plots**\n","--------\n","<font size = 4>The provided code aims to visually represent the distribution of different track parameters across the identified clusters. Specifically, for each parameter selected, a boxplot is generated to showcase the spread of its values across different clusters. This approach provides a comprehensive view of how each track parameter varies within and across the clusters.\n","\n","\n"],"metadata":{"id":"89JgYyb7KUSO"}},{"cell_type":"code","source":["import os\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import ipywidgets as widgets\n","\n","# @title ##Plot track parameters based on clusters\n","\n","# Define paths for Tsne\n","tsne_track_parameters_path = os.path.join(Results_Folder, 'Tsne', 'Track_parameters')\n","os.makedirs(tsne_track_parameters_path, exist_ok=True)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(3, 300px)\")),\n","    ]))\n","    return variable_checkboxes\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Results_Folder):\n","    print(\"Plotting in progress...\")\n","\n","    # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","    for var in variables_to_plot:\n","        # Extract data for the specific variable and cluster\n","        data_to_save = df[['Cluster_tsne', var]]\n","\n","        # Save data for the plot to CSV\n","        data_to_save.to_csv(os.path.join(tsne_track_parameters_path, f\"{var}_data_by_Cluster.csv\"), index=False)\n","\n","        plt.figure(figsize=(16, 10))\n","\n","        # Plotting\n","        sns.boxplot(x='Cluster_tsne', y=var, data=df, color='lightgray')  # Boxplot by cluster\n","        sns.stripplot(x='Cluster_tsne', y=var, data=df, jitter=True, alpha=0.2)  # Individual data points\n","\n","        plt.title(f\"{var} by Cluster\")\n","        plt.xlabel('Cluster')\n","        plt.ylabel(var)\n","        plt.xticks(rotation=90)\n","        plt.tight_layout()\n","\n","        # Save the plot to PDF\n","        plt.savefig(os.path.join(tsne_track_parameters_path, f\"{var}_Boxplots_by_Cluster.pdf\"))\n","        plt.show()\n","\n","selectable_columns = get_selectable_columns(merged_tracks_df)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","# Create and display the plot button\n","button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, merged_tracks_df, Results_Folder))\n","display(button)\n"],"metadata":{"cellView":"form","id":"j9TYO-_qFqvy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6.7 Plot track parameters for a selected cluster**\n","---"],"metadata":{"id":"-40pLxGPnSZ7"}},{"cell_type":"code","source":["# @title ##Plot track parameters for a selected cluster\n","\n","\n","# Check and create \"cluster_plots\" folder\n","if not os.path.exists(f\"{Results_Folder}/Tsne/cluster_plots\"):\n","    os.makedirs(f\"{Results_Folder}/Tsne/cluster_plots\")\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/Tsne/cluster_plots/pdf\"):\n","    os.makedirs(f\"{Results_Folder}/Tsne/cluster_plots/pdf\")\n","\n","# Check and create \"csv\" folder\n","if not os.path.exists(f\"{Results_Folder}/Tsne/cluster_plots/csv\"):\n","    os.makedirs(f\"{Results_Folder}/Tsne/cluster_plots/csv\")\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'Cluster_tsne', 'TRACK_Z_LOCATION']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","\n","def display_cluster_dropdown(df):\n","    # Extract unique clusters\n","    unique_clusters = df['Cluster_tsne'].unique()\n","    cluster_dropdown = widgets.Dropdown(\n","        options=unique_clusters,\n","        description='Select Cluster:',\n","        disabled=False,\n","    )\n","    display(cluster_dropdown)\n","    return cluster_dropdown\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def plot_selected_vars(button, variable_checkboxes, cluster_dropdown, df, Results_Folder):\n","    selected_cluster = cluster_dropdown.value\n","    print(f\"Plotting in progress for Cluster {selected_cluster}...\")\n","\n","    # Attempt to filter the dataframe for the selected cluster\n","    filtered_df = df[df['Cluster_tsne'] == selected_cluster]\n","\n","    # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df['Condition'].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/Tsne/cluster_plots/pdf/Cluster_{selected_cluster}_{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = filtered_df[filtered_df['Condition'] == cond1][var]\n","        group2 = filtered_df[filtered_df['Condition'] == cond2][var]\n","\n","        original_d = cohen_d(group1, group2)\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = cohen_d(new_group1, new_group2)\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/Tsne/cluster_plots/csv/Cluster_{selected_cluster}_{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = filtered_df[['Condition', var, 'Repeat', 'File_name', 'Cluster_tsne']]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/Tsne/cluster_plots/csv/Cluster_{selected_cluster}_{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = filtered_df[var].quantile(0.25)\n","      Q3 = filtered_df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x='Condition', y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x='Condition', y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var} for Cluster {selected_cluster}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      ax_box.set_xticklabels(ax_box.get_xticklabels(), rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"coolwarm\", cbar=True, square=True, ax=ax_d)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","selectable_columns = get_selectable_columns(merged_tracks_df)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","cluster_dropdown = display_cluster_dropdown(merged_tracks_df)\n","\n","#merged_tracks_df = merged_tracks_df.dropna\n","\n","# Create and display the plot button\n","button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, cluster_dropdown, merged_tracks_df, Results_Folder))\n","display(button)"],"metadata":{"cellView":"form","id":"5AwcVmBBnKkH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YheRveJdrFyP"},"source":["# **Part 7. Version log**\n","---\n","<font size = 4>While I strive to provide accurate and helpful information, please be aware that:\n","  - This notebook may contain bugs.\n","  - Features are currently limited and will be expanded in future releases.\n","\n","<font size = 4>We encourage users to report any issues or suggestions for improvement. Please check the [repository](https://github.com/guijacquemet/CellTracksColab) regularly for updates and the latest version of this notebook.\n","\n","#### **Known Issues**:\n","- Tracks are displayed in 2D in section 1.4\n","\n","<font size = 4>**Version 0.9**\n","  - Improved plotting strategy. Specific conditions can be chosen\n","  - absolute cohen d values are now shown\n","  - In the QC the heatmap is automatically divided in subplot when too many columns are in the df\n","\n","<font size = 4>**Version 0.8**\n","  - Settings are now saved\n","  - Order of the section has been modified to help streamline biological discoveries\n","  - New section added to quality Control to check if the dataset is balanced\n","  - New section added to the UMAP and tsne section to plot track parameters for selected clusters\n","  - clusters for UMAP and t-sne are now saved in the dataframe separetly\n","\n","<font size = 4>**Version 0.7**\n","  - check_for_nans function added\n","  - Clustering using t-SNE added\n","\n","<font size = 4>**Version 0.6**\n","  - Improved organisation of the results\n","  - Tracks visualisation are now saved\n","\n","<font size = 4>**Version 0.5**\n","  - Improved part 5\n","  - Added the possibility to find examplar on the raw movies when available\n","  - Added the possibility to export video with the examplar labeled\n","  - Code improved to deal with larger dataset (tested with over 50k tracks)\n","  - test dataset now contains raw video and is hosted on Zenodo\n","  - Results are now organised in folders\n","  - Added progress bars\n","  - Minor code fixes\n","\n","<font size = 4>**Version 0.4**\n","\n","  - Added the possibility to filter and smooth tracks\n","  - Added spatial and temporal calibration\n","  - Notebook is streamlined\n","  - multiple bug fix\n","  - Remove the t-sne\n","  - Improved documentation\n","\n","<font size = 4>**Version 0.3**\n","  - Fix a nasty bug in the import functions\n","  - Add basic examplar for UMAP\n","  - Added the statistical analyses and their explanations.\n","  - Added a new quality control part that helps assessing the similarity of results between FOV, conditions and repeats\n","  - Improved part 5 (previously part 4).\n","\n","<font size = 4>**Version 0.2**\n","  - Added support for 3D tracks\n","  - New documentation and metrics added.\n","\n","<font size = 4>**Version 0.1**\n","This is the first release of this notebook.\n","\n","---"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["UlUCXg5QUC4f"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}