{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF4zYMmXULP7"
   },
   "source": [
    "# **CellTracksColab - Track clustering analysis**\n",
    "---\n",
    "<font size = 4>Explore Spatial Clustering in Track Data with CellTracksColab: This Colab Notebook is designed to help you determine whether tracks exhibit spatial clustering. Before beginning, ensure that your data is properly loaded in the CellTracksColab format for optimal analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JrkfFr7mgZmA"
   },
   "outputs": [],
   "source": [
    "# @title #MIT License\n",
    "\n",
    "print(\"\"\"\n",
    "**MIT License**\n",
    "\n",
    "Copyright (c) 2023 Guillaume Jacquemet\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "# **Part 0. Prepare the Google Colab session (skip this section when using a local installation)**\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h0prdayn0qG"
   },
   "source": [
    "## **0.1. Install key dependencies**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rAP0ahCzn1V6"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Play to install\n",
    "!git clone https://github.com/CellMigrationLab/CellTracksColab.git\n",
    "!pip install -r \"CellTracksColab/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Kzd_8GUnpbw"
   },
   "source": [
    "## **0.2. Mount your Google Drive**\n",
    "---\n",
    "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
    "\n",
    "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
    "\n",
    "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GA1wCrkoV4i5"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "# **Part 1. Prepare the session and load the data**\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Load key dependencies**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uiuJFVIsXOsl"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Play to load the dependancies\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Current version of the notebook the user is running\n",
    "current_version = \"1.0.1\"\n",
    "Notebook_name = 'Track_Clustering'\n",
    "\n",
    "# URL to the raw content of the version file in the repository\n",
    "version_url = \"https://raw.githubusercontent.com/guijacquemet/CellTracksColab/main/Notebook/latest_version.txt\"\n",
    "\n",
    "# Function to define colors for formatting messages\n",
    "class bcolors:\n",
    "    WARNING = '\\033[91m'  # Red color for warning messages\n",
    "    ENDC = '\\033[0m'      # Reset color to default\n",
    "\n",
    "# Check if this is the latest version of the notebook\n",
    "try:\n",
    "    All_notebook_versions = pd.read_csv(version_url, dtype=str)\n",
    "    print('Notebook version: ' + current_version)\n",
    "\n",
    "    # Check if 'Version' column exists in the DataFrame\n",
    "    if 'Version' in All_notebook_versions.columns:\n",
    "        Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Notebook_name]['Version'].iloc[0]\n",
    "        print('Latest notebook version: ' + Latest_Notebook_version)\n",
    "\n",
    "        if current_version == Latest_Notebook_version:\n",
    "            print(\"This notebook is up-to-date.\")\n",
    "        else:\n",
    "            print(bcolors.WARNING + \"A new version of this notebook has been released. We recommend that you download it at https://github.com/guijacquemet/CellTracksColab\" + bcolors.ENDC)\n",
    "    else:\n",
    "        print(\"The 'Version' column is not present in the version file.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Unable to fetch the latest version information. Please check your internet connection.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "#----------------------- Key functions -----------------------------#\n",
    "\n",
    "# Function to calculate Cohen's d\n",
    "def cohen_d(group1, group2):\n",
    "    diff = group1.mean() - group2.mean()\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    return d\n",
    "\n",
    "def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n",
    "    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n",
    "\n",
    "    # Estimating the number of chunks based on the provided chunk size\n",
    "    num_chunks = int(len(df) / chunk_size) + 1\n",
    "\n",
    "    # Create a tqdm instance for progress tracking\n",
    "    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n",
    "        # Open the file for writing\n",
    "        with open(path, \"w\") as f:\n",
    "            # Write the header once at the beginning\n",
    "            df.head(0).to_csv(f, index=False)\n",
    "\n",
    "            for chunk in np.array_split(df, num_chunks):\n",
    "                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "def check_for_nans(df, df_name):\n",
    "    \"\"\"\n",
    "    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame to be checked for NaN values.\n",
    "    df_name (str): The name of the DataFrame as a string, used for printing.\n",
    "    \"\"\"\n",
    "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
    "    nan_columns = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "    if nan_columns:\n",
    "        for col in nan_columns:\n",
    "            nan_count = df[col].isna().sum()\n",
    "            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n",
    "    else:\n",
    "        print(f\"No NaN values found in {df_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsDAwkSOo1gV"
   },
   "source": [
    "## **1.2.Load your CellTracksColab dataset**\n",
    "---\n",
    "\n",
    "<font size = 4> Please ensure that your data was properly processed using CellTracksColab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This section is the code is to test the same functions in all notebooks. \n",
    "## Once they work, we can copy paste all the code in the Part 1. Load Key dependencies.\n",
    "import sys\n",
    "# The following paths may vary a bit locally dependending on where the jupyter lab is running. \n",
    "# It's basically the path to the github repository, also in colab\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"CellTracksColab/\")\n",
    "import celltracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "CQKXq3giI3nX"
   },
   "outputs": [],
   "source": [
    "#@markdown ###Provide the path to your Track table\n",
    "\n",
    "Track_table = ''  # @param {type: \"string\"}\n",
    "\n",
    "#@markdown ###Provide the path to your Result folder\n",
    "\n",
    "Results_Folder = \"\"  # @param {type: \"string\"}\n",
    "\n",
    "if not os.path.exists(Results_Folder):\n",
    "    os.makedirs(Results_Folder, exist_ok=True)  # Create Results_Folder if it doesn't exist\n",
    "\n",
    "# Print the location of the result folder\n",
    "print(f\"Result folder is located at: {Results_Folder}\")\n",
    "\n",
    "# For existing dataframes\n",
    "\n",
    "print(\"Loading track table file....\")\n",
    "merged_tracks_df = pd.read_csv(Track_table, low_memory=False)\n",
    "Data_Dims = \"2D\" #@param [\"2D\", \"3D\"]\n",
    "Data_Type = \"TrackMate Files\"\n",
    "Track_table = ''  # @param {type: \"string\"}\n",
    "Spot_table = ''  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "#@markdown ###Or use a test dataset (up to 10 min download)\n",
    "Use_test_dataset = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Update the parameters to load the data\n",
    "CellTracks = celltracks.TrackingData()\n",
    "if Use_test_dataset:\n",
    "    # Download the test dataset\n",
    "    T.DownloadTestData()\n",
    "else:\n",
    "    CellTracks.Folder_path = Folder_path\n",
    "    if Data_Type == \"TrackMate Table\":\n",
    "            CellTracks.Spot_table = Spot_table\n",
    "            CellTracks.Track_table = Track_table\n",
    "    \n",
    "CellTracks.Results_Folder = Results_Folder\n",
    "CellTracks.data_type = Data_Type\n",
    "CellTracks.data_dims = Data_Dims\n",
    "\n",
    "# Load data\n",
    "CellTracks.LoadTrackingData()\n",
    "merged_spots_df = CellTracks.spots_data\n",
    "merged_tracks_df = CellTracks.tracks_data\n",
    "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n",
    "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n",
    "print(\"...Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52STmnv43d45"
   },
   "source": [
    "## **1.4. Visualise your tracks**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AE881uJW5ukQ"
   },
   "outputs": [],
   "source": [
    "# @title ##Run the cell and choose the file you want to inspect\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not os.path.exists(Results_Folder+\"/Tracks\"):\n",
    "    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n",
    "\n",
    "# Extract unique filenames from the dataframe\n",
    "filenames = merged_spots_df['File_name'].unique()\n",
    "\n",
    "# Create a Dropdown widget with the filenames\n",
    "filename_dropdown = widgets.Dropdown(\n",
    "    options=filenames,\n",
    "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
    "    description='File Name:',\n",
    ")\n",
    "\n",
    "def plot_coordinates(filename):\n",
    "    if filename:\n",
    "        # Filter the DataFrame based on the selected filename\n",
    "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for unique_id in filtered_df['Unique_ID'].unique():\n",
    "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
    "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n",
    "\n",
    "        plt.xlabel('POSITION_X')\n",
    "        plt.ylabel('POSITION_Y')\n",
    "        plt.title(f'Coordinates for {filename}')\n",
    "        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid filename selected\")\n",
    "\n",
    "# Link the Dropdown widget to the plotting function\n",
    "interact(plot_coordinates, filename=filename_dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eIekawNM9aS"
   },
   "source": [
    "# **Part 2: Assess spatial clustering using Ripley's L function**\n",
    "\n",
    "<font size = 4>In the specific spatial analysis being performed here, the choice of a single point within each track serves to focus on key moments or characteristics of object movement that are particularly relevant to the research objectives. For instance, when analyzing spatial distribution patterns of tracked objects within each field of view (FOV), different analysis points such as the beginning, end, middle, average, or median point of each track offer unique insights. Selecting the \"beginning\" point might help identify where objects enter an area, while the \"end\" point can indicate exit locations. Choosing the \"middle\" point provides insights into where objects spend a significant portion of their time. On the other hand, the \"average\" or \"median\" point offers a summary of the overall movement tendencies within each track. By accommodating these various analysis point options, researchers can tailor their spatial analysis to uncover specific aspects of object distribution that are most pertinent to their research questions, enhancing the depth and relevance of their findings.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC_pKRUuyn7A"
   },
   "source": [
    "## **2.1. Choose the point to use for each track**\n",
    "\n",
    "<font size = 4>This section offers users an interactive visualization tool to compare and select the most suitable analysis point within each track for spatial analysis. By providing a dynamic interface, users can assess the impact of different analysis points (e.g., \"beginning,\" \"end,\" \"middle,\" etc.) on spatial distribution patterns. This hands-on exploration empowers users to make informed decisions, ensuring that the chosen analysis point effectively captures the spatial characteristics of each track. Ultimately, this customization enhances the precision and relevance of spatial analysis results for a wide range of research objectives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OCcIC7ytzAzW"
   },
   "outputs": [],
   "source": [
    "# @title ##Run the cell and choose the analysis point you want to use\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def select_analysis_point(track, analysis_option):\n",
    "    if analysis_option == \"beginning\":\n",
    "        point = track.iloc[0][['POSITION_X', 'POSITION_Y']]\n",
    "    elif analysis_option == \"end\":\n",
    "        point = track.iloc[-1][['POSITION_X', 'POSITION_Y']]\n",
    "    elif analysis_option == \"middle\":\n",
    "        middle_index = len(track) // 2\n",
    "        point = track.iloc[middle_index][['POSITION_X', 'POSITION_Y']]\n",
    "    elif analysis_option == \"average\":\n",
    "        point = track[['POSITION_X', 'POSITION_Y']].mean()\n",
    "    elif analysis_option == \"median\":\n",
    "        point = track[['POSITION_X', 'POSITION_Y']].median()\n",
    "    else:\n",
    "        point = pd.Series([np.nan, np.nan], index=['POSITION_X', 'POSITION_Y'])\n",
    "\n",
    "    return point\n",
    "\n",
    "if not os.path.exists(Results_Folder+\"/Tracks\"):\n",
    "    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n",
    "\n",
    "# Extract unique filenames from the dataframe\n",
    "filenames = merged_spots_df['File_name'].unique()\n",
    "\n",
    "# Create Dropdown widgets with labels and fixed width\n",
    "filename_dropdown = widgets.Dropdown(\n",
    "    options=filenames,\n",
    "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
    "    description='File Name:',\n",
    "    layout=widgets.Layout(width='300px'),  # Adjust width as needed\n",
    ")\n",
    "\n",
    "analysis_option_dropdown = widgets.Dropdown(\n",
    "    options=[\"beginning\", \"end\", \"middle\", \"average\", \"median\"],\n",
    "    value=\"beginning\",\n",
    "    description='Point:',\n",
    "    layout=widgets.Layout(width='300px'),  # Adjust width as needed\n",
    ")\n",
    "\n",
    "# Define the plotting function\n",
    "def plot_coordinates(filename, analysis_option):\n",
    "    if filename:\n",
    "        # Filter the DataFrame based on the selected filename\n",
    "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for unique_id in filtered_df['Unique_ID'].unique():\n",
    "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
    "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n",
    "\n",
    "            # Find and mark the selected analysis point\n",
    "            analysis_point = select_analysis_point(unique_df, analysis_option)\n",
    "            if not analysis_point.isna().any():\n",
    "                plt.scatter(analysis_point['POSITION_X'], analysis_point['POSITION_Y'], color='red', s=50)\n",
    "\n",
    "        plt.xlabel('POSITION_X')\n",
    "        plt.ylabel('POSITION_Y')\n",
    "        plt.title(f'Coordinates for {filename} ({analysis_option} point)')\n",
    "        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}_{analysis_option}.pdf\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid filename selected\")\n",
    "\n",
    "# Link both Dropdown widgets to the plotting function\n",
    "interact(plot_coordinates, filename=filename_dropdown, analysis_option=analysis_option_dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_Py1TcJwqwB"
   },
   "source": [
    "## **2.2. Compute Ripley's L function for each FOV**\n",
    "\n",
    "<font size = 4>This code aims to compute Ripley's L function for each Field of View (FOV) in a dataset of tracked objects. Ripley's L function is a spatial statistics tool used to analyze the spatial distribution of points or objects in a given area. In this analysis, we are interested in understanding how objects are distributed within each FOV.\n",
    "\n",
    "## User Input Options\n",
    "\n",
    "1. **Analysis Option**: This option allows you to choose the point within each track that will be used for analysis. You can select one of the following options:\n",
    "   - \"beginning\": Use the initial position of each track.\n",
    "   - \"end\": Use the final position of each track.\n",
    "   - \"middle\": Use the middle position of each track.\n",
    "   - \"average\": Use the average position of all points within each track.\n",
    "   - \"median\": Use the median position of all points within each track.\n",
    "\n",
    "2. **r_values Range**: Ripley's L function is computed for a range of spatial distances denoted by \"r.\" You can specify the range of r_values using the following parameters:\n",
    "   - **Start Value**: The starting value of \"r\" (minimum distance). This number should be greater than 0.\n",
    "   - **End Value**: The ending value of \"r\" (maximum distance).\n",
    "   - **Number of Points**: The number of points or steps within the specified range. The analysis will be performed at equidistant intervals between the start and end values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Y0Ikco9nMopc"
   },
   "outputs": [],
   "source": [
    "# @title ##Run to compute Ripley's L function for each FOV\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define Ripley's K function\n",
    "def ripley_k(points, r, area):\n",
    "    n = len(points)\n",
    "    d_matrix = distance_matrix(points, points)\n",
    "    sum_indicator = np.sum(d_matrix < r) - n  # Subtract n to exclude self-pairs\n",
    "\n",
    "    K_r = (area / (n ** 2)) * sum_indicator\n",
    "\n",
    "    # Check if K_r is negative and print relevant information\n",
    "    if K_r < 0:\n",
    "        print(\"Negative K_r encountered!\")\n",
    "        print(\"Distance matrix:\", d_matrix)\n",
    "        print(\"Sum indicator:\", sum_indicator)\n",
    "        print(\"Area:\", area, \"Number of points:\", n, \"Distance threshold r:\", r)\n",
    "\n",
    "    return K_r\n",
    "\n",
    "\n",
    "# Define Ripley's L function\n",
    "\n",
    "def ripley_l(points, r, area):\n",
    "    K_r = ripley_k(points, r, area)\n",
    "    # Check if K_r has negative values\n",
    "    if np.any(K_r < 0):\n",
    "        print(\"Warning: Negative value encountered in K_r\")\n",
    "\n",
    "    L_r = np.sqrt(K_r / np.pi) - r\n",
    "    return L_r\n",
    "\n",
    "\n",
    "\n",
    "def select_analysis_point(track, analysis_option):\n",
    "    if analysis_option == \"beginning\":\n",
    "        point = track.iloc[0][['POSITION_X', 'POSITION_Y']]\n",
    "    elif analysis_option == \"end\":\n",
    "        point = track.iloc[-1][['POSITION_X', 'POSITION_Y']]\n",
    "    elif analysis_option == \"middle\":\n",
    "        middle_index = len(track) // 2\n",
    "        point = track.iloc[middle_index][['POSITION_X', 'POSITION_Y']]\n",
    "    elif analysis_option == \"average\":\n",
    "        point = track[['POSITION_X', 'POSITION_Y']].mean()\n",
    "    elif analysis_option == \"median\":\n",
    "        point = track[['POSITION_X', 'POSITION_Y']].median()\n",
    "    else:\n",
    "        point = pd.Series([np.nan, np.nan], index=['POSITION_X', 'POSITION_Y'])\n",
    "\n",
    "    return point\n",
    "\n",
    "analysis_option = \"beginning\" # @param [\"beginning\", \"end\", \"middle\", \"average\", \"median\"]\n",
    "\n",
    "# Prompt the user for the desired r_values range\n",
    "r_values_start = 0.1 # @param {type: \"number\"}\n",
    "r_values_end = 50# @param {type: \"number\"}\n",
    "r_values_count = 50# @param {type: \"number\"}\n",
    "\n",
    "r_values = np.linspace(r_values_start, r_values_end, r_values_count)\n",
    "\n",
    "\n",
    "# Check and create necessary directories\n",
    "if not os.path.exists(f\"{Results_Folder}/Track_Clustering/RipleyL\"):\n",
    "    os.makedirs(f\"{Results_Folder}/Track_Clustering/RipleyL\")\n",
    "\n",
    "# Define area based on your dataset's extent\n",
    "area = (merged_spots_df['POSITION_X'].max() - merged_spots_df['POSITION_X'].min()) * \\\n",
    "       (merged_spots_df['POSITION_Y'].max() - merged_spots_df['POSITION_Y'].min())\n",
    "\n",
    "# Compute Ripley's L function for each FOV\n",
    "l_values_per_fov_slow = {}\n",
    "for file_name, group in tqdm(merged_spots_df.groupby('File_name'), desc=\"Processing FOVs\"):\n",
    "    # Sort each track by POSITION_T\n",
    "    group = group.sort_values(by=['TRACK_ID', 'POSITION_T'])\n",
    "\n",
    "    representative_points = group.groupby('TRACK_ID').apply(lambda track: select_analysis_point(track, analysis_option)).dropna()\n",
    "    if not representative_points.empty:\n",
    "        l_values = [ripley_l(representative_points.values, r, area) for r in r_values]\n",
    "        l_values_per_fov_slow[file_name] = l_values\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlGW9UIqvqh6"
   },
   "source": [
    "## **2.3. Compute Monte Carlo Simulations for Each FOV**\n",
    "\n",
    "This code section performs Monte Carlo simulations to assess the significance of the observed spatial distribution patterns within each Field of View (FOV) in a dataset of tracked objects. The simulations help establish confidence envelopes for Ripley's L function, allowing for statistical testing.\n",
    "\n",
    "\n",
    "**Number of Simulations (Nb_simulation)**: You can specify the number of Monte Carlo simulations to run for each FOV. This parameter determines the level of statistical confidence and computational resources used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "INwTjl0JyM70"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# @title ##Run to compute Monte Carlo simulations for each FOV\n",
    "\n",
    "Nb_simulation = 10 # @param {type: \"number\"}\n",
    "\n",
    "# Simulate random points for Monte Carlo simulations\n",
    "def simulate_random_points(num_points, x_range, y_range):\n",
    "    x_coords = np.random.uniform(x_range[0], x_range[1], num_points)\n",
    "    y_coords = np.random.uniform(y_range[0], y_range[1], num_points)\n",
    "    return np.column_stack((x_coords, y_coords))\n",
    "\n",
    "# Initialize simulated_l_values as an empty dictionary\n",
    "simulated_l_values_dict_slow = {}\n",
    "\n",
    "# Perform Monte Carlo simulations for significance testing\n",
    "confidence_envelopes_slow = {}\n",
    "for file_name, group in tqdm(merged_spots_df.groupby('File_name'), desc='Processing FOVs'):\n",
    "\n",
    "    group = group.sort_values(by=['TRACK_ID', 'POSITION_T'])\n",
    "    representative_points = group.groupby('TRACK_ID').apply(lambda track: select_analysis_point(track, analysis_option)).dropna()\n",
    "\n",
    "    simulations = [simulate_random_points(len(representative_points),\n",
    "                                          (merged_spots_df['POSITION_X'].min(), merged_spots_df['POSITION_X'].max()),\n",
    "                                          (merged_spots_df['POSITION_Y'].min(), merged_spots_df['POSITION_Y'].max()))\n",
    "                   for _ in tqdm(range(Nb_simulation), desc=f'Simulating for {file_name}', leave=False)]\n",
    "\n",
    "    simulated_l_values = [[ripley_l(points, r, area) for r in r_values] for points in simulations]\n",
    "    simulated_l_values_dict_slow[file_name] = simulated_l_values  # Store the simulated values in the dictionary\n",
    "\n",
    "    lower_bound = np.percentile(simulated_l_values, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(simulated_l_values, 97.5, axis=0)\n",
    "    confidence_envelopes_slow[file_name] = (lower_bound, upper_bound)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcTrPDFNLF58"
   },
   "source": [
    "## **2.4. Plots the results for each FOV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FYkImji3TPOg"
   },
   "outputs": [],
   "source": [
    "# @title ##Plots the results for each FOV\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualization of Ripley's L function with confidence envelopes\n",
    "for file_name, l_values in l_values_per_fov_slow.items():\n",
    "    # Retrieve the confidence envelope for the current file\n",
    "    lower_bound, upper_bound = confidence_envelopes_slow.get(file_name, (None, None))\n",
    "\n",
    "    # Only proceed if the confidence envelope exists\n",
    "    if lower_bound is not None and upper_bound is not None:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(r_values, l_values, label=f'L(r) for {file_name}')\n",
    "        plt.fill_between(r_values, lower_bound, upper_bound, color='gray', alpha=0.5)\n",
    "        plt.xlabel('Radius (r)')\n",
    "        plt.ylabel(\"Ripley's L Function\")\n",
    "        plt.title(f\"Ripley's L Function - {file_name}_{analysis_option}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot as a PDF in the specified folder\n",
    "        pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/RipleyL/{file_name}_{analysis_option}.pdf\")\n",
    "        plt.savefig(pdf_path,bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()  # Close the plot to free memory\n",
    "    else:\n",
    "        print(f\"No confidence envelope data available for {file_name}_{analysis_option}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsL11TsbLEIY"
   },
   "source": [
    "## **2.5. Chose a specific radius and plot the results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hQlY8X-nJqYb"
   },
   "outputs": [],
   "source": [
    "# @title ##Define a specific radius and run\n",
    "\n",
    "# Define the specific radius for comparison\n",
    "specific_radius = 25 # @param {type: \"number\"}\n",
    "\n",
    "# Extract L values at the specific radius\n",
    "specific_radius_index = np.argmin(np.abs(r_values - specific_radius))  # Find the index of the closest radius value\n",
    "l_values_at_specific_radius_slow = {fov: l_values[specific_radius_index] for fov, l_values in l_values_per_fov_slow.items()}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(l_values_at_specific_radius_slow.keys(), l_values_at_specific_radius_slow.values())\n",
    "plt.xlabel('Field of View')\n",
    "plt.ylabel(f\"Ripley's L at radius {specific_radius}\")\n",
    "plt.title(f\"Comparison of Ripley's L Function at Radius {specific_radius} Across Different FOVs\")\n",
    "plt.xticks(rotation=45)\n",
    "# Save the plot as a PDF in the specified folder\n",
    "pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/RipleyL/l_values_at_specific_radius_{specific_radius}_{analysis_option}.pdf\")\n",
    "plt.savefig(pdf_path, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create DataFrame with confidence envelopes, median, and L values at the specific radius\n",
    "rows = []\n",
    "for fov, (lower_bound, upper_bound) in confidence_envelopes_slow.items():\n",
    "    l_value = l_values_per_fov_slow[fov][specific_radius_index]\n",
    "    lower = lower_bound[specific_radius_index]\n",
    "    upper = upper_bound[specific_radius_index]\n",
    "\n",
    "    # Retrieve simulated L values for the FOV\n",
    "    simulated_l_values_for_fov_slow = simulated_l_values_dict_slow.get(fov, [])\n",
    "\n",
    "    # Calculate median if simulated L values are available for the FOV\n",
    "    if simulated_l_values_for_fov_slow:\n",
    "        median_vals = [l_vals[specific_radius_index] for l_vals in simulated_l_values_for_fov_slow]\n",
    "        median = np.median(median_vals) if median_vals else np.nan\n",
    "    else:\n",
    "        median = np.nan\n",
    "\n",
    "    rows.append([fov, l_value, lower, upper, median])\n",
    "\n",
    "confidence_df = pd.DataFrame(rows, columns=['File_name', 'Ripley_L_at_Specific_Radius', 'Lower_Bound', 'Upper_Bound', 'Median'])\n",
    "\n",
    "# Merge with additional information\n",
    "additional_info_df = merged_tracks_df[['File_name', 'Condition', 'experiment_nb', 'Repeat']].drop_duplicates('File_name')\n",
    "merged_df = pd.merge(confidence_df, additional_info_df, left_on='File_name', right_on='File_name')\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(f\"{Results_Folder}/Track_Clustering/ripleys_l_values__{specific_radius}_{analysis_option}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJJBmMXVLDPb"
   },
   "source": [
    "## **2.6. Comparison of Ripley's L Values Across Conditions**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iRdk_OfxafE9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# @title ##Comparison of Ripley\\'s L Values Across Conditions\n",
    "\n",
    "# Convert 'Condition' to string if it's not already\n",
    "merged_df['Condition'] = merged_df['Condition'].astype(str)\n",
    "\n",
    "# Create the box plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=merged_df, x='Condition', y='Ripley_L_at_Specific_Radius')\n",
    "\n",
    "# Overlay the Monte Carlo simulation results\n",
    "for condition in merged_df['Condition'].unique():\n",
    "    condition_data = merged_df[merged_df['Condition'] == condition]\n",
    "\n",
    "    # Plot median values\n",
    "    medians = condition_data['Median']\n",
    "    plt.scatter([condition] * len(medians), medians, color='red', alpha=0.5)  # Median\n",
    "\n",
    "    # Handle NaN values and calculate mean and error only for non-NaN values\n",
    "    valid_data = condition_data.dropna(subset=['Median', 'Lower_Bound', 'Upper_Bound'])\n",
    "    if not valid_data.empty:\n",
    "        median_mean = valid_data['Median'].mean()\n",
    "        lower_mean = valid_data['Lower_Bound'].mean()\n",
    "        upper_mean = valid_data['Upper_Bound'].mean()\n",
    "        yerr = [[median_mean - lower_mean], [upper_mean - median_mean]]\n",
    "\n",
    "        # Check if yerr contains valid data before plotting\n",
    "        if not any(np.isnan(yerr)):\n",
    "            plt.errorbar(condition, median_mean, yerr=yerr, fmt='o', color='black', alpha=0.5)  # Confidence interval\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel(f\"Ripley's L at radius {specific_radius}\")\n",
    "plt.title('Comparison of Ripley\\'s L Values Across Conditions with Monte Carlo Simulation Results')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the figure before showing it\n",
    "pdf_path = os.path.join(f\"{Results_Folder}/l_values_Conditions_radius_{specific_radius}_{analysis_option}.pdf\")\n",
    "plt.savefig(pdf_path, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiY-Iq5HcwA0"
   },
   "source": [
    "## **2.7. Plot the analysis point for each FOV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Fn-E8ZoIQG2m"
   },
   "outputs": [],
   "source": [
    "# @title ##Run the cell to plot the coordinates used for the spatial analysis\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from google.colab import widgets  # Import Google Colab widgets\n",
    "\n",
    "if not os.path.exists(Results_Folder + \"/Track_Clustering/Coordinates\"):\n",
    "    os.makedirs(Results_Folder + \"/Track_Clustering/Coordinates\")  # Create Results_Folder if it doesn't exist\n",
    "\n",
    "\n",
    "show_plots = False # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "# Extract unique filenames from the dataframe\n",
    "filenames = merged_spots_df['File_name'].unique()\n",
    "\n",
    "# Define the plotting function\n",
    "def plot_points(filename, analysis_option, show_plots=True):\n",
    "    if filename:\n",
    "        # Filter the DataFrame based on the filename\n",
    "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for unique_id in filtered_df['Unique_ID'].unique():\n",
    "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
    "\n",
    "            # Find and mark the selected analysis point\n",
    "            analysis_point = select_analysis_point(unique_df, analysis_option)\n",
    "            if not analysis_point.isna().any():\n",
    "                plt.scatter(analysis_point['POSITION_X'], analysis_point['POSITION_Y'], color='red', s=50, label=f'Track {unique_id}')\n",
    "\n",
    "        plt.xlabel('POSITION_X')\n",
    "        plt.ylabel('POSITION_Y')\n",
    "        plt.title(f'Analysis points for {filename} ({analysis_option} point)')\n",
    "        plt.savefig(f\"{Results_Folder}/Track_Clustering/Coordinates/Analysis_points_{filename}_{analysis_option}.pdf\")\n",
    "\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    else:\n",
    "        print(\"No valid filename selected\")\n",
    "\n",
    "# Loop through all filenames and plot them one by one\n",
    "for filename in filenames:\n",
    "    analysis_option = \"beginning\"  # You can set your preferred analysis option here\n",
    "    plot_points(filename, analysis_option, show_plots)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMfsr-DFsVN4"
   },
   "source": [
    "# **Part 3: Version log**\n",
    "---\n",
    "<font size = 4>While I strive to provide accurate and helpful information, please be aware that:\n",
    "  - This notebook may contain bugs.\n",
    "  - Features are currently limited and will be expanded in future releases.\n",
    "\n",
    "<font size = 4>We encourage users to report any issues or suggestions for improvement. Please check the [repository](https://github.com/guijacquemet/CellTracksColab) regularly for updates and the latest version of this notebook.\n",
    "\n",
    "<font size = 4>**Version 0.9.1**\n",
    "  - Improved documentation\n",
    "  - Improved saving strategy\n",
    "\n",
    "<font size = 4>**Version 0.8**\n",
    "  - First release of this notebook\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1rJ6PgmC_6z-_hdOKqXT2BC_t84vTCUkp",
     "timestamp": 1701013955783
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
