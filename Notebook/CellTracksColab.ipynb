{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CellTracksColab**\n",
        "---\n",
        "\n",
        "Colab Notebook for Analyzing Migration Tracks generated by [TrackMate](https://imagej.net/plugins/trackmate/)\n",
        "This Colab notebook is designed to analyze migration tracks, placing emphasis on those generated by using TrackMate, renowned for its proficiency in detailing single-particle tracking data.\n",
        "\n",
        "Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n"
      ],
      "metadata": {
        "id": "xF4zYMmXULP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Before getting started**\n",
        "---\n",
        "\n",
        "---\n",
        "<font size = 4>**Important note**\n",
        "\n",
        "To load your TrackMate outputs, your dataset should be meticulously organized into a two-tiered folder hierarchy as depicted below.\n",
        "\n",
        "<font size = 4>Here's a common data structure that can work:\n",
        "\n",
        "## Folder Hierarchy\n",
        "\n",
        "- üìÅ **Experiments** `[Folder_path]`\n",
        "  - üåø **Condition_1** `[‚Äòcondition‚Äô is derived from this folder name]`\n",
        "    - üîÑ **R1** `[‚Äòrepeat‚Äô is derived from this folder name]`\n",
        "      - üìÑ `FOV_spots_1.csv`\n",
        "      - üìÑ `FOV_tracks_1.csv`\n",
        "      - üìÑ `FOV_spots_2.csv`\n",
        "      - üìÑ `FOV_tracks_2.csv`\n",
        "    - üîÑ **R2**\n",
        "      - üìÑ `FOV_spots_1.csv`\n",
        "      - üìÑ `FOV_tracks_1.csv`\n",
        "      - üìÑ `FOV_spots_2.csv`\n",
        "      - üìÑ `FOV_tracks_2.csv`\n",
        "  - üåø **Condition_2**\n",
        "    - üîÑ **R1**\n",
        "    - üîÑ **R2**\n",
        "\n",
        "<font size = 4>In this representation, different symbols are used to represent folders and files clearly:\n",
        "\n",
        "üìÅ represents the main folder or directory.\n",
        "üåø represents the condition folders.\n",
        "üîÑ represents the repeat folders.\n",
        "üìÑ represents the individual CSV files.\n",
        "\n",
        "---\n",
        "<font size = 4>**Important note 2**\n",
        "\n",
        "Be advised of two significant limitation inherent to this notebook.\n",
        "\n",
        "1) <font size = 4 color=\"red\">**It does not support Track splitting**</font>. For users aiming to compute additional track metrics within this environment, it is crucial to disable track splitting in TrackMate.\n",
        "\n",
        "It‚Äôs important to clarify that the absence of track splitting support does not hinder the notebook's ability to compile and display results in part 3 of the analysis process. The results compilation and display mechanisms are designed to function independently of track splitting, allowing users to visualize and interpret the data accurately.\n",
        "\n",
        "Before initiating the analysis, ensure that track splitting is disabled if the additional metrics computations are needed, to maintain the integrity and reliability of the results obtained through this notebook.\n",
        "\n",
        "2) <font size = 4 color=\"red\">**It is currently limited to the analysis of 2D tracks**</font>.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aR2U8v9YoJcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title #MIT License\n",
        "\n",
        "print(\"\"\"\n",
        "**MIT License**\n",
        "\n",
        "Copyright (c) 2023 Guillaume Jacquemet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JrkfFr7mgZmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Version log\n",
        "---\n",
        "<font size = 4>**Version 0.1**\n",
        "\n",
        "This is the first release of this notebook. While I strive to provide accurate and helpful information, please be aware that:\n",
        "  - This version may contain bugs.\n",
        "  - Features are currently limited and will be expanded in future releases.\n",
        "\n",
        "We encourage users to report any issues or suggestions for improvement. Please check the [repository](https://github.com/guijacquemet/CellTracksColab) regularly for updates and the latest version of this notebook.\n",
        "\n",
        "### Known Issues:\n",
        "- Part 4 is limited and unstable.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YheRveJdrFyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 1: Complete the Colab session and Load your tracks and spots data**\n",
        "--------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "Y4-Ft-yNRVCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ],
      "metadata": {
        "id": "9h0prdayn0qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Play to install\n",
        "!pip -q install pandas scikit-learn\n",
        "!pip -q install hdbscan\n",
        "!pip -q install umap-learn\n",
        "!pip -q install plotly\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "# URL to the raw content of the version file in the repository\n",
        "version_url = \"https://raw.githubusercontent.com/guijacquemet/CellTracksColab/main/Notebook/latest_version.txt\"\n",
        "\n",
        "# Current version of the notebook the user is running\n",
        "current_version = \"0.1\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(version_url)\n",
        "    response.raise_for_status()  # Check whether the request was successful\n",
        "    latest_version = response.text.strip()  # Get the latest version from the version file\n",
        "\n",
        "    if latest_version != current_version:\n",
        "        print(f\"A newer version of this notebook is available: {latest_version}. \"\n",
        "              f\"Please download the latest version from the repository.\")\n",
        "    else:\n",
        "        print(\"You are running the latest version of this notebook.\")\n",
        "except requests.RequestException as e:\n",
        "    print(\"Could not check for the latest version of the notebook.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rAP0ahCzn1V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ],
      "metadata": {
        "id": "3Kzd_8GUnpbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GA1wCrkoV4i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3. Load your (or test) data**\n",
        "\n",
        "<font size = 4> Please ensure that your data is properly organised (see above)\n"
      ],
      "metadata": {
        "id": "bsDAwkSOo1gV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A3D8odLQNx41"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Provide the path to your dataset\n",
        "\n",
        "Folder_path = ''  # @param {type: \"string\"}\n",
        "\n",
        "#@markdown ###Or use a test dataset\n",
        "Use_test_dataset = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ###Provide the path to your Result folder\n",
        "\n",
        "Results_Folder = \"/content\"  # @param {type: \"string\"}\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "def populate_columns(df, filepath):\n",
        "    # Extract the parts of the file path\n",
        "    path_parts = os.path.normpath(filepath).split(os.sep)\n",
        "\n",
        "    if len(path_parts) < 3:\n",
        "        # if there are not enough parts in the path to extract folder and parent folder\n",
        "        print(f\"Error: Cannot extract parent folder and folder from the filepath: {filepath}\")\n",
        "        return df\n",
        "\n",
        "    # Assuming that the file is located at least two levels deep in the directory structure\n",
        "    folder_name = path_parts[-2]  # The folder name is the second last part of the path\n",
        "    parent_folder_name = path_parts[-3]  # The parent folder name is the third last part of the path\n",
        "\n",
        "    df['File_name'] = os.path.splitext(os.path.basename(filepath))[0]\n",
        "    df['Condition'] = parent_folder_name  # Populate 'Condition' with the parent folder name\n",
        "    df['experiment_nb'] = folder_name  # Populate 'Repeat' with the folder name\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_and_populate(file_pattern, usecols=None):\n",
        "    df_list = []\n",
        "    pattern = re.compile(file_pattern)  # Compile the file pattern to a regex object\n",
        "\n",
        "    # Go through each root, dirs, files triplet returned by os.walk\n",
        "    for dirpath, dirnames, filenames in os.walk(Folder_path):\n",
        "        for filename in filenames:\n",
        "            if pattern.match(filename):  # Check if the filename matches the file pattern\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                df = pd.read_csv(filepath, skiprows=[1, 2, 3], usecols=usecols)\n",
        "                df_list.append(populate_columns(df, filepath))\n",
        "\n",
        "    if not df_list:  # if df_list is empty, return an empty DataFrame\n",
        "        print(f\"No files found with pattern: {file_pattern}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    merged_df = pd.concat(df_list, ignore_index=True)\n",
        "    return merged_df\n",
        "\n",
        "def sort_and_generate_repeat(merged_df):\n",
        "    merged_df.sort_values(['Condition', 'experiment_nb'], inplace=True)\n",
        "    merged_df = merged_df.groupby('Condition', group_keys=False).apply(generate_repeat)\n",
        "    return merged_df\n",
        "\n",
        "def generate_repeat(group):\n",
        "    unique_experiment_nbs = sorted(group['experiment_nb'].unique())\n",
        "    experiment_nb_to_repeat = {experiment_nb: i+1 for i, experiment_nb in enumerate(unique_experiment_nbs)}\n",
        "    group['Repeat'] = group['experiment_nb'].map(experiment_nb_to_repeat)\n",
        "    return group\n",
        "\n",
        "if (Use_test_dataset):\n",
        "  print(\"Downloading test dataset\")\n",
        "  !wget -nc -O /content/T_cell_dataset.zip https://github.com/guijacquemet/CellTracksColab/raw/main/Test_dataset/T_cell_dataset.zip && unzip -q /content/T_cell_dataset.zip -d /content\n",
        "  Folder_path = \"/content/Tracks\"\n",
        "\n",
        "print(\"Merging CSV files....\")\n",
        "\n",
        "merged_tracks_df = load_and_populate(r'.*tracks.*\\.csv')  # Use raw string to avoid escape character issues\n",
        "merged_tracks_df = sort_and_generate_repeat(merged_tracks_df)\n",
        "merged_tracks_df['Unique_ID'] = merged_tracks_df['Condition'] + \"_\" + merged_tracks_df['experiment_nb'] + \"_\" + merged_tracks_df['TRACK_ID'].astype(str)\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n",
        "\n",
        "merged_spots_df = load_and_populate(r'.*spots.*\\.csv')  # Use raw string to avoid escape character issues\n",
        "merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n",
        "merged_spots_df['Unique_ID'] = merged_spots_df['Condition'] + \"_\" + merged_spots_df['experiment_nb'] + \"_\" + merged_spots_df['TRACK_ID'].astype(str)\n",
        "merged_spots_df.to_csv(Results_Folder + '/' + 'merged_Spots.csv', index=False, compression='gzip')\n",
        "\n",
        "print(\"Done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.4. Visualise your tracks**"
      ],
      "metadata": {
        "id": "52STmnv43d45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Run the cell and choose the file you want to inspect\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract unique filenames from the dataframe\n",
        "filenames = merged_spots_df['File_name'].unique()\n",
        "\n",
        "# Create a Dropdown widget with the filenames\n",
        "filename_dropdown = widgets.Dropdown(\n",
        "    options=filenames,\n",
        "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
        "    description='File Name:',\n",
        ")\n",
        "\n",
        "def plot_coordinates(filename):\n",
        "    if filename:\n",
        "        # Filter the DataFrame based on the selected filename\n",
        "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for unique_id in filtered_df['Unique_ID'].unique():\n",
        "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
        "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n",
        "\n",
        "        plt.xlabel('POSITION_X')\n",
        "        plt.ylabel('POSITION_Y')\n",
        "        plt.title(f'Coordinates for {filename}')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid filename selected\")\n",
        "\n",
        "# Link the Dropdown widget to the plotting function\n",
        "interact(plot_coordinates, filename=filename_dropdown)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AE881uJW5ukQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 2: Compute additional metrics**\n",
        "--------------------------------------------------------\n",
        "<font size = 4> Additional Metrics will be added later.\n"
      ],
      "metadata": {
        "id": "UlUCXg5QUC4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Calculate directionality\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate Directionality\n",
        "def calculate_directionality(group):\n",
        "    group = group.sort_values('POSITION_T')\n",
        "    start_point = group.iloc[0][['POSITION_X', 'POSITION_Y']]\n",
        "    end_point = group.iloc[-1][['POSITION_X', 'POSITION_Y']]\n",
        "    euclidean_distance = np.sqrt((end_point - start_point).pow(2).sum())\n",
        "\n",
        "    deltas = np.sqrt(group['POSITION_X'].diff().fillna(0)**2 + group['POSITION_Y'].diff().fillna(0)**2)\n",
        "    total_path_length = deltas.sum()\n",
        "\n",
        "    D = euclidean_distance / total_path_length if total_path_length != 0 else 0\n",
        "    return pd.Series({'Directionality': D})\n",
        "\n",
        "# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n",
        "merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "\n",
        "# Calculate directionality for each track\n",
        "df_directionality = merged_spots_df.groupby('Unique_ID').apply(calculate_directionality).reset_index()\n",
        "\n",
        "# Merge the directionality back into the original DataFrame\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, df_directionality, on='Unique_ID', how='left')\n",
        "\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ijl_DXjhTvnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot directionality\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# List of variables to plot\n",
        "variables_to_plot = [\"Directionality\"]\n",
        "\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_pages = PdfPages(Results_Folder+'Boxplots.pdf')\n",
        "# Create a single figure with 4 subplots, one for each variable\n",
        "fig, axes = plt.subplots(len(variables_to_plot), 1, figsize=(10, 6))\n",
        "\n",
        "# Make sure axes is a list in case there's only one subplot\n",
        "if len(variables_to_plot) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, var in zip(axes, variables_to_plot):\n",
        "    # Extract the data for this variable\n",
        "    data_for_var = merged_tracks_df[['Condition', var]]\n",
        "\n",
        "    # Save this data to a CSV file\n",
        "    data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}.csv\", index=False)\n",
        "    sns.boxplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, color='lightgray')  # Boxplot\n",
        "    sns.stripplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "    ax.set_title(f\"{var}\")\n",
        "    ax.set_xlabel('Condition')\n",
        "    ax.set_ylabel(var)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "\n",
        "# Save the figure to a PDF\n",
        "plt.tight_layout()\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "49ye7pbKT2B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------\n",
        "\n",
        "# **Part 3: Plot track parameters**\n",
        "-------------------------------------------\n",
        "\n",
        "<font size = 4> In this section you can plot all the track parameters previously computed. Data and graphs are automatically saved in your result folder.\n"
      ],
      "metadata": {
        "id": "joRI14WVUPuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot useful tracks data\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# Create a list of potential variables from DataFrame columns\n",
        "all_columns = merged_tracks_df.columns.tolist()\n",
        "\n",
        "# Remove unwanted columns like 'condition' and 'repeat' from the list\n",
        "selectable_columns = [col for col in all_columns if col not in ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']]\n",
        "\n",
        "# Create checkboxes for selectable columns\n",
        "variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "\n",
        "\n",
        "# Arrange and display checkboxes in the notebook\n",
        "display(widgets.VBox([\n",
        "    widgets.Label('Variables to Plot:'),\n",
        "    widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n",
        "]))\n",
        "\n",
        "# Define the plotting function\n",
        "def plot_selected_vars(button):\n",
        "  print(\"Plotting in progress...\")\n",
        "\n",
        "  variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "\n",
        "  pdf_pages = PdfPages(f\"{Results_Folder}/boxplots.pdf\")\n",
        "\n",
        "# Determine the number of variables to plot\n",
        "  n_plots = len(variables_to_plot)\n",
        "\n",
        "# If no variables are selected, avoid creating a plot\n",
        "  if n_plots == 0:\n",
        "    print(\"No variables selected for plotting\")\n",
        "  else:\n",
        "    # Set the height of each subplot and figure width\n",
        "    subplot_height = 5  # Adjust as per your requirement\n",
        "    fig_width = 10  # Adjust as per your requirement\n",
        "\n",
        "    # Calculate the total figure height\n",
        "    fig_height = n_plots * subplot_height\n",
        "\n",
        "    # Create subplots with dynamic figure size\n",
        "    fig, axes = plt.subplots(n_plots, 1, figsize=(fig_width, fig_height))\n",
        "\n",
        "    # Make axes iterable in case there's only one subplot\n",
        "    if n_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, var in zip(axes, variables_to_plot):\n",
        "        data_for_var = merged_tracks_df[['Condition', var]]\n",
        "        data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}.csv\", index=False)\n",
        "        sns.boxplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, color='lightgray')\n",
        "        sns.stripplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, hue='Repeat', dodge=True, jitter=True, alpha=0.2)\n",
        "        ax.set_title(f\"{var}\")\n",
        "        ax.set_xlabel('Condition')\n",
        "        ax.set_ylabel(var)\n",
        "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    pdf_pages.savefig(fig)\n",
        "    pdf_pages.close()\n",
        "    plt.show()\n",
        "\n",
        "# Create a button that will execute the plotting function when clicked\n",
        "button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n",
        "button.on_click(plot_selected_vars)\n",
        "display(button)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AdzJCNTyUTEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "# **Part 4: Visualization of high-dimensional data (work in progress)**\n",
        "--------\n",
        "\n",
        "<font size = 4> The workflow provided below is inspired by [CellPlato](https://github.com/Michael-shannon/cellPLATO)"
      ],
      "metadata": {
        "id": "6tFTER9rg2Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1: UMAP**\n",
        "\n",
        "<font size = 4> The given code performs UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction on the merged tracks dataframe, focusing on its numeric columns, and visualizes the result. UMAP is often used for visualization of high-dimensional data in a 2D or 3D space."
      ],
      "metadata": {
        "id": "GLlDuCKmlxvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Perform UMAP\n",
        "\n",
        "# User alterable parameters\n",
        "umap_nn = 30  # UMAP nearest neighbors\n",
        "min_dist = 0.0  # UMAP minimum distance\n",
        "n_components = 3  # Number of UMAP dimensions to calculate\n",
        "\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_df = merged_tracks_df.select_dtypes(include=['number'])\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_df = merged_tracks_df.select_dtypes(include=['number'])\n",
        "\n",
        "# Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "nan_columns = numeric_df.columns[numeric_df.isna().any()].tolist()\n",
        "\n",
        "if nan_columns:\n",
        "    warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n",
        "    numeric_df = numeric_df.dropna()\n",
        "\n",
        "# Initialize UMAP object with the specified settings\n",
        "reducer = umap.UMAP(n_neighbors=umap_nn, min_dist=min_dist, n_components=n_components, random_state=42)\n",
        "embedding = reducer.fit_transform(numeric_df)\n",
        "\n",
        "# Create dynamic column names based on n_components\n",
        "column_names = [f'UMAP dimension {i}' for i in range(1, n_components + 1)]\n",
        "\n",
        "# Create a DataFrame with the UMAP results\n",
        "umap_df = pd.DataFrame(embedding, columns=column_names)\n",
        "\n",
        "# Concatenate the conditions (if available)\n",
        "if 'Condition' in merged_tracks_df.columns:\n",
        "    umap_df = pd.concat([umap_df, merged_tracks_df['Condition'].reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Visualize the UMAP projection\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# The plot will adjust automatically based on the n_components\n",
        "if n_components == 2:\n",
        "    sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=umap_df, palette='viridis', s=60)\n",
        "    plt.title('UMAP Projection of the Dataset')\n",
        "    plt.show()\n",
        "elif n_components == 1:\n",
        "    sns.stripplot(x=column_names[0], hue='Condition', data=umap_df, palette='viridis', jitter=0.05, size=6)\n",
        "    plt.title('UMAP Projection of the Dataset')\n",
        "    plt.show()\n",
        "else:\n",
        "    # umap_df should have columns like 'UMAP dimension 1', 'UMAP dimension 2', 'UMAP dimension 3', and 'condition'\n",
        "    import plotly.express as px\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    fig = px.scatter_3d(umap_df,\n",
        "                    x='UMAP dimension 1',\n",
        "                    y='UMAP dimension 2',\n",
        "                    z='UMAP dimension 3',\n",
        "                    color='Condition')\n",
        "\n",
        "    for trace in fig.data:\n",
        "      trace.marker.size = 2  # You can set this to any desired value\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h-HJIe9ug1bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 **HDBSCAN**\n",
        "\n",
        "<font size = 4> The provided code employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify clusters within a dataset that has already undergone UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction. HDBSCAN is utilized for its proficiency in determining the optimal number of clusters while managing varied densities within the data."
      ],
      "metadata": {
        "id": "hMx8DVRMmvKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Identify clusters using HDBSCAN\n",
        "\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Suppose you have a DataFrame of features (numeric_df) and have performed UMAP on it, resulting in umap_df\n",
        "\n",
        "# Apply HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=200, metric='euclidean')  # You may need to tune these parameters\n",
        "clusterer.fit(umap_df[['UMAP dimension 1', 'UMAP dimension 2']])  # Use the UMAP results for clustering\n",
        "#clusterer.fit(merged_tracks_df.select_dtypes(include=['number']))\n",
        "\n",
        "# Add the cluster labels to your UMAP DataFrame\n",
        "umap_df['Cluster'] = clusterer.labels_\n",
        "\n",
        "if n_components == 2:\n",
        "  # Plotting the results\n",
        "  plt.figure(figsize=(12,10))\n",
        "  sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', hue='Cluster', palette='viridis', data=umap_df, s=60)\n",
        "  plt.title('Clusters Identified by HDBSCAN')\n",
        "  plt.show()\n",
        "\n",
        "if n_components == 3:\n",
        "  import plotly.express as px\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "\n",
        "  fig = px.scatter_3d(umap_df,\n",
        "                    x='UMAP dimension 1',\n",
        "                    y='UMAP dimension 2',\n",
        "                    z='UMAP dimension 3',\n",
        "                    color='Cluster')\n",
        "\n",
        "  for trace in fig.data:\n",
        "    trace.marker.size = 2  # You can set this to any desired value\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1VV75MUjixkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Identify exemplar cells using HDBSCAN (not available)\n",
        "\n",
        "# Extracting exemplar points\n",
        "exemplars = []\n",
        "for exemplar in clusterer.exemplars_:\n",
        "    exemplars.extend(exemplar)\n",
        "\n",
        "# Flatten the exemplars list of lists into a single list\n",
        "flattened_exemplars = [index for sublist in exemplars for index in sublist]\n",
        "\n",
        "# Now pass the flattened list to iloc\n",
        "exemplar_df = umap_df.iloc[flattened_exemplars]\n",
        "\n",
        "# Plotting clusters and exemplar points\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', hue='Cluster', palette='viridis', data=umap_df, s=60)\n",
        "sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', color='red', label='Exemplars', data=exemplar_df, s=100, marker='X')\n",
        "plt.title('Clusters and Exemplar Cells Identified by HDBSCAN')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xaY77_YgjaST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot the 'fingerprint' of each cluster per condition\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Group by 'Condition' and 'Cluster' and calculate the size of each group\n",
        "cluster_counts = umap_df.groupby(['Condition', 'Cluster']).size().reset_index(name='counts')\n",
        "\n",
        "# Calculate the total number of points per condition\n",
        "total_counts = umap_df.groupby('Condition').size().reset_index(name='total_counts')\n",
        "\n",
        "# Merge the DataFrames on 'Condition' to calculate percentages\n",
        "percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n",
        "percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n",
        "\n",
        "\n",
        "# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n",
        "pivot_df = percentage_df.pivot(index='Condition', columns='Cluster', values='percentage')\n",
        "\n",
        "# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n",
        "pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting\n",
        "ax = pivot_df.plot(kind='bar', stacked=True, figsize=(10, 7), colormap='viridis')\n",
        "plt.title('Percentage in each cluster per Condition')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Condition')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B8S0uwynjht1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Perform t-SNE\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming df is your DataFrame containing the data\n",
        "# Drop non-numeric columns or encode them to numeric\n",
        "\n",
        "numeric_df = merged_tracks_df.select_dtypes(include=['float64', 'int64'])  # Selecting only numeric columns\n",
        "\n",
        "\n",
        "# Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "nan_columns = numeric_df.columns[numeric_df.isna().any()].tolist()\n",
        "\n",
        "if nan_columns:\n",
        "    warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\", UserWarning)\n",
        "    numeric_df = numeric_df.dropna()\n",
        "\n",
        "scaled_df = StandardScaler().fit_transform(numeric_df)  # Scaling the numeric features\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=42)\n",
        "tsne_results = tsne.fit_transform(scaled_df)\n",
        "\n",
        "tsne_df = pd.DataFrame(data=tsne_results, columns=['Dimension 1', 'Dimension 2'])\n",
        "tsne_df['Condition'] = merged_tracks_df['Condition']\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.scatterplot(\n",
        "    x='Dimension 1', y='Dimension 2',\n",
        "    hue='Condition',\n",
        "    palette=sns.color_palette(\"hsv\", len(tsne_df['Condition'].unique())),\n",
        "    data=tsne_df,\n",
        "    legend=\"full\",\n",
        "    alpha=0.9\n",
        ")\n",
        "plt.title('t-SNE Plot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KelZFuntkDF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Identify clusters using HDBSCAN\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500, metric='euclidean')  # You may need to tune these parameters\n",
        "\n",
        "clusterer.fit(tsne_results)\n",
        "\n",
        "# Create a DataFrame with the t-SNE results and cluster labels\n",
        "clustered_df = pd.DataFrame(data=tsne_results, columns=['Dimension 1', 'Dimension 2'])\n",
        "clustered_df['Cluster'] = clusterer.labels_\n",
        "clustered_df['Condition'] = tsne_df['Condition']\n",
        "\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.scatterplot(x='Dimension 1', y='Dimension 2', hue='Cluster', palette='viridis', data=clustered_df, s=60)\n",
        "plt.title('Clusters identified by HDBSCAN on t-SNE Results')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ypPWiZWCkZF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot the 'fingerprint' of each cluster per condition\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your dataframe is clustered_df after t-SNE and HDBSCAN\n",
        "# and it contains a 'condition' column with the original condition labels,\n",
        "# and a 'Cluster' column with the HDBSCAN cluster labels\n",
        "\n",
        "# Group by 'condition' and 'Cluster' and calculate the size of each group\n",
        "cluster_counts = clustered_df.groupby(['Condition', 'Cluster']).size().reset_index(name='counts')\n",
        "\n",
        "# Calculate the total number of points per condition\n",
        "total_counts = clustered_df.groupby('Condition').size().reset_index(name='total_counts')\n",
        "\n",
        "# Merge the DataFrames on 'condition' to calculate percentages\n",
        "percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n",
        "percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n",
        "\n",
        "# Pivot the percentage_df to have conditions as index, Clusters as columns, and percentages as values\n",
        "pivot_df = percentage_df.pivot(index='Condition', columns='Cluster', values='percentage')\n",
        "\n",
        "# Fill NaN values with 0, as there might be some condition-Cluster combinations that are not present\n",
        "pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "# Plotting\n",
        "ax = pivot_df.plot(kind='bar', stacked=True, figsize=(10, 7), colormap='viridis')\n",
        "plt.title('Percentage in each cluster per condition')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Condition')\n",
        "plt.xticks(rotation=90)  # Changed from 90 to 45 for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cZJWY5rMkgmW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}