{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41c8da98-a283-4d55-93f7-39e47671a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded in /Users/esti/Documents/PROYECTOS/CELLTRACKS/CellTracksColab/Tracks\n",
      " Please remove this file if you want to download the data again.\n",
      "/Users/esti/Documents/PROYECTOS/CELLTRACKS/CellTracksColab/Tracks/T_cell_dataset.zip\n",
      "Test dataset extracted successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e2caec86ad41d18ba125ec2b09c13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking data loaded in memory.\n",
      "These are its column names:Index(['ID', 'X', 'Y', 'T', 'File_name', 'Condition', 'experiment_nb'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e192fdf274bb4c818ad56507308a6ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving Spots:   0%|          | 0/38822 [00:00<?, ?rows/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n",
    "    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Estimating the number of chunks based on the provided chunk size\n",
    "    num_chunks = int(len(df) / chunk_size) + 1\n",
    "\n",
    "    # Create a tqdm instance for progress tracking\n",
    "    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n",
    "        # Open the file for writing\n",
    "        with open(path, \"w\") as f:\n",
    "            # Write the header once at the beginning\n",
    "            df.head(0).to_csv(f, index=False)\n",
    "\n",
    "            for chunk in np.array_split(df, num_chunks):\n",
    "                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n",
    "                pbar.update(len(chunk))\n",
    "                \n",
    "def download_test_datasets(path_extracted_dir, url, local_zip_file=''):\n",
    "    \n",
    "    local_zip_file = os.path.join(path_extracted_dir, \"T_cell_dataset.zip\")\n",
    "    \n",
    "    # Check if the extracted directory exists\n",
    "    if os.path.exists(path_extracted_dir):\n",
    "        print(f\"Dataset already downloaded in {path_extracted_dir}\") ##TODO: should we just say that there's something there already?\n",
    "        print(f\" Please remove this file if you want to download the data again.\")\n",
    "        \n",
    "    # Check if the ZIP file already exists\n",
    "    print(local_zip_file)\n",
    "    if not os.path.exists(local_zip_file):\n",
    "        print(\"Downloading test dataset\")\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Create the extracted directory if it doesn't exist\n",
    "            os.makedirs(path_extracted_dir, exist_ok=True)\n",
    "\n",
    "            # Calculate the total file size for the progress bar\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                # Download and save the content with progress tracking\n",
    "                with open(local_zip_file, \"wb\") as file:\n",
    "                    for data in response.iter_content(chunk_size=1024):\n",
    "                        pbar.update(len(data))\n",
    "                        file.write(data)\n",
    "\n",
    "            print(\"Test dataset downloaded successfully.\")\n",
    "        \n",
    "    # Extract the contents of the zip file to the specified directory\n",
    "    with zipfile.ZipFile(local_zip_file, 'r') as zip_ref:\n",
    "      zip_ref.extractall(path_extracted_dir)\n",
    "    print(\"Test dataset extracted successfully.\")\n",
    "    \n",
    "def populate_columns(df, filepath):\n",
    "    # Extract the parts of the file path\n",
    "    path_parts = os.path.normpath(filepath).split(os.sep)\n",
    "\n",
    "    if len(path_parts) < 3:\n",
    "        # if there are not enough parts in the path to extract folder and parent folder\n",
    "        print(f\"Error: Cannot extract parent folder and folder from the filepath: {filepath}\")\n",
    "        return df\n",
    "\n",
    "    # Assuming that the file is located at least two levels deep in the directory structure\n",
    "    folder_name = path_parts[-2]  # The folder name is the second last part of the path\n",
    "    parent_folder_name = path_parts[-3]  # The parent folder name is the third last part of the path\n",
    "\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    df['File_name'] = filename_without_extension\n",
    "    df['Condition'] = parent_folder_name  # Populate 'Condition' with the parent folder name\n",
    "    df['experiment_nb'] = folder_name  # Populate 'Repeat' with the folder name\n",
    "\n",
    "    return df\n",
    "\n",
    "def find_calibration(filepath, line=3):\n",
    "    k=0\n",
    "    for row in open(filepath):\n",
    "        k+=1\n",
    "        if k>line:  \n",
    "            return row[37:43]\n",
    "    \n",
    "def load_and_populate(Folder_path, file_pattern, skiprows=None, usecols=None, check_calibration=False, row=3):\n",
    "    df_list = []\n",
    "    pattern = re.compile(file_pattern)  # Compile the file pattern to a regex object\n",
    "    files_to_process = []\n",
    "\n",
    "    # First, list all the files we'll be processing\n",
    "    for dirpath, dirnames, filenames in os.walk(Folder_path):\n",
    "        for filename in filenames:\n",
    "            if pattern.match(filename):  # Check if the filename matches the file pattern\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                files_to_process.append(filepath)\n",
    "                \n",
    "    # Metadata list used to check for correct loading of rows\n",
    "    metadata_list = []\n",
    "    \n",
    "    # Create a tqdm instance for progress tracking\n",
    "    for filepath in tqdm(files_to_process, desc=\"Processing Files\"):\n",
    "        \n",
    "        # Add to the metadata list\n",
    "        if check_calibration:\n",
    "            calibration_units = find_calibration(filepath)\n",
    "            metadata_list.append({\n",
    "                'filename': os.path.basename(filepath),\n",
    "                'expected_rows': sum(1 for row in open(filepath)) - 4, # Get the expected number of rows in the file (subtracting header rows)\n",
    "                'file_size': os.path.getsize(filepath), # Get file size\n",
    "                'calibration_units': calibration_units\n",
    "            })\n",
    "        else:\n",
    "            metadata_list.append({\n",
    "            'filename': os.path.basename(filepath),\n",
    "            'expected_rows': sum(1 for row in open(filepath)) - 4, # Get the expected number of rows in the file (subtracting header rows)\n",
    "            'file_size': os.path.getsize(filepath) # Get file size\n",
    "        })\n",
    "        # Load the data in chunksizes to avoid memory colapse\n",
    "        df = pd.read_csv(filepath, skiprows=skiprows, usecols=usecols, chunksize=chunksize)\n",
    "        for chunk in chunked_reader:\n",
    "            processed_chunk = populate_columns(chunk, filepath)\n",
    "            df_list.append(processed_chunk))\n",
    "\n",
    "    if not df_list:  # if df_list is empty, return an empty DataFrame\n",
    "        print(f\"No files found with pattern: {file_pattern}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Verify the total rows in the merged dataframe matches the total expected rows from metadata\n",
    "    total_expected_rows = sum(item['expected_rows'] for item in metadata_list)\n",
    "    calibration_units = [item['calibration_units'] for item in metadata_list]\n",
    "    \n",
    "    if len(np.unique(calibration_units))>1:\n",
    "        print(f'Warning: The data is calibrated using different units: {np.unique(calibration_units)}')\n",
    "        \n",
    "    if len(merged_df) != total_expected_rows:\n",
    "      print(f\"Warning: Mismatch in total rows. Expected {total_expected_rows}, found {len(merged_df)} in the merged dataframe.\")\n",
    "    else:\n",
    "      print(f\"Success: The processed dataframe matches the metadata. Total rows: {len(merged_df)}\")\n",
    "    return merged_df\n",
    "    \n",
    "def generate_repeat(group):\n",
    "    unique_experiment_nbs = sorted(group['experiment_nb'].unique())\n",
    "    experiment_nb_to_repeat = {experiment_nb: i+1 for i, experiment_nb in enumerate(unique_experiment_nbs)}\n",
    "    group['Repeat'] = group['experiment_nb'].map(experiment_nb_to_repeat)\n",
    "    return group\n",
    "    \n",
    "def sort_and_generate_repeat(merged_df):\n",
    "    merged_df.sort_values(['Condition', 'experiment_nb'], inplace=True)\n",
    "    merged_df = merged_df.groupby('Condition', group_keys=False).apply(generate_repeat)\n",
    "    return merged_df\n",
    "\n",
    "def remove_suffix(filename):\n",
    "    suffixes_to_remove = [\"-tracks\", \"-spots\"]\n",
    "    for suffix in suffixes_to_remove:\n",
    "        if filename.endswith(suffix):\n",
    "            filename = filename[:-len(suffix)]\n",
    "            break\n",
    "    return filename\n",
    "\n",
    "\n",
    "def validate_tracks_df(df):\n",
    "    \"\"\"Validate the tracks dataframe for necessary columns and data types.\"\"\"\n",
    "    required_columns = ['TRACK_ID']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Error: Column '{col}' missing in tracks dataframe.\")\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def validate_spots_df(df):\n",
    "    \"\"\"Validate the spots dataframe for necessary columns and data types, and clean NaN values from TRACK_ID.\"\"\"\n",
    "    required_columns = ['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_Z', 'POSITION_T']\n",
    "\n",
    "    # Check for required columns\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Error: Column '{col}' missing in spots dataframe.\")\n",
    "            return False\n",
    "\n",
    "    # Check for NaN in TRACK_ID\n",
    "    if df['TRACK_ID'].isnull().any():\n",
    "        print(\"Warning: NaN values found in TRACK_ID column.\")\n",
    "        # Find filenames associated with NaN TRACK_IDs\n",
    "        filenames_with_nan = df[df['TRACK_ID'].isnull()]['File_name'].unique()\n",
    "        for filename in filenames_with_nan:\n",
    "            print(f\"Removing rows with NaN in TRACK_ID for file: {filename}\")\n",
    "\n",
    "        # Remove rows where TRACK_ID is NaN\n",
    "        initial_row_count = len(df)\n",
    "        df.dropna(subset=['TRACK_ID'], inplace=True)\n",
    "        final_row_count = len(df)\n",
    "        print(f\"Rows removed: {initial_row_count - final_row_count}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def check_unique_id_match(df1, df2):\n",
    "    df1_ids = set(df1['Unique_ID'])\n",
    "    df2_ids = set(df2['Unique_ID'])\n",
    "\n",
    "    # Check if the IDs in the two dataframes match\n",
    "    if df1_ids == df2_ids:\n",
    "        print(\"The Unique_ID values in both dataframes match perfectly!\")\n",
    "    else:\n",
    "        missing_in_df1 = df2_ids - df1_ids\n",
    "        missing_in_df2 = df1_ids - df2_ids\n",
    "\n",
    "        if missing_in_df1:\n",
    "            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n",
    "            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n",
    "\n",
    "        if missing_in_df2:\n",
    "            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n",
    "            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n",
    "\n",
    "\n",
    "\n",
    "merged_tracks_df = pd.DataFrame()\n",
    "\n",
    "class TrackingData:\n",
    "\n",
    "    def __init__(self):\n",
    "        #---------Parameters---------#\n",
    "        self.test_data_url = \"https://zenodo.org/record/8420011/files/T_Cells_spots_only.zip?download=1\"\n",
    "        self.parent_dir = os.getcwd().split(\"/Notebook\")[0] # We want the parent dir to the github repo. Sometimes the notebook may run somewhere else.\n",
    "        self.Folder_path = os.path.join(self.parent_dir, \"Tracks\")\n",
    "        self.Results_Folder = os.path.join(self.parent_dir, \"Results\")\n",
    "        self.skiprows = [1, 2, 3] # Rows to skip if a TrackMate CSV file was given\n",
    "        self.usecols = None # One could choose to load only specific columns, for example: self.usecols = [1, 2, 3] or self.usecols = ['ID', 'X', 'Y', 'T']\n",
    "        self.fileformat = \"csv\" # CSV is the format by default but it could be XML file\n",
    "        self.datatype = \"Custom\" # One of the followings [\"Custom\", \"TrackMate Table\", \"TrackMate Files\"]\n",
    "        self.data_dimensionality = \"2D\" # One of the followings [\"2D\", \"3D\"]\n",
    "        self.dim_mapping = { # Dictionary to map data\n",
    "            'TRACK_ID': \"ID\",\n",
    "            'POSITION_X': \"X\", \n",
    "            'POSITION_Y': \"Y\",\n",
    "            'POSITION_Z': \"\" ,\n",
    "            'POSITION_T': \"T\" \n",
    "        }\n",
    "        #----------------------------#\n",
    "    def DownloadTestData(self):\n",
    "        \n",
    "        # Define the path to the ready to use tracking data\n",
    "        local_zip_file = os.path.join(self.parent_dir, \"Test_dataset\", \"T_cell_dataset.zip\")\n",
    "        \n",
    "        # Download and extract the test data\n",
    "        download_test_datasets(self.Folder_path, self.test_data_url, local_zip_file=local_zip_file)\n",
    "\n",
    "\n",
    "    def column_mapping(self):\n",
    "\n",
    "        # If 2D is selected and POSITION_Z exists, delete it\n",
    "        if sel.data_dimensionality.value == '2D' and 'POSITION_Z' in merged_spots_df.columns:\n",
    "            merged_spots_df = merged_spots_df.drop('POSITION_Z', axis=1)\n",
    "            self.dim_mapping.pop('POSITION_Z')\n",
    "\n",
    "        #column_mapping = {dropdown.value: col for col, dropdown in dropdowns.items()}\n",
    "        column_mapping = {dropdown: col for col, dropdown in self.dim_mapping.items()}\n",
    "        merged_spots_df = merged_spots_df.rename(columns=column_mapping)\n",
    "        print(\"Columns Renamed!\")\n",
    "        merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n",
    "        merged_spots_df['Unique_ID'] = merged_spots_df['File_name'] + \"_\" + merged_spots_df['TRACK_ID'].astype(str)\n",
    "        \n",
    "        #TODO: This if 2D should not even exist\n",
    "        # If 2D was chosen, add back the POSITION_Z column filled with 0s at the end\n",
    "        if self.data_dimensionality.value == '2D':\n",
    "            merged_spots_df['POSITION_Z'] = 0\n",
    "        \n",
    "        # Extracting unique Unique_ID values from merged_spots_df\n",
    "        unique_ids = merged_spots_df['Unique_ID'].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "        # Creating merged_tracks_df with only the unique Unique_ID values\n",
    "        merged_tracks_df = pd.DataFrame(unique_ids, columns=['Unique_ID'])\n",
    "        print(\"Create a the merged_tracks_df to store track parameters\")\n",
    "        # Specify the columns you want to merge\n",
    "        columns_to_merge = ['Unique_ID', 'File_name', 'Condition', 'experiment_nb', 'Repeat']\n",
    "    \n",
    "        # Filter to only include the desired columns\n",
    "        filtered_df = merged_spots_df[columns_to_merge].drop_duplicates(subset='Unique_ID')\n",
    "    \n",
    "        # Find the overlapping columns between the two DataFrames, excluding the merging key\n",
    "        overlapping_columns = merged_tracks_df.columns.intersection(filtered_df.columns).drop('Unique_ID')\n",
    "    \n",
    "        # Drop the overlapping columns from the left DataFrame\n",
    "        merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
    "    \n",
    "        # Merge the filtered df_directionality back into the original DataFrame\n",
    "        merged_tracks_df = pd.merge(merged_tracks_df, filtered_df, on='Unique_ID', how='left')\n",
    "        \n",
    "        check_unique_id_match(merged_spots_df, merged_tracks_df)\n",
    "\n",
    "        # Save the DataFrame with the selected columns merged\n",
    "        save_dataframe_with_progress(merged_tracks_df, os.path.join(Results_Folder, 'merged_Tracks.csv'), desc=\"Saving Tracks\")\n",
    "        self.tracks_data = merged_tracks_df\n",
    "        save_dataframe_with_progress(merged_spots_df,  os.path.join(self.Results_Folder, 'merged_Spots.csv'), desc=\"Saving Spots\")\n",
    "        self.spots_data = merged_spots_df\n",
    "\n",
    "    def __load_csv__(self):\n",
    "        # Load Tracking data in memory\n",
    "        file_pattern = f'.*\\.{self.fileformat}$'\n",
    "        merged_spots_df = load_and_populate(self.Folder_path, file_pattern, skiprows = self.skiprows, usecols = self.usecols)\n",
    "        print(f\"Tracking data loaded in memory.\")\n",
    "        print(f\"These are its column names:{merged_spots_df.columns}\")\n",
    "        merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n",
    "        save_dataframe_with_progress(merged_spots_df, os.path.join(self.Results_Folder, 'merged_Spots.csv'), desc=\"Saving Spots\")\n",
    "        self.spots_data = merged_spots_df\n",
    "   \n",
    "        \n",
    "    def __load_trackmate_xml__(self):\n",
    "        # Load Tracking data in memory\n",
    "        merged_spots_df = load_and_populate(self.Folder_path, self.fileformat, skiprows = self.skiprows, usecols = self.usecols)\n",
    "        print(f\"Tracking data loaded in memory.\")\n",
    "        print(f\"These are its column names:{merged_spots_df.columns}\")\n",
    "        merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n",
    "        save_dataframe_with_progress(merged_spots_df, os.path.join(self.Results_Folder, 'merged_Spots.csv'), desc=\"Saving Spots\")\n",
    "        self.spots_data = merged_spots_df\n",
    "        \n",
    "    def __load_trackmate_csv__(self):\n",
    "        # Trackmate is composed of tracks and spots\n",
    "        # Load the tracking info data in memory\n",
    "        file_pattern = f'.*tracks.*\\.{self.fileformat}$'\n",
    "        merged_tracks_df = load_and_populate(self.Folder_path, file_pattern, skiprows = self.skiprows, usecols = self.usecols)\n",
    "        print(f\"Tracking data loaded in memory.\")\n",
    "        print(f\"These are its column names:{merged_tracks_df.columns}\")\n",
    "\n",
    "        if not validate_tracks_df(merged_tracks_df):\n",
    "            print(\"Error: Validation failed for merged tracks dataframe.\")\n",
    "        else:\n",
    "            merged_tracks_df = sort_and_generate_repeat(merged_tracks_df)\n",
    "            merged_tracks_df['Unique_ID'] = merged_tracks_df['File_name'] + \"_\" + merged_tracks_df['TRACK_ID'].astype(str)\n",
    "            save_dataframe_with_progress(merged_tracks_df, os.path.join(self.Results_Folder, 'merged_Tracks.csv'), desc=\"Saving Tracks\")\n",
    "        \n",
    "        # Load the spots data info in memory\n",
    "        file_pattern = f'.*spots.*\\.{self.fileformat}$'\n",
    "        merged_spots_df = load_and_populate(self.Folder_path, file_pattern, skiprows = self.skiprows, usecols = self.usecols)\n",
    "\n",
    "        if not validate_spots_df(merged_spots_df):\n",
    "            print(\"Error: Validation failed for merged spots dataframe.\")\n",
    "        else:\n",
    "            merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n",
    "            merged_spots_df['TRACK_ID'] = merged_spots_df['TRACK_ID'].astype(int)\n",
    "            merged_spots_df['Unique_ID'] = merged_spots_df['File_name'] + \"_\" + merged_spots_df['TRACK_ID'].astype(str)\n",
    "            merged_spots_df.dropna(subset=['POSITION_X', 'POSITION_Y', 'POSITION_Z'], inplace=True)\n",
    "            save_dataframe_with_progress(merged_spots_df, os.path.join(self.Results_Folder, 'merged_Spots.csv'), desc=\"Saving Spots\")\n",
    "            \n",
    "        self.spots_data = merged_spots_df\n",
    "        self.tracks_data = merged_tracks_df\n",
    "    \n",
    "    def CompileTrackingData(self):\n",
    "        ## Check the units and that all are the same\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.datatype==\"TrackMate Files\" and self.fileformat.contains(\"csv\"):\n",
    "            \n",
    "            self.skiprows=[1, 2, 3]\n",
    "            self.__load_trackmate_csv__()   \n",
    "            \n",
    "        elif self.datatype==\"Custom\" and self.fileformat.contains(\"csv\"):\n",
    "            \n",
    "            self.__load_csv__()\n",
    "            \n",
    "        elif self.datatype==\"Custom\" and self.fileformat.contains(\"xml\")\n",
    "            \n",
    "            self.__load_trackmate_xml__()\n",
    "        \n",
    "        elif self.datatype=\"TrackMate Table\":\n",
    "            \n",
    "            print(\"Loading track table file....\")\n",
    "            merged_tracks_df =  pd.read_csv(self.Track_table, low_memory=False)\n",
    "            if not validate_tracks_df(merged_tracks_df):\n",
    "                print(\"Error: Validation failed for loaded tracks dataframe.\")\n",
    "            self.tracks_data = merged_tracks_df\n",
    "    \n",
    "            print(\"Loading spot table file....\")\n",
    "            merged_spots_df = pd.read_csv(Spot_table, low_memory=False)\n",
    "            if not validate_spots_df(merged_spots_df):\n",
    "                print(\"Error: Validation failed for loaded spots dataframe.\")\n",
    "            self.spots_data = merged_spots_df\n",
    "        \n",
    "        # Now, call the check function\n",
    "        check_unique_id_match(self.spots_data , self.tracks_data)\n",
    "        \n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "        print(f\"Dataset processing completed in {elapsed_time:.2f} seconds.\")    \n",
    "\n",
    "\n",
    "\n",
    "# ------------\n",
    "#@markdown ###Or use a test dataset (up to 10 min download)\n",
    "Use_test_dataset = True #@param {type:\"boolean\"}\n",
    "\n",
    "T = TrackingData()\n",
    " \n",
    "if Use_test_dataset:\n",
    "    # Download the test dataset\n",
    "    T.DownloadTestData()\n",
    "T.CompileTrackingData()\n",
    "data = T.tracks_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d0e93-2e81-4a23-b1b5-c40841a45b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d18122d-962d-4e03-955d-c7ec79447f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>T</th>\n",
       "      <th>File_name</th>\n",
       "      <th>Condition</th>\n",
       "      <th>experiment_nb</th>\n",
       "      <th>Repeat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>114.585365</td>\n",
       "      <td>207.868122</td>\n",
       "      <td>2</td>\n",
       "      <td>Exp 1 20180126 ICAM 2</td>\n",
       "      <td>ICAM</td>\n",
       "      <td>R1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>82.839914</td>\n",
       "      <td>210.570742</td>\n",
       "      <td>9</td>\n",
       "      <td>Exp 1 20180126 ICAM 2</td>\n",
       "      <td>ICAM</td>\n",
       "      <td>R1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>76.097875</td>\n",
       "      <td>210.046280</td>\n",
       "      <td>11</td>\n",
       "      <td>Exp 1 20180126 ICAM 2</td>\n",
       "      <td>ICAM</td>\n",
       "      <td>R1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>68.878343</td>\n",
       "      <td>221.037571</td>\n",
       "      <td>15</td>\n",
       "      <td>Exp 1 20180126 ICAM 2</td>\n",
       "      <td>ICAM</td>\n",
       "      <td>R1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>79.246350</td>\n",
       "      <td>226.705611</td>\n",
       "      <td>17</td>\n",
       "      <td>Exp 1 20180126 ICAM 2</td>\n",
       "      <td>ICAM</td>\n",
       "      <td>R1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30310</th>\n",
       "      <td>219</td>\n",
       "      <td>422.149207</td>\n",
       "      <td>75.044851</td>\n",
       "      <td>19</td>\n",
       "      <td>Exp 3 20180905 VCAM 1</td>\n",
       "      <td>VCAM</td>\n",
       "      <td>R3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30311</th>\n",
       "      <td>220</td>\n",
       "      <td>264.429882</td>\n",
       "      <td>240.309097</td>\n",
       "      <td>19</td>\n",
       "      <td>Exp 3 20180905 VCAM 1</td>\n",
       "      <td>VCAM</td>\n",
       "      <td>R3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30312</th>\n",
       "      <td>220</td>\n",
       "      <td>265.892297</td>\n",
       "      <td>243.113919</td>\n",
       "      <td>20</td>\n",
       "      <td>Exp 3 20180905 VCAM 1</td>\n",
       "      <td>VCAM</td>\n",
       "      <td>R3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30313</th>\n",
       "      <td>221</td>\n",
       "      <td>268.117971</td>\n",
       "      <td>29.577792</td>\n",
       "      <td>19</td>\n",
       "      <td>Exp 3 20180905 VCAM 1</td>\n",
       "      <td>VCAM</td>\n",
       "      <td>R3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30314</th>\n",
       "      <td>221</td>\n",
       "      <td>266.804836</td>\n",
       "      <td>28.978508</td>\n",
       "      <td>20</td>\n",
       "      <td>Exp 3 20180905 VCAM 1</td>\n",
       "      <td>VCAM</td>\n",
       "      <td>R3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38822 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID           X           Y   T              File_name Condition  \\\n",
       "0        0  114.585365  207.868122   2  Exp 1 20180126 ICAM 2      ICAM   \n",
       "1        0   82.839914  210.570742   9  Exp 1 20180126 ICAM 2      ICAM   \n",
       "2        0   76.097875  210.046280  11  Exp 1 20180126 ICAM 2      ICAM   \n",
       "3        0   68.878343  221.037571  15  Exp 1 20180126 ICAM 2      ICAM   \n",
       "4        0   79.246350  226.705611  17  Exp 1 20180126 ICAM 2      ICAM   \n",
       "...    ...         ...         ...  ..                    ...       ...   \n",
       "30310  219  422.149207   75.044851  19  Exp 3 20180905 VCAM 1      VCAM   \n",
       "30311  220  264.429882  240.309097  19  Exp 3 20180905 VCAM 1      VCAM   \n",
       "30312  220  265.892297  243.113919  20  Exp 3 20180905 VCAM 1      VCAM   \n",
       "30313  221  268.117971   29.577792  19  Exp 3 20180905 VCAM 1      VCAM   \n",
       "30314  221  266.804836   28.978508  20  Exp 3 20180905 VCAM 1      VCAM   \n",
       "\n",
       "      experiment_nb  Repeat  \n",
       "0                R1       1  \n",
       "1                R1       1  \n",
       "2                R1       1  \n",
       "3                R1       1  \n",
       "4                R1       1  \n",
       "...             ...     ...  \n",
       "30310            R3       3  \n",
       "30311            R3       3  \n",
       "30312            R3       3  \n",
       "30313            R3       3  \n",
       "30314            R3       3  \n",
       "\n",
       "[38822 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.tracking_spots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d54bbcb-7b89-40ef-b7c5-7955201ab00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_mapping = { # Dictionary to map data\n",
    "            'TRACK_ID': \"ID\",\n",
    "            'POSITION_X': \"X\", \n",
    "            'POSITION_Y': \"Y\",\n",
    "            'POSITION_Z': \"\" ,\n",
    "            'POSITION_T': \"T\" \n",
    "        }\n",
    "column_mapping = {dropdown: col for col, dropdown in dim_mapping.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b5c28ec-591c-4c77-b53d-01618f214f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_mapping.pop('POSITION_Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "711b4da2-31d1-4eb8-a47f-d6a1fcc7a93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pixels', 4)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_calibration(filepath, line=3):\n",
    "    k=0\n",
    "    for row in open(filepath):\n",
    "        k+=1\n",
    "        if k>line:  \n",
    "            return row[37:43], k\n",
    "find_calibration(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cf0adbd-904e-4b74-8cc6-4ca312828df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pixels'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[37:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2dc0ea7-6176-4510-9a33-76f4e30c3ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(',,,,,,,,,(frames),(frames),(frames),')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
