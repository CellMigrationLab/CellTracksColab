{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF4zYMmXULP7"
      },
      "source": [
        "# **CellTracksColab Dimensionality Reduction**\n",
        "---\n",
        "\n",
        "\n",
        "<font size = 4>**Notebook Overview:**\n",
        "\n",
        "<font size = 4>This notebook is designed for analyzing datasets stored in the CellTracksColab format, utilizing advanced dimensionality reduction techniques to facilitate the interpretation of complex, high-dimensional data.\n",
        "\n",
        "<font size = 4>**Techniques Employed:**\n",
        "\n",
        "- <font size = 4>**UMAP (Uniform Manifold Approximation and Projection):** UMAP is a powerful method for dimensionality reduction that preserves as much of the local and global structure of the data as possible. It's particularly suited for large datasets and is capable of revealing intricate structures within the data that other techniques might miss.\n",
        "\n",
        "- <font size = 4>**t-SNE (t-distributed Stochastic Neighbor Embedding):** t-SNE is another highly effective technique used for exploring high-dimensional data. It converts affinities of data points to probabilities and then tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\n",
        "\n",
        "- <font size = 4>**HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise):** Complementing the above dimensionality reduction techniques, HDBSCAN identifies clusters using a density-based approach. This method excels in finding clusters of varying densities and sizes from data shaped by UMAP or t-SNE, making it invaluable for discerning the nuanced groupings within complex datasets.\n",
        "\n",
        "<font size = 4>**Objective:**\n",
        "\n",
        "<font size = 4>The goal of this notebook is to make complex, high-dimensional data more interpretable and actionable by applying these techniques. This approach not only aids in visualizing the data in two or three dimensions but also in identifying inherent clusters and patterns critical for further analysis and decision-making processes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR2U8v9YoJcW"
      },
      "source": [
        "# **Before getting started**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JrkfFr7mgZmA"
      },
      "outputs": [],
      "source": [
        "# @title #MIT License\n",
        "\n",
        "print(\"\"\"\n",
        "**MIT License**\n",
        "\n",
        "Copyright (c) 2023 Guillaume Jacquemet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-Ft-yNRVCc"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 0. Prepare the Google Colab session**\n",
        "--------------------------------------------------------\n",
        "<font size = 4>skip this section when using a local installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h0prdayn0qG"
      },
      "source": [
        "## **0.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_BZuYOQGo1p"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to install\n",
        "\n",
        "print(\"In progress....\")\n",
        "\n",
        "!git clone --single-branch --branch dev-egm https://github.com/CellMigrationLab/CellTracksColab.git\n",
        "\n",
        "%pip -q install pandas scikit-learn\n",
        "%pip -q install hdbscan\n",
        "%pip -q install umap-learn\n",
        "%pip -q install plotly\n",
        "%pip -q install tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzd_8GUnpbw"
      },
      "source": [
        "## **0.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GA1wCrkoV4i5"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/Gdrive')\n",
        "# This command was originally but I think it doesn't do anything really\n",
        "## %cd /gdrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etkbaQ9fm5u2"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 1. Prepare the session and load the data**\n",
        "--------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n07c40vn6CK"
      },
      "source": [
        "## **1.1 Load key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rAP0ahCzn1V6"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to load the dependancies\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import requests\n",
        "import ipywidgets as widgets\n",
        "import warnings\n",
        "import scipy.stats as stats\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from ipywidgets import Dropdown, interact,Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display, clear_output\n",
        "from scipy.spatial import ConvexHull\n",
        "from scipy.spatial.distance import cosine, pdist\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.stats import zscore, ks_2samp\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from multiprocessing import Pool\n",
        "from matplotlib.ticker import FixedLocator\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "sys.path.append(\"../\")\n",
        "sys.path.append(\"CellTracksColab/\")\n",
        "\n",
        "import celltracks\n",
        "from celltracks import *\n",
        "from celltracks.Track_Plots import *\n",
        "from celltracks.BoxPlots_Statistics import *\n",
        "from celltracks.Dimensionality_Reduction import *\n",
        "\n",
        "\n",
        "# Current version of the notebook the user is running\n",
        "current_version = \"1.0.1\"\n",
        "Notebook_name = 'Dimensionality_Reduction'\n",
        "\n",
        "# URL to the raw content of the version file in the repository\n",
        "version_url = \"https://raw.githubusercontent.com/guijacquemet/CellTracksColab/main/Notebook/latest_version.txt\"\n",
        "\n",
        "# Function to define colors for formatting messages\n",
        "class bcolors:\n",
        "    WARNING = '\\033[91m'  # Red color for warning messages\n",
        "    ENDC = '\\033[0m'      # Reset color to default\n",
        "\n",
        "# Check if this is the latest version of the notebook\n",
        "try:\n",
        "    All_notebook_versions = pd.read_csv(version_url, dtype=str)\n",
        "    print('Notebook version: ' + current_version)\n",
        "\n",
        "    # Check if 'Version' column exists in the DataFrame\n",
        "    if 'Version' in All_notebook_versions.columns:\n",
        "        Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Notebook_name]['Version'].iloc[0]\n",
        "        print('Latest notebook version: ' + Latest_Notebook_version)\n",
        "\n",
        "        if current_version == Latest_Notebook_version:\n",
        "            print(\"This notebook is up-to-date.\")\n",
        "        else:\n",
        "            print(bcolors.WARNING + \"A new version of this notebook has been released. We recommend that you download it at https://github.com/guijacquemet/CellTracksColab\" + bcolors.ENDC)\n",
        "    else:\n",
        "        print(\"The 'Version' column is not present in the version file.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"Unable to fetch the latest version information. Please check your internet connection.\")\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", str(e))\n",
        "\n",
        "#----------------------- Key functions -----------------------------#\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2zxeUs-m5u2"
      },
      "source": [
        "## **1.2. Load your CellTracksColab dataset**\n",
        "---\n",
        "\n",
        "<font size=\"4\"> Before proceeding, please ensure that your data has been properly processed using CellTracksColab. Typically, your Track table should be named `merged_Tracks.csv`, and your Spot table should be named `merged_Spots.csv`.\n",
        "\n",
        "For the `Results_Folder` parameter, you can choose the same folder that already contains all the results associated with your dataset. Any results generated by this notebook will be saved in the `Distance_to_ROI` subfolder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZWqpUp6BjcSR"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Provide the path to your CellTracksColab dataset:\n",
        "\n",
        "Data_Dims = \"2D\" #@param [\"2D\", \"3D\"]\n",
        "Data_Type = \"CellTracksColab\"\n",
        "\n",
        "Track_table = ''  # @param {type: \"string\"}\n",
        "Spot_table = ''  # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "Use_test_dataset = False\n",
        "\n",
        "#@markdown ###Provide the path to your Result folder\n",
        "\n",
        "Results_Folder = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "# Update the parameters to load the data\n",
        "CellTracks = celltracks.TrackingData()\n",
        "if Use_test_dataset:\n",
        "    # Download the test dataset\n",
        "    test_celltrackscolab = \"https://zenodo.org/record/8420011/files/T_Cells_spots_only.zip?download=1\"\n",
        "    CellTracks.DownloadTestData(test_celltrackscolab)\n",
        "    File_Format = \"csv\"\n",
        "else:\n",
        "\n",
        "    CellTracks.Spot_table = Spot_table\n",
        "    CellTracks.Track_table = Track_table\n",
        "\n",
        "CellTracks.Results_Folder = Results_Folder\n",
        "CellTracks.skiprows = None\n",
        "CellTracks.data_type = Data_Type\n",
        "CellTracks.data_dims = Data_Dims\n",
        "\n",
        "# Load data\n",
        "CellTracks.LoadTrackingData()\n",
        "\n",
        "merged_spots_df = CellTracks.spots_data\n",
        "check_for_nans(merged_spots_df, \"merged_spots_df\")\n",
        "merged_tracks_df = CellTracks.tracks_data\n",
        "print(\"...Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpyycyZShCF5"
      },
      "source": [
        "--------\n",
        "# **Part 2. Explore your high-dimensional data using UMAP and HDBSCAN**\n",
        "--------\n",
        "\n",
        "<font size = 4> The workflow provided below is inspired by [CellPlato](https://github.com/Michael-shannon/cellPLATO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lFNGk1ChCF6"
      },
      "source": [
        "## **2.1. Choose the track metrics to use for clustering**\n",
        "--------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rigx28S3BoZd"
      },
      "outputs": [],
      "source": [
        "# @title ##Choose the track metrics to use\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Check and create \"pdf\" folder\n",
        "if not os.path.exists(os.path.join(Results_Folder, \"Umap\")):\n",
        "    os.makedirs(os.path.join(Results_Folder, \"Umap\"), exist_ok=True)\n",
        "\n",
        "\n",
        "excluded_columns = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n",
        "\n",
        "# Columns you want to always include\n",
        "columns_to_include = ['File_name', 'Repeat', 'Condition', 'Unique_ID']\n",
        "\n",
        "selected_df = pd.DataFrame()\n",
        "nan_columns = pd.DataFrame()\n",
        "# Extract the columns you always want to include and ensure they exist in the original dataframe\n",
        "saved_columns = {col: merged_tracks_df[col].copy() for col in columns_to_include if col in merged_tracks_df}\n",
        "\n",
        "# Filter out non-numeric columns\n",
        "numeric_df = merged_tracks_df.select_dtypes(include=['float64', 'int64'])  # Selecting only numeric columns\n",
        "\n",
        "column_names = [col for col in numeric_df.columns if col not in excluded_columns]\n",
        "\n",
        "# Text area for user to paste the list of metrics\n",
        "text_area = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Copy and paste your list of metrics here, separated by commas.',\n",
        "    description='Metrics:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "\n",
        "# Function to parse the text area content into a list\n",
        "def parse_text_area(text):\n",
        "    return [item.strip() for item in text.split(',') if item.strip() in column_names]\n",
        "\n",
        "# Create a checkbox for each column\n",
        "def create_checkboxes(parsed_metrics):\n",
        "    return [widgets.Checkbox(value=(col in parsed_metrics or not parsed_metrics), description=col, indent=False) for col in column_names]\n",
        "\n",
        "checkboxes = create_checkboxes(column_names)  # Initialize with all metrics\n",
        "\n",
        "# Grid for displaying checkboxes\n",
        "grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n",
        "\n",
        "# Create a button to trigger the selection\n",
        "button = widgets.Button(description=\"Select the track parameters\",\n",
        "                        layout=widgets.Layout(width='400px'),\n",
        "                        button_style='info')\n",
        "\n",
        "\n",
        "def on_button_click(b):\n",
        "    global selected_df\n",
        "    global nan_columns\n",
        "    parsed_metrics = parse_text_area(text_area.value)\n",
        "    selected_columns = [box.description for box in checkboxes if box.value]\n",
        "\n",
        "    # Extract the selected columns from the DataFrame\n",
        "    selected_df = numeric_df[selected_columns].copy()\n",
        "\n",
        "        # Prepare the parameters dictionary\n",
        "    UMAP_params = {\n",
        "        'Selected Columns': ', '.join(selected_columns)\n",
        "    }\n",
        "\n",
        "    # Save the parameters\n",
        "    params_file_path = os.path.join(Results_Folder, \"Umap\", \"analysis_parameters.csv\")\n",
        "    save_parameters(UMAP_params, params_file_path, 'UMAP')\n",
        "\n",
        "    # Add back the always-included columns to selected_df\n",
        "    for col, data in saved_columns.items():\n",
        "        selected_df.loc[:, col] = data\n",
        "\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n",
        "    if nan_columns:\n",
        "        for col in nan_columns:\n",
        "            selected_df = selected_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "# Set the button click event handler\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "# Function to update checkboxes based on text area input\n",
        "def update_checkboxes(b):\n",
        "    parsed_metrics = parse_text_area(text_area.value)\n",
        "    global checkboxes\n",
        "    checkboxes = create_checkboxes(parsed_metrics)\n",
        "    grid.children = checkboxes\n",
        "\n",
        "# Update checkboxes when text area content changes\n",
        "text_area.observe(update_checkboxes, names='value')\n",
        "\n",
        "# Display the text area, grid of checkboxes, and the button\n",
        "display(text_area, grid, button)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-MPZE_IhCF7"
      },
      "source": [
        "## **2.2. UMAP**\n",
        "---\n",
        "\n",
        "<font size = 4> The given code performs UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction on the merged tracks dataframe, focusing on its numeric columns, and visualizes the result. In the provided UMAP code, the parameters `n_neighbors`, `min_dist`, and `n_components` are crucial for determining the structure and appearance of the resulting low-dimensional representation of the data.\n",
        "\n",
        "<font size = 4>`n_neighbors`: This parameter controls how UMAP balances local versus global structure in the data. It determines the size of the local neighborhood UMAP will look at when learning the manifold structure of the data.\n",
        "- A smaller value emphasizes the local structure of the data, potentially at the expense of the global structure.\n",
        "- A larger value allows UMAP to consider more distant neighbors, emphasizing more on the global structure of the data.\n",
        "- Typically, values in the range of 5 to 50 are chosen, depending on the density and scale of the data.\n",
        "\n",
        "<font size = 4>`min_dist`: This parameter controls how tightly UMAP is allowed to pack points together. It determines the minimum distance between points in the low-dimensional representation.\n",
        "- Setting it to a low value will allow points to be packed more closely, potentially revealing clusters in the data.\n",
        "- A higher value ensures that points are more spread out in the representation.\n",
        "- Values usually range between 0 and 1.\n",
        "\n",
        "<font size = 4>`n_dimension`: This parameter determines the number of dimensions in the low-dimensional space that the data will be reduced to.\n",
        "For visualization purposes, `n_dimension` is typically set to 2 or 3 to obtain 2D or 3D representations, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YbZmlpGFhCF7"
      },
      "outputs": [],
      "source": [
        "# @title ##Perform UMAP\n",
        "import umap\n",
        "import plotly.offline as pyo\n",
        "\n",
        "#@markdown ###UMAP parameters:\n",
        "\n",
        "n_neighbors = 10  # @param {type: \"number\"}\n",
        "min_dist = 0  # @param {type: \"number\"}\n",
        "n_dimension = 2  # @param {type: \"slider\", min: 1, max: 3}\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 30 # @param {type: \"number\"}\n",
        "\n",
        "# Initialize UMAP object with the specified settings\n",
        "reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_dimension, random_state=42)\n",
        "# Exclude non-numeric columns when fitting UMAP\n",
        "embedding = reducer.fit_transform(selected_df.drop(columns=columns_to_include))\n",
        "# Create dynamic column names based on n_components\n",
        "column_names = [f'UMAP dimension {i}' for i in range(1, n_dimension + 1)]\n",
        "\n",
        "# Extract the columns_to_include from selected_df\n",
        "included_data = selected_df[columns_to_include].reset_index(drop=True)\n",
        "\n",
        "# Concatenate the UMAP embedding with the included columns\n",
        "umap_df = pd.concat([pd.DataFrame(embedding, columns=column_names), included_data], axis=1)\n",
        "\n",
        "\n",
        "# Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "nan_columns = umap_df.columns[umap_df.isna().any()].tolist()\n",
        "\n",
        "if nan_columns:\n",
        "  warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n",
        "  for col in nan_columns:\n",
        "    umap_df = umap_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n",
        "\n",
        "  # Prepare the parameters dictionary\n",
        "UMAP_params = {\n",
        "        'n_neighbors': n_neighbors,\n",
        "        'min_dist': min_dist,\n",
        "        'n_dimension': n_dimension\n",
        "    }\n",
        "\n",
        "    # Save the parameters\n",
        "params_file_path = os.path.join(Results_Folder, \"Umap\",\"analysis_parameters.csv\")\n",
        "save_parameters(UMAP_params, params_file_path, 'UMAP')\n",
        "\n",
        "# Visualize the UMAP projection\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# The plot will adjust automatically based on the n_components\n",
        "if n_dimension == 2:\n",
        "    sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=umap_df, palette='Set2', s=spot_size)\n",
        "    plt.title('UMAP Projection of the Dataset')\n",
        "    plt.savefig(f\"{Results_Folder}/Umap/umap_projection_2D.pdf\")  # Save 2D plot as PDF\n",
        "    plt.show()\n",
        "elif n_dimension == 1:\n",
        "    sns.stripplot(x=column_names[0], hue='Condition', data=umap_df, palette='Set2', jitter=0.05, size=spot_size)\n",
        "    plt.title('UMAP Projection of the Dataset')\n",
        "    plt.savefig(f\"{Results_Folder}/Umap/umap_projection_1D.pdf\")  # Save 2D plot as PDF\n",
        "    plt.show()\n",
        "else:\n",
        "    # umap_df should have columns like 'UMAP dimension 1', 'UMAP dimension 2', 'UMAP dimension 3', and 'condition'\n",
        "    import plotly.express as px\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    fig = px.scatter_3d(umap_df,\n",
        "                    x='UMAP dimension 1',\n",
        "                    y='UMAP dimension 2',\n",
        "                    z='UMAP dimension 3',\n",
        "                    color='Condition')\n",
        "\n",
        "    for trace in fig.data:\n",
        "      trace.marker.size = spot_size/10  # You can set this to any desired value\n",
        "\n",
        "    fig.show()\n",
        "    pyo.plot(fig, filename=os.path.join(Results_Folder, \"Umap\", \"umap_projection.html\"), auto_open=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Wc3LoNhCF8"
      },
      "source": [
        "## **2.3. HDBSCAN**\n",
        "---\n",
        "\n",
        "<font size=\"4\">The provided code employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify clusters within a dataset that has already undergone UMAP dimensionality reduction. HDBSCAN is utilized for its proficiency in determining the optimal number of clusters while managing varied densities within the data.</font>\n",
        "\n",
        "<font size=\"4\">In the provided HDBSCAN code, the parameters `min_samples`, `min_cluster_size`, and `metric` are crucial for determining the structure and appearance of the resulting clusters in the data.</font>\n",
        "\n",
        "<font size=\"4\">`min_samples`: This parameter primarily controls the degree to which the algorithm is willing to declare noise. It's the number of samples in a neighborhood for a point to be considered as a core point.</font>\n",
        "- <font size=\"4\">A smaller value of `min_samples` makes the algorithm more prone to declaring points as part of a cluster, potentially leading to larger clusters and fewer noise points.</font>\n",
        "- <font size=\"4\">A larger value makes the algorithm more conservative, resulting in more points declared as noise and smaller, more defined clusters.</font>\n",
        "- <font size=\"4\">The choice of `min_samples` typically depends on the density of the data; denser datasets may require a larger value.</font>\n",
        "\n",
        "<font size=\"4\">`min_cluster_size`: This parameter determines the smallest size grouping that you wish to consider a cluster.</font>\n",
        "- <font size=\"4\">A smaller value will allow the formation of smaller clusters, whereas a larger value will prevent small isolated groups of points from being declared as clusters.</font>\n",
        "- <font size=\"4\">The choice of `min_cluster_size` depends on the scale of the data and the desired level of granularity in the clustering.</font>\n",
        "\n",
        "<font size=\"4\">`metric`: This parameter is the metric used for distance computation between data points, and it affects the shape of the clusters.</font>\n",
        "- <font size=\"4\">The `euclidean` metric is a good starting point, and depending on the clustering results and the data type, it might be beneficial to experiment with different metrics.</font>\n",
        "\n",
        "---\n",
        "\n",
        "| <font size=\"4\">Metric</font> | <font size=\"4\">Description</font>                                   | <font size=\"4\">Data Type</font>            |\n",
        "|-------------------|-------------------------------------------------------|-------------------------------|\n",
        "| <font size=\"4\">Euclidean</font>   | <font size=\"4\">Standard distance metric.</font>                          | <font size=\"4\">Numerical data.</font>               |\n",
        "| <font size=\"4\">Manhattan</font>   | <font size=\"4\">Sum of absolute differences.</font>                       | <font size=\"4\">Numerical/Categorical data.</font>   |\n",
        "| <font size=\"4\">Chebyshev</font>   | <font size=\"4\">Maximum value of absolute differences.</font>             | <font size=\"4\">Numerical data.</font>               |\n",
        "| <font size=\"4\">Bray-Curtis</font> | <font size=\"4\">Dissimilarity between sample sets.</font>                 | <font size=\"4\">Numerical data.</font>               |\n",
        "| <font size=\"4\">Canberra</font>    | <font size=\"4\">Weighted version of Manhattan distance.</font>            | <font size=\"4\">Numerical data.</font>               |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oWaRUmCxhCF8"
      },
      "outputs": [],
      "source": [
        "# @title ##Identify clusters using HDBSCAN\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#@markdown ###HDBSCAN parameters:\n",
        "clustering_data_source = 'umap'  # @param ['umap', 'raw']\n",
        "min_samples = 20  # @param {type: \"number\"}\n",
        "min_cluster_size = 200  # @param {type: \"number\"}\n",
        "metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'braycurtis', 'canberra']\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 30 # @param {type: \"number\"}\n",
        "# Apply HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)  # You may need to tune these parameters\n",
        "\n",
        "if clustering_data_source == 'umap':\n",
        "  if n_dimension == 1:\n",
        "    clusterer.fit(umap_df[['UMAP dimension 1']])  # Use only one UMAP dimension for clustering\n",
        "\n",
        "  elif n_dimension == 2:\n",
        "    clusterer.fit(umap_df[['UMAP dimension 1', 'UMAP dimension 2']])  # Use two UMAP dimensions for clustering\n",
        "\n",
        "  elif n_dimension == 3:\n",
        "    clusterer.fit(umap_df[['UMAP dimension 1', 'UMAP dimension 2', 'UMAP dimension 3']])  # Use three UMAP dimensions for clustering\n",
        "\n",
        "else:\n",
        "  clusterer.fit(selected_df.select_dtypes(include=['number']))\n",
        "\n",
        "# Add the cluster labels to your UMAP DataFrame\n",
        "umap_df['Cluster_UMAP'] = clusterer.labels_\n",
        "\n",
        "# If the Cluster column already exists in merged_tracks_df, drop it to avoid duplications\n",
        "if 'Cluster_UMAP' in merged_tracks_df.columns:\n",
        "    merged_tracks_df.drop(columns='Cluster_UMAP', inplace=True)\n",
        "\n",
        "# Merge the Cluster column from umap_df to merged_tracks_df based on Unique_ID\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, umap_df[['Unique_ID', 'Cluster_UMAP']], on='Unique_ID', how='left')\n",
        "\n",
        "# Handle cases where some rows in merged_tracks_df might not have a corresponding cluster label\n",
        "merged_tracks_df['Cluster_UMAP'].fillna(-1, inplace=True)  # Assigning -1 to cells that were not assigned to any cluster\n",
        "\n",
        "# Save the DataFrame with the identified clusters\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n",
        "\n",
        "  # Prepare the parameters dictionary\n",
        "UMAP_params = {\n",
        "        'clustering_data_source': clustering_data_source,\n",
        "        'min_samples': min_samples,\n",
        "        'min_cluster_size': min_cluster_size,\n",
        "        'metric': metric\n",
        "    }\n",
        "\n",
        "    # Save the parameters\n",
        "params_file_path = os.path.join(Results_Folder, \"Umap/analysis_parameters.csv\")\n",
        "save_parameters(UMAP_params, params_file_path, 'HDBSCAN')\n",
        "\n",
        "# Plotting the results\n",
        "if n_dimension == 1:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.stripplot(data=umap_df, x='UMAP dimension 1', hue='Cluster_UMAP', palette='viridis', s=spot_size)\n",
        "    plt.title('Clusters Identified by HDBSCAN (1D)')\n",
        "    plt.xlabel('UMAP dimension 1')\n",
        "    plt.ylabel('Count')\n",
        "    plt.savefig(f\"{Results_Folder}/Umap/HDBSCAN_clusters_1D.pdf\")  # Save 1D histogram as PDF\n",
        "    plt.show()\n",
        "\n",
        "if n_dimension == 2:\n",
        "\n",
        "  plt.figure(figsize=(12,10))\n",
        "  sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', hue='Cluster_UMAP', palette='viridis', data=umap_df, s=spot_size)\n",
        "  plt.title('Clusters Identified by HDBSCAN')\n",
        "  plt.savefig(os.path.join(Results_Folder, \"Umap\", \"HDBSCAN_clusters_2D.pdf\"))  # Save 2D plot as PDF\n",
        "  plt.show()\n",
        "\n",
        "if n_dimension == 3:\n",
        "\n",
        "  fig = px.scatter_3d(umap_df,\n",
        "                    x='UMAP dimension 1',\n",
        "                    y='UMAP dimension 2',\n",
        "                    z='UMAP dimension 3',\n",
        "                    color='Cluster_UMAP')\n",
        "\n",
        "  for trace in fig.data:\n",
        "    trace.marker.size = spot_size/10\n",
        "\n",
        "  fig.show()\n",
        "  pyo.plot(fig, filename=os.path.join(Results_Folder, \"Umap\", \"HDBSCAN_clusters.html\"), auto_open=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwqcM2-dhCF8"
      },
      "source": [
        "## **2.4. Fingerprint**\n",
        "---\n",
        "\n",
        "<font size = 4>This section is designed to visualize the distribution of different clusters within each condition in a dataset, showing the 'fingerprint' of each cluster per condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yjooPN_ghCF9"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot the 'fingerprint' of each cluster per condition\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# Group by 'Condition' and 'Cluster' and calculate the size of each group\n",
        "cluster_counts = umap_df.groupby(['Condition', 'Cluster_UMAP']).size().reset_index(name='counts')\n",
        "\n",
        "# Calculate the total number of points per condition\n",
        "total_counts = umap_df.groupby('Condition').size().reset_index(name='total_counts')\n",
        "\n",
        "# Merge the DataFrames on 'Condition' to calculate percentages\n",
        "percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n",
        "percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n",
        "\n",
        "# Save the percentage_df DataFrame as a CSV file\n",
        "percentage_df.to_csv(Results_Folder+'/Umap/UMAP_percentage_results.csv', index=False)\n",
        "\n",
        "# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n",
        "pivot_df = percentage_df.pivot(index='Condition', columns='Cluster_UMAP', values='percentage')\n",
        "\n",
        "# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n",
        "pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_pages = PdfPages(os.path.join(Results_Folder, 'Umap', 'UMAP_Cluster_Fingerprint_Plot.pdf'))\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n",
        "plt.title('Percentage in each cluster per Condition')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Condition')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ot8Qn-PhCF9"
      },
      "source": [
        "## **2.5. Understand your clusters using heatmaps**\n",
        "--------\n",
        "\n",
        "<font size = 4>This section help visualize how different track parameters vary across the identified clusters. The approach is to display these variations using a heatmap, which offers a color-coded representation of the median values of each parameter for each cluster. This visualization technique can make it easier to spot differences or patterns among the clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RYgcVxG-meLP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# @title ##Plot track normalized track parameters based on clusters as an heatmap\n",
        "\n",
        "# Parameters to adapt in function of the notebook section\n",
        "base_folder = os.path.join(Results_Folder, \"Umap\", \"Track_parameters\")\n",
        "Conditions = 'Cluster_UMAP'\n",
        "\n",
        "# Check and create necessary directories\n",
        "folders = [\"pdf\", \"csv\"]\n",
        "for folder in folders:\n",
        "    dir_path = os.path.join(base_folder, folder)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "\n",
        "heatmap_comparison(merged_tracks_df, base_folder, Conditions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIl59KNXhCF9"
      },
      "source": [
        "## **2.6. Understand your clusters using box plots**\n",
        "--------\n",
        "\n",
        "<font size = 4>The provided code aims to visually represent the distribution of different track parameters across the identified clusters. Specifically, for each parameter selected, a boxplot is generated to showcase the spread of its values across different clusters. This approach provides a comprehensive view of how each track parameter varies within and across the clusters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Np6Sqxl1hCF9"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot track parameters for each clusters\n",
        "\n",
        "base_folder = f\"{Results_Folder}/Umap/Track_parameters/\"\n",
        "Cluster = \"Cluster_UMAP\"\n",
        "\n",
        "if not os.path.exists(base_folder):\n",
        "  os.makedirs(base_folder)\n",
        "\n",
        "checkboxes_dict, checkboxes_accordion = display_variable_checkboxes(categorize_columns(merged_tracks_df))\n",
        "variable_checkboxes, checkboxes_widget = display_variable_checkboxes(get_selectable_columns_plots(merged_tracks_df))\n",
        "\n",
        "# Create and display the plot button\n",
        "button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n",
        "button.on_click(lambda b: plot_selected_vars_per_cluster(b, Cluster, checkboxes_dict, merged_tracks_df, base_folder));\n",
        "\n",
        "# Display the UI components\n",
        "display(VBox([checkboxes_accordion, button]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UptYX85jhCF-"
      },
      "source": [
        "## **2.7. Plot track parameters for a selected cluster**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bV07WmEV0QAM"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot track parameters for a selected cluster\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, VBox, Button, Accordion\n",
        "import pandas as pd\n",
        "import os\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from matplotlib.ticker import FixedLocator\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "# Parameters to adapt in function of the notebook section\n",
        "base_folder = f\"{Results_Folder}/Umap/cluster_plots\"\n",
        "Conditions = 'Condition'\n",
        "df_to_plot = merged_tracks_df\n",
        "Cluster = \"Cluster_UMAP\"\n",
        "\n",
        "# Check and create necessary directories\n",
        "folders = [\"pdf\", \"csv\"]\n",
        "for folder in folders:\n",
        "    dir_path = os.path.join(base_folder, folder)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "condition_selector, condition_accordion = display_condition_selection(df_to_plot, Conditions)\n",
        "checkboxes_dict, checkboxes_accordion = display_variable_checkboxes(categorize_columns(df_to_plot))\n",
        "variable_checkboxes, checkboxes_widget = display_variable_checkboxes(get_selectable_columns_plots(df_to_plot))\n",
        "stat_method_selector = widgets.Dropdown(\n",
        "    options=['randomization test', 't-test'],\n",
        "    value='randomization test',\n",
        "    description='Stat Method:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "cluster_dropdown = display_cluster_dropdown(merged_tracks_df, Cluster)\n",
        "\n",
        "button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n",
        "button.on_click(lambda b: plot_selected_vars_cluster(b, checkboxes_dict, df_to_plot, Conditions, Cluster, cluster_dropdown, base_folder, condition_selector, stat_method_selector));\n",
        "\n",
        "display(VBox([condition_accordion, checkboxes_accordion, stat_method_selector, cluster_dropdown, button]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHxA5KwwhCF_"
      },
      "source": [
        "## **2.8. Identify exemplar tracks from each clusters**\n",
        "---\n",
        "\n",
        "<font size = 4>Exemplars, in the context of clustering analysis, refer to representative data points that are selected to encapsulate the essential characteristics of a cluster. They are often chosen because they are central or prototypical members of a cluster, making them valuable for summarizing the cluster's properties. In the provided code, exemplars are identified using the HDBSCAN clustering algorithm and marked within the dataset.\n",
        "\n",
        "<font size = 4>**Keep in mind that not all cluster will have examplar**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1PbLJH7khCF_"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px  # Importing plotly for 3D plots\n",
        "\n",
        "# @title ##Identify exemplar tracks using HDBSCAN\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 15 # @param {type: \"number\"}\n",
        "\n",
        "# Check and create \"pdf\" folder\n",
        "if not os.path.exists(f\"{Results_Folder}/Umap/Examplar\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Umap/Examplar\")\n",
        "\n",
        "# Extracting exemplar points\n",
        "exemplars = []\n",
        "for exemplar in clusterer.exemplars_:\n",
        "    exemplars.extend(exemplar)\n",
        "\n",
        "# Flatten the exemplars list of lists into a single list\n",
        "flattened_exemplars = [index for sublist in exemplars for index in sublist]\n",
        "\n",
        "# Now pass the flattened list to iloc\n",
        "exemplar_df = umap_df.iloc[flattened_exemplars]\n",
        "\n",
        "# Deduplicate exemplar_df based on 'Unique_ID'\n",
        "exemplar_df = exemplar_df.drop_duplicates(subset='Unique_ID')\n",
        "\n",
        "# Create a new column in exemplar_df to indicate it's an exemplar\n",
        "exemplar_df['Exemplar'] = 1\n",
        "\n",
        "# If the Exemplar column already exists in merged_tracks_df, drop it to avoid duplications\n",
        "if 'Exemplar' in merged_tracks_df.columns:\n",
        "    merged_tracks_df.drop(columns='Exemplar', inplace=True)\n",
        "\n",
        "# Merge the Exemplar column from exemplar_df to merged_tracks_df based on Unique_ID\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, exemplar_df[['Unique_ID', 'Exemplar']], on='Unique_ID', how='left')\n",
        "\n",
        "# Handle cases where some rows in merged_tracks_df might not have a corresponding Exemplar label\n",
        "merged_tracks_df['Exemplar'].fillna(0, inplace=True)  # Assigning 0 to cells that were not identified as exemplars\n",
        "\n",
        "# Save the DataFrame with the identified clusters Exemplar label\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n",
        "\n",
        "\n",
        "# Plotting clusters and exemplar points\n",
        "if n_dimension == 1:\n",
        "    plt.figure(figsize=(12,10))\n",
        "    sns.stripplot(x='UMAP dimension 1', hue='Cluster_UMAP', data=umap_df, palette='viridis', jitter=0.05, size=spot_size)\n",
        "    sns.stripplot(x='UMAP dimension 1', color='red', label='Exemplars', data=exemplar_df, jitter=0.05, size=spot_size, marker='X')\n",
        "    plt.title('Clusters and Exemplar tracks Identified by HDBSCAN')\n",
        "    plt.show()\n",
        "\n",
        "elif n_dimension == 2:\n",
        "    plt.figure(figsize=(12,10))\n",
        "    sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', hue='Cluster_UMAP', palette='viridis', data=umap_df, s=spot_size)\n",
        "    sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', color='red', label='Exemplars', data=exemplar_df, s=spot_size*2, marker='X')\n",
        "    plt.title('Clusters and Exemplar tracks Identified by HDBSCAN')\n",
        "    plt.show()\n",
        "\n",
        "elif n_dimension == 3:\n",
        "    fig = px.scatter_3d(umap_df,\n",
        "                        x='UMAP dimension 1',\n",
        "                        y='UMAP dimension 2',\n",
        "                        z='UMAP dimension 3',\n",
        "                        color='Cluster_UMAP',\n",
        "                        color_discrete_sequence=px.colors.qualitative.Vivid)\n",
        "\n",
        "    # Add a new column for coloring exemplars\n",
        "    exemplar_df['ExemplarColor'] = 'Exemplar'\n",
        "    exemplar_fig = px.scatter_3d(exemplar_df,\n",
        "                                 x='UMAP dimension 1',\n",
        "                                 y='UMAP dimension 2',\n",
        "                                 z='UMAP dimension 3',\n",
        "                                 color='ExemplarColor',\n",
        "                                 color_discrete_map={'Exemplar':'red'})\n",
        "\n",
        "    for trace in fig.data:\n",
        "        trace.marker.size = spot_size\n",
        "\n",
        "    for trace in exemplar_fig.data:\n",
        "        trace.marker.size = spot_size\n",
        "        trace.marker.symbol = 'x'\n",
        "\n",
        "    fig.add_trace(exemplar_fig.data[0])\n",
        "    fig.show()\n",
        "    pyo.plot(fig, filename=f\"{Results_Folder}/Umap/Examplar/HDBSCAN_examplar.html\", auto_open=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1R2WCEvhCGA"
      },
      "source": [
        "## **2.9. See the exemplar tracks**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EmB6wQt7hCGA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# @title ##Plot the examplar tracks for each cluster\n",
        "\n",
        "# Check and create \"pdf\" folder\n",
        "if not os.path.exists(f\"{Results_Folder}/Umap/Examplar\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Umap/Examplar\")\n",
        "\n",
        "\n",
        "# Extracting actual indices for exemplar rows\n",
        "exemplar_indices = umap_df.iloc[flattened_exemplars].index\n",
        "\n",
        "# Add a new column to umap_df to indicate if a point is an exemplar\n",
        "umap_df['Exemplar'] = 0\n",
        "\n",
        "# Mark the rows corresponding to exemplars as 1\n",
        "umap_df.loc[exemplar_indices, 'Exemplar'] = 1\n",
        "\n",
        "# Determine max and min coordinates from the DataFrame\n",
        "min_x = merged_spots_df['POSITION_X'].min()\n",
        "max_x = merged_spots_df['POSITION_X'].max()\n",
        "min_y = merged_spots_df['POSITION_Y'].min()\n",
        "max_y = merged_spots_df['POSITION_Y'].max()\n",
        "\n",
        "# Extract exemplars from the umap_df\n",
        "exemplar_info = umap_df[umap_df['Exemplar'] == 1]\n",
        "\n",
        "# Determine the unique clusters from exemplar_info\n",
        "clusters = exemplar_info['Cluster_UMAP'].unique()\n",
        "\n",
        "# Create a PDF object to save the plots\n",
        "with PdfPages(f\"{Results_Folder}/Umap/Examplar/Examplar_tracks_Clusters.pdf\") as pdf:\n",
        "\n",
        "    # Iterate over each cluster\n",
        "    for cluster in clusters:\n",
        "\n",
        "        # Start a new figure for this cluster\n",
        "        plt.figure(figsize=(7, 7))\n",
        "\n",
        "        # Extract unique IDs for the current cluster from exemplar_info\n",
        "        cluster_unique_ids = exemplar_info[exemplar_info['Cluster_UMAP'] == cluster]['Unique_ID'].tolist()\n",
        "\n",
        "        # For each unique ID in the cluster, plot the track\n",
        "        for unique_id in cluster_unique_ids:\n",
        "\n",
        "            # Filter dataframe based on the unique ID\n",
        "            unique_df = merged_spots_df[merged_spots_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
        "\n",
        "            # Color code tracks based on 'Condition' using seaborn's color palette\n",
        "            color = sns.color_palette('husl', n_colors=merged_spots_df['Condition'].nunique())[merged_spots_df['Condition'].unique().tolist().index(unique_df['Condition'].iloc[0])]\n",
        "\n",
        "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2, color=color, label=unique_df['Condition'].iloc[0])\n",
        "\n",
        "            # Set title for the subplot\n",
        "            plt.title(f'Coordinates for Cluster {cluster}')\n",
        "\n",
        "            # Limit the plot dimensions based on your data's extent\n",
        "            plt.xlim(min_x - 1, max_x + 1)\n",
        "            plt.ylim(min_y - 1, max_y + 1)\n",
        "\n",
        "            # Add legend to differentiate tracks based on condition\n",
        "            plt.legend(loc='best')\n",
        "\n",
        "            plt.xlabel('POSITION_X')\n",
        "            plt.ylabel('POSITION_Y')\n",
        "\n",
        "        # Save the figure in the PDF\n",
        "        pdf.savefig()\n",
        "\n",
        "# Adjust layout to avoid overlap\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_r0pSYIhCGA"
      },
      "source": [
        "## **2.10. Find the exemplar on your raw images**\n",
        "--------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAUUjA_xhCGA"
      },
      "source": [
        "<font size = 4>This Python script serves as a user-friendly tool for visualizing exemplar tracks within your microscopy video.\n",
        "\n",
        "<font size = 4>To utilize it effectively, **users should provide the path to the directory containing the raw stacks of their data**.\n",
        "<font size = 4>It's essential to ensure that these stack files have the same name as the\n",
        "*  Use .tif or .tiff files only\n",
        "*  It's essential to ensure that these tif files have the same name as the corresponding CSV file used in the analysis.\n",
        "*   The Tif files can be in the same folder as your csv file\n",
        "\n",
        "<font size = 4>Additionally, users are required to **specify the pixel calibration** value to accurately scale the visualization.\n",
        "*   Use the same calibration as the one used before\n",
        "\n",
        "\n",
        "<font size = 4>With these inputs, the script automates the retrieval of matching TIFF files, adjusts for pixel calibration, and overlays vital information on video frames. Users can interactively select a specific cluster and initiate the visualization process with a single click."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UZE5V1-PhCGA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from tifffile import imread\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, interactive, widgets, Button, Output\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# @title #Find your examplar tracks\n",
        "\n",
        "Use_test_dataset = False\n",
        "\n",
        "#@markdown ##Provide the path to the folder containing your .tif files\n",
        "\n",
        "Video_path = ''  # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "#@markdown ###Provide the pixel calibration\n",
        "\n",
        "Pixel_calibration = None # @param {type: \"number\"}\n",
        "\n",
        "\n",
        "# Function to visualize a track for a cluster\n",
        "def visualize_track_for_cluster(cluster_number):\n",
        "    # Filter merged_tracks_df for exemplars and the selected cluster\n",
        "    exemplar_tracks = merged_tracks_df[(merged_tracks_df['Cluster_UMAP'] == cluster_number) & (merged_tracks_df['Exemplar'] == 1)]\n",
        "\n",
        "    if exemplar_tracks.empty:\n",
        "        display_error_message(\"No exemplar found for this cluster.\")\n",
        "        return\n",
        "\n",
        "    for idx, track in exemplar_tracks.iterrows():\n",
        "        # Get the filename\n",
        "        filename = track['File_name']\n",
        "\n",
        "        # Find the corresponding tiff file\n",
        "        full_path = find_matching_tiff_file(Video_path, filename)\n",
        "\n",
        "        if not full_path:\n",
        "            display_error_message(f\"No matching .tif or .tiff file found for filename: {filename}\")\n",
        "            continue\n",
        "\n",
        "        # Load the movie\n",
        "        movie = imread(full_path)\n",
        "\n",
        "        if len(movie.shape) != 3:\n",
        "            display_error_message(f\"Warning: The loaded movie from file '{filename}' is not 2D over time.\")\n",
        "            continue\n",
        "\n",
        "        # Fetch the track coordinates from merged_spots_df and adjust for calibration\n",
        "        track_id = track['Unique_ID']\n",
        "        track_coordinates = merged_spots_df[merged_spots_df['Unique_ID'] == track_id][['POSITION_T', 'POSITION_X', 'POSITION_Y']].copy()\n",
        "        track_coordinates['POSITION_X'] = track_coordinates['POSITION_X'] / Pixel_calibration\n",
        "        track_coordinates['POSITION_Y'] = track_coordinates['POSITION_Y'] / Pixel_calibration\n",
        "\n",
        "        # Define a function to update the display based on the frame slider\n",
        "        def update_display(frame_number):\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            frame_with_square = movie[frame_number, :, :].copy()\n",
        "            coords_for_frame = track_coordinates[track_coordinates['POSITION_T'] == frame_number]\n",
        "            if not coords_for_frame.empty:\n",
        "                x, y = int(coords_for_frame['POSITION_X'].values[0]), int(coords_for_frame['POSITION_Y'].values[0])\n",
        "                frame_with_square = overlay_square_on_frame(frame_with_square, x, y)\n",
        "            plt.imshow(frame_with_square, cmap='gray')\n",
        "            plt.title(f\"Frame {frame_number} for Exemplar in Cluster {cluster_number} from file {filename}\")\n",
        "            plt.show()\n",
        "\n",
        "        # Create a slider for frame navigation\n",
        "        frame_slider = widgets.IntSlider(min=0, max=len(movie) - 1, description='Frame')\n",
        "\n",
        "        # Display the visualization with interactive for more reactive updates\n",
        "        w = interactive(update_display, frame_number=frame_slider)\n",
        "        display(w)  # This line explicitly displays the widget\n",
        "        break  # Only display for the first matching exemplar for the sake of demonstration\n",
        "\n",
        "\n",
        "# Error message widget\n",
        "error_output = Output()\n",
        "\n",
        "if Use_test_dataset:\n",
        "  Video_path = '/content/Tracks/Tracks'\n",
        "\n",
        "# Dropdown widget for cluster selection\n",
        "clusters = merged_tracks_df['Cluster_UMAP'].unique()\n",
        "cluster_dropdown = widgets.Dropdown(\n",
        "    options=clusters,\n",
        "    description='Select Cluster:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Button to trigger visualization\n",
        "plot_button = Button(description=\"Plot\")\n",
        "\n",
        "# Function to handle button click\n",
        "def on_plot_button_click(b):\n",
        "    cluster_number = cluster_dropdown.value\n",
        "        # Clear the previous output\n",
        "    clear_output()\n",
        "\n",
        "    display(cluster_dropdown)\n",
        "    display(plot_button)\n",
        "    display(error_output)\n",
        "    visualize_track_for_cluster(cluster_number)\n",
        "\n",
        "# Bind the function to the button click event\n",
        "plot_button.on_click(on_plot_button_click)\n",
        "\n",
        "# Display the widgets\n",
        "display(cluster_dropdown)\n",
        "display(plot_button)\n",
        "display(error_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6zGL9f-tKA2"
      },
      "source": [
        "## **2.11. Export movies of the exemplar tracks**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_cIPhYbGkCLM"
      },
      "outputs": [],
      "source": [
        "# @title ##Export movies with the examplar tracks labelled\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tifffile import imwrite\n",
        "from tqdm.notebook import tqdm\n",
        "import imageio\n",
        "\n",
        "# Create a directory to store the exported videos\n",
        "video_export_folder = Results_Folder + \"/Umap/Examplar/Exported_Videos\"\n",
        "if not os.path.exists(video_export_folder):\n",
        "    os.makedirs(video_export_folder)\n",
        "\n",
        "# Iterate over all exemplar tracks\n",
        "# Iterate over all exemplar tracks\n",
        "for idx, track in tqdm(merged_tracks_df[merged_tracks_df['Exemplar'] == 1].iterrows(), total=merged_tracks_df[merged_tracks_df['Exemplar'] == 1].shape[0]):\n",
        "    # Get the filename and cluster number\n",
        "    filename = track['File_name']\n",
        "    cluster_num = track['Cluster_UMAP']\n",
        "\n",
        "    # Find the corresponding tiff file\n",
        "    full_path = find_matching_tiff_file(Video_path, filename)\n",
        "\n",
        "    if not full_path:\n",
        "        print(f\"No matching .tif or .tiff file found for filename: {filename}\")\n",
        "        continue\n",
        "\n",
        "    # Load the movie\n",
        "    movie = imread(full_path)\n",
        "\n",
        "    # Check dimensions to ensure 2D video\n",
        "    if len(movie.shape) != 3:\n",
        "        print(f\"Skipping {filename} as it is not a 2D video.\")\n",
        "        continue\n",
        "\n",
        "    # Fetch the track coordinates from merged_spots_df\n",
        "    track_id = track['Unique_ID']\n",
        "    track_coordinates = merged_spots_df[merged_spots_df['Unique_ID'] == track_id][['POSITION_T', 'POSITION_X', 'POSITION_Y']]\n",
        "\n",
        "    # Overlay the track on the video using the overlay_square_on_frame function\n",
        "    for _, coord in track_coordinates.iterrows():\n",
        "        frame_number = int(coord['POSITION_T'])\n",
        "        x = int(coord['POSITION_X'] / Pixel_calibration)\n",
        "        y = int(coord['POSITION_Y'] / Pixel_calibration)\n",
        "        movie[frame_number] = overlay_square_on_frame(movie[frame_number], x, y)\n",
        "\n",
        "    # Incorporate the cluster number in the output filenames\n",
        "    output_video_path_tiff = os.path.join(video_export_folder, f\"{filename}_Cluster_{cluster_num}_with_track.tiff\")\n",
        "    output_video_path_mp4 = os.path.join(video_export_folder, f\"{filename}_Cluster_{cluster_num}_with_track.mp4\")\n",
        "\n",
        "    # Save the video with overlaid track as TIFF\n",
        "    imwrite(output_video_path_tiff, movie)\n",
        "\n",
        "    # Normalize and convert the movie to uint8\n",
        "    movie_uint8 = percentile_normalize_and_convert_uint8(movie)\n",
        "\n",
        "    # Convert and save as MP4\n",
        "    imageio.mimwrite(output_video_path_mp4, movie_uint8, fps=10)\n",
        "\n",
        "print(\"Video export completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4MI-XFWhCGB"
      },
      "source": [
        "--------\n",
        "# **Part 3. Explore your high-dimensional data using t-SNE and HDBSCAN**\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QLokDRXhCGB"
      },
      "source": [
        "## **3.1. Choose the track metrics to use for clustering**\n",
        "--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VLE_paYHss1M"
      },
      "outputs": [],
      "source": [
        "# @title ##Choose the track metrics to use\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Check and create \"pdf\" folder\n",
        "if not os.path.exists(f\"{Results_Folder}/Tsne\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Tsne\")\n",
        "\n",
        "excluded_columns = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n",
        "\n",
        "# Columns you want to always include\n",
        "columns_to_include = ['File_name', 'Repeat', 'Condition', 'Unique_ID']\n",
        "\n",
        "selected_df = pd.DataFrame()\n",
        "nan_columns = pd.DataFrame()\n",
        "# Extract the columns you always want to include and ensure they exist in the original dataframe\n",
        "saved_columns = {col: merged_tracks_df[col].copy() for col in columns_to_include if col in merged_tracks_df}\n",
        "\n",
        "# Filter out non-numeric columns\n",
        "numeric_df = merged_tracks_df.select_dtypes(include=['float64', 'int64'])  # Selecting only numeric columns\n",
        "\n",
        "column_names = [col for col in numeric_df.columns if col not in excluded_columns]\n",
        "\n",
        "# Text area for user to paste the list of metrics\n",
        "text_area = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Copy and paste your list of metrics here, separated by commas.',\n",
        "    description='Metrics:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "\n",
        "# Function to parse the text area content into a list\n",
        "def parse_text_area(text):\n",
        "    return [item.strip() for item in text.split(',') if item.strip() in column_names]\n",
        "\n",
        "# Create a checkbox for each column\n",
        "def create_checkboxes(parsed_metrics):\n",
        "    return [widgets.Checkbox(value=(col in parsed_metrics or not parsed_metrics), description=col, indent=False) for col in column_names]\n",
        "\n",
        "checkboxes = create_checkboxes(column_names)  # Initialize with all metrics\n",
        "\n",
        "# Grid for displaying checkboxes\n",
        "grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n",
        "\n",
        "# Create a button to trigger the selection\n",
        "button = widgets.Button(description=\"Select the track parameters\",\n",
        "                        layout=widgets.Layout(width='400px'),\n",
        "                        button_style='info')\n",
        "\n",
        "\n",
        "def on_button_click(b):\n",
        "    global selected_df\n",
        "    global nan_columns\n",
        "    parsed_metrics = parse_text_area(text_area.value)\n",
        "    selected_columns = [box.description for box in checkboxes if box.value]\n",
        "\n",
        "    # Extract the selected columns from the DataFrame\n",
        "    selected_df = numeric_df[selected_columns].copy()\n",
        "\n",
        "        # Prepare the parameters dictionary\n",
        "    Tsne_params = {\n",
        "        'Selected Columns': ', '.join(selected_columns)\n",
        "    }\n",
        "\n",
        "    # Save the parameters\n",
        "    params_file_path = os.path.join(Results_Folder, \"Tsne/analysis_parameters.csv\")\n",
        "    save_parameters(Tsne_params, params_file_path, 'Tsne')\n",
        "\n",
        "    # Add back the always-included columns to selected_df\n",
        "    for col, data in saved_columns.items():\n",
        "        selected_df.loc[:, col] = data\n",
        "\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n",
        "    if nan_columns:\n",
        "        for col in nan_columns:\n",
        "            selected_df = selected_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "# Set the button click event handler\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "# Function to update checkboxes based on text area input\n",
        "def update_checkboxes(b):\n",
        "    parsed_metrics = parse_text_area(text_area.value)\n",
        "    global checkboxes\n",
        "    checkboxes = create_checkboxes(parsed_metrics)\n",
        "    grid.children = checkboxes\n",
        "\n",
        "# Update checkboxes when text area content changes\n",
        "text_area.observe(update_checkboxes, names='value')\n",
        "\n",
        "# Display the text area, grid of checkboxes, and the button\n",
        "display(text_area, grid, button)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRATZZE1hCGC"
      },
      "source": [
        "## **3.2. t-SNE**\n",
        "--------\n",
        "\n",
        "The code snippet provided performs **t-Distributed Stochastic Neighbor Embedding (t-SNE)**, a powerful technique for dimensionality reduction, particularly suited for the visualization of high-dimensional datasets. The process is applied to the merged tracks dataframe, focusing on its numeric columns, with the goal of visualizing the data in a lower-dimensional space.\n",
        "\n",
        "**Key Parameters of t-SNE:**\n",
        "\n",
        "- **Perplexity (`perplexity`):**\n",
        "  - This parameter is a measure of the effective number of local neighbors each point has.\n",
        "  - Perplexity influences the t-SNE algorithm's ability to capture local versus global aspects of the data.\n",
        "  - Typical values for perplexity range between 5 and 50, with the choice depending on dataset size and density.\n",
        "\n",
        "- **Learning Rate (`learning_rate`):**\n",
        "  - This parameter controls the step size in the optimization process.\n",
        "  - A suitable learning rate helps t-SNE to converge to a meaningful low-dimensional representation.\n",
        "  - Values too high might cause the algorithm to converge to a suboptimal solution, while too low values can slow down the convergence.\n",
        "\n",
        "- **Number of Iterations (`n_iter`):**\n",
        "  - This parameter defines the number of optimization iterations t-SNE will run.\n",
        "  - A higher number of iterations allows the algorithm more time to find a stable configuration.\n",
        "  - Generally, a value of 1000 iterations is sufficient for most datasets.\n",
        "\n",
        "- **Number of Dimensions (`n_dimension`):**\n",
        "  - The target dimensionality for the lower-dimensional space.\n",
        "  - For visualization purposes, this is commonly set to 2, allowing the data to be plotted in a 2D scatter plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wFaskkOthCGC"
      },
      "outputs": [],
      "source": [
        "# @title ##Perform t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "# Check and create necessary directories\n",
        "tsne_folder_path = f\"{Results_Folder}/Tsne/\"\n",
        "if not os.path.exists(tsne_folder_path):\n",
        "    os.makedirs(tsne_folder_path)\n",
        "\n",
        "#@markdown ###t-SNE parameters:\n",
        "\n",
        "perplexity = 20  # @param {type: \"number\"}\n",
        "learning_rate = 100  # @param {type: \"number\"}\n",
        "n_iter = 1000  # @param {type: \"number\"}\n",
        "n_dimension = 2  # The number of dimensions is set to 2 for t-SNE as standard practice\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 15  # @param {type: \"number\"}\n",
        "\n",
        "# Initialize t-SNE object with the specified settings\n",
        "tsne = TSNE(n_components=n_dimension, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter, random_state=42)\n",
        "\n",
        "# Exclude non-numeric columns when fitting t-SNE\n",
        "numeric_columns = selected_df._get_numeric_data()\n",
        "embedding = tsne.fit_transform(numeric_columns)\n",
        "\n",
        "  # Prepare the parameters dictionary\n",
        "tsne_params = {\n",
        "        'perplexity': perplexity,\n",
        "        'learning_rate': learning_rate,\n",
        "        'n_iter': n_iter,\n",
        "        'n_dimension': n_dimension,\n",
        "        'spot_size': spot_size\n",
        "    }\n",
        "\n",
        "    # Save the parameters\n",
        "params_file_path = os.path.join(Results_Folder, \"Tsne/analysis_parameters.csv\")\n",
        "save_parameters(tsne_params, params_file_path, 'tsne')\n",
        "\n",
        "# Create dynamic column names based on n_components\n",
        "column_names = [f't-SNE dimension {i+1}' for i in range(n_dimension)]\n",
        "\n",
        "# Extract the columns_to_include from selected_df\n",
        "included_data = selected_df[columns_to_include].reset_index(drop=True)\n",
        "\n",
        "# Concatenate the t-SNE embedding with the included columns\n",
        "tsne_df = pd.concat([pd.DataFrame(embedding, columns=column_names), included_data], axis=1)\n",
        "\n",
        "# Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "nan_columns = tsne_df.columns[tsne_df.isna().any()].tolist()\n",
        "if nan_columns:\n",
        "  warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n",
        "  tsne_df.dropna(subset=nan_columns, inplace=True)  # Drop NaN values only from columns containing them\n",
        "\n",
        "# Visualize the t-SNE projection\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=tsne_df, palette='Set2', s=spot_size)\n",
        "plt.title('t-SNE Projection of the Dataset')\n",
        "tsne_output_path = os.path.join(tsne_folder_path, 'tsne_projection_2D.pdf')\n",
        "plt.savefig(tsne_output_path)  # Save 2D plot as PDF\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yve0T2t0hCGC"
      },
      "source": [
        "## **3.3. HDBSCAN**\n",
        "---\n",
        "\n",
        "<font size = 4> The provided code employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify clusters within a dataset that has already undergone UMAP dimensionality reduction. HDBSCAN is utilized for its proficiency in determining the optimal number of clusters while managing varied densities within the data.\n",
        "\n",
        "<font size = 4>In the provided HDBSCAN code, the parameters `min_samples`, `min_cluster_size`, and `metric` are crucial for determining the structure and appearance of the resulting clusters in the data.\n",
        "\n",
        "<font size = 4>`min_samples`: This parameter primarily controls the degree to which the algorithm is willing to declare noise. It's the number of samples in a neighborhood for a point to be considered as a core point.\n",
        "- A smaller value of `min_samples` makes the algorithm more prone to declaring points as part of a cluster, potentially leading to larger clusters and fewer noise points.\n",
        "- A larger value makes the algorithm more conservative, resulting in more points declared as noise and smaller, more defined clusters.\n",
        "- The choice of `min_samples` typically depends on the density of the data; denser datasets may require a larger value.\n",
        "\n",
        "<font size = 4>`min_cluster_size`: This parameter determines the smallest size grouping that you wish to consider a cluster.\n",
        "- A smaller value will allow the formation of smaller clusters, whereas a larger value will prevent small isolated groups of points from being declared as clusters.\n",
        "- The choice of `min_cluster_size` depends on the scale of the data and the desired level of granularity in the clustering.\n",
        "\n",
        "<font size = 4>`metric`: This parameter is the metric used for distance computation between data points, and it affects the shape of the clusters.\n",
        "- The `euclidean` metric is a good starting point, and depending on the clustering results and the data type, it might be beneficial to experiment with different metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ULIQ1acChCGC"
      },
      "outputs": [],
      "source": [
        "# @title ##Identify clusters using HDBSCAN\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "#@markdown ###HDBSCAN parameters:\n",
        "clustering_data_source = 'tsne'  # @param ['tsne', 'raw']\n",
        "min_samples = 20  # @param {type: \"number\"}\n",
        "min_cluster_size = 200  # @param {type: \"number\"}\n",
        "metric = \"canberra\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'braycurtis', 'canberra']\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 15 # @param {type: \"number\"}\n",
        "\n",
        "# Apply HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)\n",
        "\n",
        "\n",
        "  # Prepare the parameters dictionary\n",
        "tsne_params = {\n",
        "        'clustering_data_source': clustering_data_source,\n",
        "        'min_samples': min_samples,\n",
        "        'min_cluster_size': min_cluster_size,\n",
        "        'metric': metric\n",
        "    }\n",
        "\n",
        "    # Save the parameters\n",
        "params_file_path = os.path.join(Results_Folder, \"Tsne/analysis_parameters.csv\")\n",
        "save_parameters(tsne_params, params_file_path, 'tsne')\n",
        "\n",
        "# Depending on the data source, we fit HDBSCAN to the t-SNE dimensions or the raw data\n",
        "if clustering_data_source == 'tsne':\n",
        "    # We only have two t-SNE dimensions based on the previous t-SNE code provided\n",
        "    clusterer.fit(tsne_df[['t-SNE dimension 1', 't-SNE dimension 2']])\n",
        "else:\n",
        "    # If raw data is selected, we use all the numerical columns for clustering\n",
        "    clusterer.fit(selected_df.select_dtypes(include=['number']))\n",
        "\n",
        "# Add the cluster labels to your t-SNE DataFrame\n",
        "tsne_df['Cluster_tsne'] = clusterer.labels_\n",
        "\n",
        "# If the Cluster column already exists in merged_tracks_df, drop it to avoid duplications\n",
        "if 'Cluster_tsne' in merged_tracks_df.columns:\n",
        "    merged_tracks_df.drop(columns='Cluster_tsne', inplace=True)\n",
        "\n",
        "# Merge the Cluster column from tsne_df to merged_tracks_df based on Unique_ID\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, tsne_df[['Unique_ID', 'Cluster_tsne']], on='Unique_ID', how='left')\n",
        "\n",
        "# Handle cases where some rows in merged_tracks_df might not have a corresponding cluster label\n",
        "merged_tracks_df['Cluster_tsne'].fillna(-1, inplace=True)  # Assigning -1 to cells that were not assigned to any cluster\n",
        "\n",
        "# Save the DataFrame with the identified clusters\n",
        "save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.scatterplot(x='t-SNE dimension 1', y='t-SNE dimension 2', hue='Cluster_tsne', palette='viridis', data=tsne_df, s=spot_size)\n",
        "plt.title('Clusters Identified by HDBSCAN')\n",
        "plt.savefig(os.path.join(Results_Folder, 'Tsne', 'HDBSCAN_clusters_2D.pdf'))  # Save 2D plot as PDF\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJKP5VQ0hCGC"
      },
      "source": [
        "## **3.4. Fingerprint**\n",
        "---\n",
        "\n",
        "<font size = 4>This section is designed to visualize the distribution of different clusters within each condition in a dataset, showing the 'fingerprint' of each cluster per condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YBzEJJVux539"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot the 'fingerprint' of each cluster per condition\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# Group by 'Condition' and 'Cluster' and calculate the size of each group\n",
        "cluster_counts = tsne_df.groupby(['Condition', 'Cluster_tsne']).size().reset_index(name='counts')\n",
        "\n",
        "# Calculate the total number of points per condition\n",
        "total_counts = tsne_df.groupby('Condition').size().reset_index(name='total_counts')\n",
        "\n",
        "# Merge the DataFrames on 'Condition' to calculate percentages\n",
        "percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n",
        "percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n",
        "\n",
        "# Save the percentage_df DataFrame as a CSV file\n",
        "percentage_df.to_csv(os.path.join(Results_Folder, 'Tsne', 'TSNE_percentage_results.csv'), index=False)\n",
        "\n",
        "# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n",
        "pivot_df = percentage_df.pivot(index='Condition', columns='Cluster_tsne', values='percentage')\n",
        "\n",
        "# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n",
        "pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_path = os.path.join(Results_Folder, 'Tsne', 'TSNE_Cluster_Fingerprint_Plot.pdf')\n",
        "pdf_pages = PdfPages(pdf_path)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n",
        "plt.title('Percentage in each cluster per Condition')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Condition')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96v0-7eIhCGD"
      },
      "source": [
        "## **3.5. Understand your clusters using heatmaps**\n",
        "--------\n",
        "<font size = 4>This section help visualize how different track parameters vary across the identified clusters. The approach is to display these variations using a heatmap, which offers a color-coded representation of the median values of each parameter for each cluster. This visualization technique can make it easier to spot differences or patterns among the clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q2EOQOE4ngH9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# @title ##Plot track normalized track parameters based on clusters as an heatmap\n",
        "\n",
        "# Parameters to adapt in function of the notebook section\n",
        "base_folder = f\"{Results_Folder}/Tsne/Track_parameters\"\n",
        "Conditions = 'Cluster_tsne'\n",
        "\n",
        "# Check and create necessary directories\n",
        "folders = [\"pdf\", \"csv\"]\n",
        "for folder in folders:\n",
        "    dir_path = os.path.join(base_folder, folder)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "# Example usage\n",
        "heatmap_comparison(merged_tracks_df, base_folder, Conditions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTx8eA5thCGD"
      },
      "source": [
        "## **3.6. Understand your clusters using box plots**\n",
        "--------\n",
        "<font size = 4>The provided code aims to visually represent the distribution of different track parameters across the identified clusters. Specifically, for each parameter selected, a boxplot is generated to showcase the spread of its values across different clusters. This approach provides a comprehensive view of how each track parameter varies within and across the clusters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j4Fts7wrhCGD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# @title ##Plot track parameters based on clusters\n",
        "\n",
        "# Check and create \"pdf\" folder\n",
        "if not os.path.exists(os.path.join(Results_Folder, \"Tsne\", \"Track_parameters\")):\n",
        "    os.makedirs(os.path.join(Results_Folder, \"Tsne\", \"Track_parameters\"), exist_ok=True)\n",
        "\n",
        "Cluster = \"Cluster_tsne\"\n",
        "base_folder = f\"{Results_Folder}/Tsne/Track_parameters/\"\n",
        "\n",
        "if not os.path.exists(base_folder):\n",
        "  os.makedirs(base_folder)\n",
        "\n",
        "checkboxes_dict, checkboxes_accordion = display_variable_checkboxes(categorize_columns(merged_tracks_df))\n",
        "variable_checkboxes, checkboxes_widget = display_variable_checkboxes(get_selectable_columns_plots(merged_tracks_df))\n",
        "\n",
        "# Create and display the plot button\n",
        "button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n",
        "button.on_click(lambda b: plot_selected_vars_per_cluster(b, Cluster, checkboxes_dict, merged_tracks_df, base_folder));\n",
        "\n",
        "# Display the UI components\n",
        "display(VBox([checkboxes_accordion, button]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IXI1As7hCGD"
      },
      "source": [
        "## **3.7. Plot track parameters for a selected cluster**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-birx44tJCmv"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot track parameters for a selected cluster\n",
        "\n",
        "base_folder = f\"{Results_Folder}/Tsne/Track_parameters/\"\n",
        "Conditions = 'Condition'\n",
        "df_to_plot = merged_tracks_df\n",
        "Cluster = \"Cluster_tsne\"\n",
        "base_folder = f\"{Results_Folder}/Tsne/Track_parameters/\"\n",
        "\n",
        "# Check and create necessary directories\n",
        "folders = [\"pdf\", \"csv\"]\n",
        "for folder in folders:\n",
        "    dir_path = os.path.join(base_folder, folder)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "condition_selector, condition_accordion = display_condition_selection(df_to_plot, Conditions)\n",
        "checkboxes_dict, checkboxes_accordion = display_variable_checkboxes(categorize_columns(df_to_plot))\n",
        "variable_checkboxes, checkboxes_widget = display_variable_checkboxes(get_selectable_columns_plots(df_to_plot))\n",
        "stat_method_selector = widgets.Dropdown(\n",
        "    options=['randomization test', 't-test'],\n",
        "    value='randomization test',\n",
        "    description='Stat Method:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "cluster_dropdown = display_cluster_dropdown(merged_tracks_df, Cluster)\n",
        "\n",
        "button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n",
        "button.on_click(lambda b: plot_selected_vars_cluster(b, checkboxes_dict, df_to_plot, Conditions, Cluster, cluster_dropdown, base_folder, condition_selector, stat_method_selector));\n",
        "\n",
        "display(VBox([condition_accordion, checkboxes_accordion, stat_method_selector, cluster_dropdown, button]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd817UHYhCGD"
      },
      "source": [
        "# **Part 4. Version log**\n",
        "---\n",
        "<font size = 4>While I strive to provide accurate and helpful information, please be aware that:\n",
        "  - This notebook may contain bugs.\n",
        "  - Features are currently limited and will be expanded in future releases.\n",
        "\n",
        "<font size = 4>We encourage users to report any issues or suggestions for improvement. Please check the [repository](https://github.com/guijacquemet/CellTracksColab) regularly for updates and the latest version of this notebook.\n",
        "\n",
        "#### **Known Issues**:\n",
        "- Tracks are displayed in 2D in section 1.4\n",
        "\n",
        "<font size = 4>**Version 1.0.1**\n",
        "  - Includes a general data reader\n",
        "  - Plotting functions are imported from the main code\n",
        "    \n",
        "<font size = 4>**Version 0.9.2**\n",
        "  - Added the Origin normalized plots\n",
        "\n",
        "<font size = 4>**Version 0.9.1**\n",
        "  - Added the PIP freeze option to save a requirement text\n",
        "  - Added the heatmap visualisation of track parameters\n",
        "  - Heatmaps can now be displayed on multiple pages\n",
        "  - Fix userwarning message during plotting (all box plots)\n",
        "  - Added the possibility to copy and paste an existing list of selected metric for clustering analyses\n",
        "\n",
        "<font size = 4>**Version 0.9**\n",
        "  - Improved plotting strategy. Specific conditions can be chosen\n",
        "  - absolute cohen d values are now shown\n",
        "  - In the QC the heatmap is automatically divided in subplot when too many columns are in the df\n",
        "\n",
        "<font size = 4>**Version 0.8**\n",
        "  - Settings are now saved\n",
        "  - Order of the section has been modified to help streamline biological discoveries\n",
        "  - New section added to quality Control to check if the dataset is balanced\n",
        "  - New section added to the UMAP and tsne section to plot track parameters for selected clusters\n",
        "  - clusters for UMAP and t-sne are now saved in the dataframe separetly\n",
        "\n",
        "<font size = 4>**Version 0.7**\n",
        "  - check_for_nans function added\n",
        "  - Clustering using t-SNE added\n",
        "\n",
        "<font size = 4>**Version 0.6**\n",
        "  - Improved organisation of the results\n",
        "  - Tracks visualisation are now saved\n",
        "\n",
        "<font size = 4>**Version 0.5**\n",
        "  - Improved part 5\n",
        "  - Added the possibility to find examplar on the raw movies when available\n",
        "  - Added the possibility to export video with the examplar labeled\n",
        "  - Code improved to deal with larger dataset (tested with over 50k tracks)\n",
        "  - test dataset now contains raw video and is hosted on Zenodo\n",
        "  - Results are now organised in folders\n",
        "  - Added progress bars\n",
        "  - Minor code fixes\n",
        "\n",
        "<font size = 4>**Version 0.4**\n",
        "\n",
        "  - Added the possibility to filter and smooth tracks\n",
        "  - Added spatial and temporal calibration\n",
        "  - Notebook is streamlined\n",
        "  - multiple bug fix\n",
        "  - Remove the t-sne\n",
        "  - Improved documentation\n",
        "\n",
        "<font size = 4>**Version 0.3**\n",
        "  - Fix a nasty bug in the import functions\n",
        "  - Add basic examplar for UMAP\n",
        "  - Added the statistical analyses and their explanations.\n",
        "  - Added a new quality control part that helps assessing the similarity of results between FOV, conditions and repeats\n",
        "  - Improved part 5 (previously part 4).\n",
        "\n",
        "<font size = 4>**Version 0.2**\n",
        "  - Added support for 3D tracks\n",
        "  - New documentation and metrics added.\n",
        "\n",
        "<font size = 4>**Version 0.1**\n",
        "This is the first release of this notebook.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y4-Ft-yNRVCc",
        "etkbaQ9fm5u2",
        "mpyycyZShCF5",
        "H4MI-XFWhCGB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}