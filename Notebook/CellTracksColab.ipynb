{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CellTracksColab**\n",
        "---\n",
        "\n",
        "Colab Notebook for Analyzing Migration Tracks generated by [TrackMate](https://imagej.net/plugins/trackmate/)\n",
        "This Colab notebook is designed to analyze migration tracks, placing emphasis on those generated by using TrackMate, renowned for its proficiency in detailing single-particle tracking data.\n",
        "\n",
        "Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n"
      ],
      "metadata": {
        "id": "xF4zYMmXULP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Before getting started**\n",
        "---\n",
        "\n",
        "---\n",
        "<font size = 4>**Important note**\n",
        "\n",
        "To load your TrackMate outputs, your dataset should be meticulously organized into a two-tiered folder hierarchy as depicted below.\n",
        "\n",
        "<font size = 4>Here's a common data structure that can work:\n",
        "\n",
        "## Folder Hierarchy\n",
        "\n",
        "- üìÅ **Experiments** `[Folder_path]`\n",
        "  - üåø **Condition_1** `[‚Äòcondition‚Äô is derived from this folder name]`\n",
        "    - üîÑ **R1** `[‚Äòrepeat‚Äô is derived from this folder name]`\n",
        "      - üìÑ `FOV_spots_1.csv`\n",
        "      - üìÑ `FOV_tracks_1.csv`\n",
        "      - üìÑ `FOV_spots_2.csv`\n",
        "      - üìÑ `FOV_tracks_2.csv`\n",
        "    - üîÑ **R2**\n",
        "      - üìÑ `FOV_spots_1.csv`\n",
        "      - üìÑ `FOV_tracks_1.csv`\n",
        "      - üìÑ `FOV_spots_2.csv`\n",
        "      - üìÑ `FOV_tracks_2.csv`\n",
        "  - üåø **Condition_2**\n",
        "    - üîÑ **R1**\n",
        "    - üîÑ **R2**\n",
        "\n",
        "<font size = 4>In this representation, different symbols are used to represent folders and files clearly:\n",
        "\n",
        "üìÅ represents the main folder or directory.\n",
        "üåø represents the condition folders.\n",
        "üîÑ represents the repeat folders.\n",
        "üìÑ represents the individual CSV files.\n",
        "\n",
        "---\n",
        "<font size = 4>**Important note 2**\n",
        "\n",
        "Be advised of one significant limitation inherent to this notebook.\n",
        "\n",
        "<font size = 4 color=\"red\">**Part2 does not support Track splitting**</font>. For users aiming to compute additional track metrics within this environment, it is crucial to disable track splitting in TrackMate.\n",
        "\n",
        "It‚Äôs important to clarify that the absence of track splitting support does not hinder the notebook's ability to compile and display results in part 3 of the analysis process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aR2U8v9YoJcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title #MIT License\n",
        "\n",
        "print(\"\"\"\n",
        "**MIT License**\n",
        "\n",
        "Copyright (c) 2023 Guillaume Jacquemet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JrkfFr7mgZmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Version log\n",
        "---\n",
        "While I strive to provide accurate and helpful information, please be aware that:\n",
        "  - This notebook may contain bugs.\n",
        "  - Features are currently limited and will be expanded in future releases.\n",
        "\n",
        "We encourage users to report any issues or suggestions for improvement. Please check the [repository](https://github.com/guijacquemet/CellTracksColab) regularly for updates and the latest version of this notebook.\n",
        "\n",
        "#### Known Issues:\n",
        "- Part 4 is limited and unstable.\n",
        "\n",
        "\n",
        "<font size = 4>**Version 0.2**\n",
        "  - Added support for 3D tracks\n",
        "  - New documentation and metrics added.\n",
        "\n",
        "<font size = 4>**Version 0.1**\n",
        "This is the first release of this notebook.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YheRveJdrFyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 1: Prepare the session and Load your data**\n",
        "--------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "Y4-Ft-yNRVCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ],
      "metadata": {
        "id": "9h0prdayn0qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Play to install\n",
        "!pip -q install pandas scikit-learn\n",
        "!pip -q install hdbscan\n",
        "!pip -q install umap-learn\n",
        "!pip -q install plotly\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "# URL to the raw content of the version file in the repository\n",
        "version_url = \"https://raw.githubusercontent.com/guijacquemet/CellTracksColab/main/Notebook/latest_version.txt\"\n",
        "\n",
        "# Current version of the notebook the user is running\n",
        "current_version = \"0.2\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(version_url)\n",
        "    response.raise_for_status()  # Check whether the request was successful\n",
        "    latest_version = response.text.strip()  # Get the latest version from the version file\n",
        "\n",
        "    if latest_version != current_version:\n",
        "        print(f\"A newer version of this notebook is available: {latest_version}. \"\n",
        "              f\"Please download the latest version from the repository.\")\n",
        "    else:\n",
        "        print(\"You are running the latest version of this notebook.\")\n",
        "except requests.RequestException as e:\n",
        "    print(\"Could not check for the latest version of the notebook.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rAP0ahCzn1V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ],
      "metadata": {
        "id": "3Kzd_8GUnpbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GA1wCrkoV4i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3. Load your (or test) data**\n",
        "\n",
        "<font size = 4> Please ensure that your data is properly organised (see above)\n"
      ],
      "metadata": {
        "id": "bsDAwkSOo1gV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A3D8odLQNx41"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Provide the path to your dataset\n",
        "\n",
        "Folder_path = ''  # @param {type: \"string\"}\n",
        "\n",
        "#@markdown ###Or use a test dataset\n",
        "Use_test_dataset = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ###Provide the path to your Result folder\n",
        "\n",
        "Results_Folder = \"/content\"  # @param {type: \"string\"}\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "def populate_columns(df, filepath):\n",
        "    # Extract the parts of the file path\n",
        "    path_parts = os.path.normpath(filepath).split(os.sep)\n",
        "\n",
        "    if len(path_parts) < 3:\n",
        "        # if there are not enough parts in the path to extract folder and parent folder\n",
        "        print(f\"Error: Cannot extract parent folder and folder from the filepath: {filepath}\")\n",
        "        return df\n",
        "\n",
        "    # Assuming that the file is located at least two levels deep in the directory structure\n",
        "    folder_name = path_parts[-2]  # The folder name is the second last part of the path\n",
        "    parent_folder_name = path_parts[-3]  # The parent folder name is the third last part of the path\n",
        "\n",
        "    df['File_name'] = os.path.splitext(os.path.basename(filepath))[0]\n",
        "    df['Condition'] = parent_folder_name  # Populate 'Condition' with the parent folder name\n",
        "    df['experiment_nb'] = folder_name  # Populate 'Repeat' with the folder name\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_and_populate(file_pattern, usecols=None):\n",
        "    df_list = []\n",
        "    pattern = re.compile(file_pattern)  # Compile the file pattern to a regex object\n",
        "\n",
        "    # Go through each root, dirs, files triplet returned by os.walk\n",
        "    for dirpath, dirnames, filenames in os.walk(Folder_path):\n",
        "        for filename in filenames:\n",
        "            if pattern.match(filename):  # Check if the filename matches the file pattern\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                df = pd.read_csv(filepath, skiprows=[1, 2, 3], usecols=usecols)\n",
        "                df_list.append(populate_columns(df, filepath))\n",
        "\n",
        "    if not df_list:  # if df_list is empty, return an empty DataFrame\n",
        "        print(f\"No files found with pattern: {file_pattern}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    merged_df = pd.concat(df_list, ignore_index=True)\n",
        "    return merged_df\n",
        "\n",
        "def sort_and_generate_repeat(merged_df):\n",
        "    merged_df.sort_values(['Condition', 'experiment_nb'], inplace=True)\n",
        "    merged_df = merged_df.groupby('Condition', group_keys=False).apply(generate_repeat)\n",
        "    return merged_df\n",
        "\n",
        "def generate_repeat(group):\n",
        "    unique_experiment_nbs = sorted(group['experiment_nb'].unique())\n",
        "    experiment_nb_to_repeat = {experiment_nb: i+1 for i, experiment_nb in enumerate(unique_experiment_nbs)}\n",
        "    group['Repeat'] = group['experiment_nb'].map(experiment_nb_to_repeat)\n",
        "    return group\n",
        "\n",
        "if (Use_test_dataset):\n",
        "  print(\"Downloading test dataset\")\n",
        "  !wget -nc -O /content/T_cell_dataset.zip https://github.com/guijacquemet/CellTracksColab/raw/main/Test_dataset/T_cell_dataset.zip && unzip -q /content/T_cell_dataset.zip -d /content\n",
        "  Folder_path = \"/content/Tracks\"\n",
        "\n",
        "print(\"Merging CSV files....\")\n",
        "\n",
        "merged_tracks_df = load_and_populate(r'.*tracks.*\\.csv')  # Use raw string to avoid escape character issues\n",
        "merged_tracks_df = sort_and_generate_repeat(merged_tracks_df)\n",
        "merged_tracks_df['Unique_ID'] = merged_tracks_df['Condition'] + \"_\" + merged_tracks_df['experiment_nb'] + \"_\" + merged_tracks_df['TRACK_ID'].astype(str)\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n",
        "\n",
        "merged_spots_df = load_and_populate(r'.*spots.*\\.csv')  # Use raw string to avoid escape character issues\n",
        "merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n",
        "merged_spots_df['Unique_ID'] = merged_spots_df['Condition'] + \"_\" + merged_spots_df['experiment_nb'] + \"_\" + merged_spots_df['TRACK_ID'].astype(str)\n",
        "merged_spots_df.to_csv(Results_Folder + '/' + 'merged_Spots.csv', index=False, compression='gzip')\n",
        "\n",
        "print(\"Done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.4. Visualise your tracks**"
      ],
      "metadata": {
        "id": "52STmnv43d45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Run the cell and choose the file you want to inspect\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract unique filenames from the dataframe\n",
        "filenames = merged_spots_df['File_name'].unique()\n",
        "\n",
        "# Create a Dropdown widget with the filenames\n",
        "filename_dropdown = widgets.Dropdown(\n",
        "    options=filenames,\n",
        "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
        "    description='File Name:',\n",
        ")\n",
        "\n",
        "def plot_coordinates(filename):\n",
        "    if filename:\n",
        "        # Filter the DataFrame based on the selected filename\n",
        "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for unique_id in filtered_df['Unique_ID'].unique():\n",
        "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
        "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n",
        "\n",
        "        plt.xlabel('POSITION_X')\n",
        "        plt.ylabel('POSITION_Y')\n",
        "        plt.title(f'Coordinates for {filename}')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid filename selected\")\n",
        "\n",
        "# Link the Dropdown widget to the plotting function\n",
        "interact(plot_coordinates, filename=filename_dropdown)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AE881uJW5ukQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 2: Compute additional metrics**\n",
        "--------------------------------------------------------\n",
        "<font size = 4 color=\"red\">Part2 does not support Track splitting</font>.\n",
        "<font size = 4> For users aiming to compute additional track metrics within this environment, it is crucial to disable track splitting in TrackMate."
      ],
      "metadata": {
        "id": "UlUCXg5QUC4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Directionality**\n",
        "To calculate the directionality of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The directionality, denoted as \\(D\\), is calculated using the formula:\n",
        "\n",
        "$$ D = \\frac{d_{\\text{euclidean}}}{d_{\\text{total path}}} $$\n",
        "\n",
        "where \\($d_{\\text{euclidean}}$\\) is the Euclidean distance between the first and the last points of the track, calculated as:\n",
        "\n",
        "$$ d_{\\text{euclidean}} = \\sqrt{(x_{\\text{end}} - x_{\\text{start}})^2 + (y_{\\text{end}} - y_{\\text{start}})^2 + (z_{\\text{end}} - z_{\\text{start}})^2} $$\n",
        "\n",
        "and \\($d_{\\text{total path}}$\\) is the sum of the Euclidean distances between all consecutive points in the track, representing the total path length traveled. If the total path length is zero, the directionality is defined to be zero. This measure provides insight into the straightness of the path taken, with a value of 1 indicating a straight path between the start and end points, and values approaching 0 indicating more circuitous paths.\n"
      ],
      "metadata": {
        "id": "8TqsbEAya-U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Calculate directionality\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate Directionality\n",
        "def calculate_directionality(group):\n",
        "    group = group.sort_values('POSITION_T')\n",
        "    start_point = group.iloc[0][['POSITION_X', 'POSITION_Y', 'POSITION_Z']]\n",
        "    end_point = group.iloc[-1][['POSITION_X', 'POSITION_Y', 'POSITION_Z']]\n",
        "\n",
        "    # Calculating Euclidean distance in 3D between start and end points\n",
        "    euclidean_distance = np.sqrt((end_point - start_point).pow(2).sum())\n",
        "\n",
        "    # Calculating the total path length in 3D\n",
        "    deltas = np.sqrt(group['POSITION_X'].diff().fillna(0)**2 +\n",
        "                     group['POSITION_Y'].diff().fillna(0)**2 +\n",
        "                     group['POSITION_Z'].diff().fillna(0)**2)\n",
        "    total_path_length = deltas.sum()\n",
        "\n",
        "    # Calculating Directionality\n",
        "    D = euclidean_distance / total_path_length if total_path_length != 0 else 0\n",
        "\n",
        "    return pd.Series({'Directionality': D})\n",
        "\n",
        "# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n",
        "merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "\n",
        "# Calculate directionality for each track\n",
        "df_directionality = merged_spots_df.groupby('Unique_ID').apply(calculate_directionality).reset_index()\n",
        "\n",
        "# Find the overlapping columns between the two DataFrames, excluding the merging key\n",
        "overlapping_columns = merged_tracks_df.columns.intersection(df_directionality.columns).drop('Unique_ID')\n",
        "\n",
        "# Drop the overlapping columns from the left DataFrame\n",
        "merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
        "\n",
        "# Merge the directionality back into the original DataFrame\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, df_directionality, on='Unique_ID', how='left')\n",
        "\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ijl_DXjhTvnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# @title ##Plot Directionality\n",
        "\n",
        "\n",
        "# List of variables to plot\n",
        "variables_to_plot = [\"Directionality\"]\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_pages = PdfPages(Results_Folder+'Boxplots.pdf')\n",
        "\n",
        "number_of_unique_conditions = merged_tracks_df['Condition'].nunique()  # Get the number of unique conditions\n",
        "width_per_condition = 3  # Width in inches per condition\n",
        "\n",
        "fig, axes = plt.subplots(len(variables_to_plot), 1, figsize=(number_of_unique_conditions * width_per_condition, 4))\n",
        "\n",
        "\n",
        "# Make sure axes is a list in case there's only one subplot\n",
        "if len(variables_to_plot) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, var in zip(axes, variables_to_plot):\n",
        "    # Extract the data for this variable\n",
        "    data_for_var = merged_tracks_df[['Condition', var]]\n",
        "\n",
        "        # Calculate the Interquartile Range (IQR) to identify outliers\n",
        "    Q1 = merged_tracks_df[var].quantile(0.25)\n",
        "    Q3 = merged_tracks_df[var].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for the outliers\n",
        "    lower_bound = Q1 - 8 * IQR\n",
        "    upper_bound = Q3 + 8 * IQR\n",
        "\n",
        "    # Save this data to a CSV file\n",
        "    data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}.csv\", index=False)\n",
        "    sns.boxplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, color='lightgray')  # Boxplot\n",
        "    sns.stripplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "    ax.set_ylim([max(min(merged_tracks_df[var]), lower_bound), min(max(merged_tracks_df[var]), upper_bound)])\n",
        "\n",
        "    ax.set_title(f\"{var}\")\n",
        "    ax.set_xlabel('Condition')\n",
        "    ax.set_ylabel(var)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the layout to prevent overlap\n",
        "plt.show()\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fQuHL_a8POQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tortuosity**\n",
        "This measure provides insight into the curvature and complexity of the path taken, with a value of 1 indicating a straight path between the start and end points, and values greater than 1 indicating paths with more twists and turns.\n",
        "To calculate the tortuosity of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The tortuosity, denoted as \\(T\\), is calculated using the formula:\n",
        "\n",
        "$$ T = \\frac{d_{\\text{total path}}}{d_{\\text{euclidean}}} $$\n",
        "\n"
      ],
      "metadata": {
        "id": "Gb_W3oSleJOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# @title ##Calculate tortuosity\n",
        "\n",
        "def calculate_tortuosity(group):\n",
        "    group = group.sort_values('POSITION_T')\n",
        "    start_point = group.iloc[0][['POSITION_X', 'POSITION_Y', 'POSITION_Z']]\n",
        "    end_point = group.iloc[-1][['POSITION_X', 'POSITION_Y', 'POSITION_Z']]\n",
        "\n",
        "    # Calculating Euclidean distance in 3D between start and end points\n",
        "    euclidean_distance = np.sqrt((end_point - start_point).pow(2).sum())\n",
        "\n",
        "    # Calculating the total path length in 3D\n",
        "    deltas = np.sqrt(group['POSITION_X'].diff().fillna(0)**2 +\n",
        "                     group['POSITION_Y'].diff().fillna(0)**2 +\n",
        "                     group['POSITION_Z'].diff().fillna(0)**2)\n",
        "    total_path_length = deltas.sum()\n",
        "\n",
        "    # Calculating Tortuosity\n",
        "    T = total_path_length / euclidean_distance if euclidean_distance != 0 else 0\n",
        "\n",
        "    return pd.Series({'Tortuosity': T})\n",
        "\n",
        "# Assuming merged_spots_df is your DataFrame\n",
        "# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n",
        "merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "\n",
        "# Calculate tortuosity for each track\n",
        "df_tortuosity = merged_spots_df.groupby('Unique_ID').apply(calculate_tortuosity).reset_index()\n",
        "\n",
        "# Find the overlapping columns between the two DataFrames, excluding the merging key\n",
        "overlapping_columns = merged_tracks_df.columns.intersection(df_tortuosity.columns).drop('Unique_ID')\n",
        "\n",
        "# Drop the overlapping columns from the left DataFrame\n",
        "merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
        "\n",
        "# Merge the tortuosity back into the original DataFrame\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, df_tortuosity, on='Unique_ID', how='left')\n",
        "\n",
        "# Save the DataFrame with the calculated tortuosity\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uLiAYGb2eKbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot Tortuosity\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# List of variables to plot\n",
        "variables_to_plot = [\"Tortuosity\"]\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_pages = PdfPages(Results_Folder+'Boxplots.pdf')\n",
        "\n",
        "number_of_unique_conditions = merged_tracks_df['Condition'].nunique()  # Get the number of unique conditions\n",
        "width_per_condition = 3  # Width in inches per condition\n",
        "\n",
        "fig, axes = plt.subplots(len(variables_to_plot), 1, figsize=(number_of_unique_conditions * width_per_condition, 4))\n",
        "\n",
        "\n",
        "# Make sure axes is a list in case there's only one subplot\n",
        "if len(variables_to_plot) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, var in zip(axes, variables_to_plot):\n",
        "    # Extract the data for this variable\n",
        "    data_for_var = merged_tracks_df[['Condition', var]]\n",
        "\n",
        "        # Calculate the Interquartile Range (IQR) to identify outliers\n",
        "    Q1 = merged_tracks_df[var].quantile(0.25)\n",
        "    Q3 = merged_tracks_df[var].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for the outliers\n",
        "    lower_bound = Q1 - 5 * IQR\n",
        "    upper_bound = Q3 + 5 * IQR\n",
        "\n",
        "    # Save this data to a CSV file\n",
        "    data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}.csv\", index=False)\n",
        "    sns.boxplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, color='lightgray')  # Boxplot\n",
        "    sns.stripplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "    ax.set_ylim([max(min(merged_tracks_df[var]), lower_bound), min(max(merged_tracks_df[var]), upper_bound)])\n",
        "\n",
        "    ax.set_title(f\"{var}\")\n",
        "    ax.set_xlabel('Condition')\n",
        "    ax.set_ylabel(var)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the layout to prevent overlap\n",
        "plt.show()\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m65o9_UaOnVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculate the total turning angle**\n",
        "\n",
        "This measure provides insight into the cumulative amount of turning along the path, with a value of 0 indicating a straight path with no turning, and higher values indicating paths with more turning.\n",
        "\n",
        "To calculate the Total Turning Angle of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The Total Turning Angle, denoted as \\(A\\), is the sum of the angles between each pair of consecutive direction vectors along the track, representing the cumulative amount of turning along the path.\n",
        "\n",
        "For each pair of consecutive segments in the track, we calculate the direction vectors \\( $\\vec{v_1}$ \\) and \\($ \\vec{v_2}$ \\), and the angle \\($ \\theta$ \\) between them is calculated using the formula:\n",
        "\n",
        "$$ \\cos(\\theta) = \\frac{\\vec{v_1} \\cdot \\vec{v_2}}{||\\vec{v_1}|| \\cdot ||\\vec{v_2}||} $$\n",
        "\n",
        "where \\( $\\vec{v_1} \\cdot$ $\\vec{v_2}$ \\) is the dot product of the direction vectors, and \\( $||\\vec{v_1}||$ \\) and \\( $||\\vec{v_2}||$ \\) are the magnitudes of the direction vectors. The Total Turning Angle \\( $A$ \\) is then the sum of all the angles \\( \\$theta$ \\) calculated between each pair of consecutive direction vectors along the track:\n",
        "\n",
        "$$ A = \\sum \\theta $$\n",
        "\n",
        "If either of the direction vectors is a zero vector, the angle between them is undefined, and such cases are skipped in the calculation.\n"
      ],
      "metadata": {
        "id": "IoZC4xJVhkpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Calculate the total turning angle\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_total_turning_angle(group):\n",
        "    group = group.sort_values('POSITION_T')\n",
        "    directions = group[['POSITION_X', 'POSITION_Y', 'POSITION_Z']].diff().dropna()\n",
        "    total_turning_angle = 0\n",
        "\n",
        "    for i in range(1, len(directions)):\n",
        "        dir1 = directions.iloc[i - 1]\n",
        "        dir2 = directions.iloc[i]\n",
        "\n",
        "        if np.linalg.norm(dir1) == 0 or np.linalg.norm(dir2) == 0:\n",
        "            continue\n",
        "\n",
        "        cos_angle = np.dot(dir1, dir2) / (np.linalg.norm(dir1) * np.linalg.norm(dir2))\n",
        "        cos_angle = np.clip(cos_angle, -1, 1)\n",
        "        angle = np.degrees(np.arccos(cos_angle))\n",
        "        total_turning_angle += angle\n",
        "\n",
        "    return pd.Series({'Total_Turning_Angle': total_turning_angle})\n",
        "\n",
        "# Assuming merged_spots_df is your DataFrame\n",
        "merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "\n",
        "df_turning_angle = merged_spots_df.groupby('Unique_ID').apply(calculate_total_turning_angle).reset_index()\n",
        "\n",
        "# Check if 'Total_Turning_Angle' is in the columns of df_turning_angle\n",
        "if 'Total_Turning_Angle' not in df_turning_angle.columns:\n",
        "    print(\"Error: 'Total_Turning_Angle' not in df_turning_angle columns\")\n",
        "\n",
        "# Find the overlapping columns between the two DataFrames, excluding the merging key\n",
        "overlapping_columns = merged_tracks_df.columns.intersection(df_turning_angle.columns).drop('Unique_ID')\n",
        "\n",
        "# Drop the overlapping columns from the left DataFrame\n",
        "merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
        "\n",
        "# Merge the total turning angle back into the original DataFrame\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, df_turning_angle, on='Unique_ID', how='left')\n",
        "\n",
        "# Save the DataFrame with the calculated total turning angle\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1Sbn-9ephHlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot Total Turning Angle\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# List of variables to plot\n",
        "variables_to_plot = [\"Total_Turning_Angle\"]\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_pages = PdfPages(Results_Folder+'Boxplots.pdf')\n",
        "\n",
        "number_of_unique_conditions = merged_tracks_df['Condition'].nunique()  # Get the number of unique conditions\n",
        "width_per_condition = 3  # Width in inches per condition\n",
        "\n",
        "fig, axes = plt.subplots(len(variables_to_plot), 1, figsize=(number_of_unique_conditions * width_per_condition, 4))\n",
        "\n",
        "\n",
        "# Make sure axes is a list in case there's only one subplot\n",
        "if len(variables_to_plot) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, var in zip(axes, variables_to_plot):\n",
        "    # Extract the data for this variable\n",
        "    data_for_var = merged_tracks_df[['Condition', var]]\n",
        "\n",
        "        # Calculate the Interquartile Range (IQR) to identify outliers\n",
        "    Q1 = merged_tracks_df[var].quantile(0.25)\n",
        "    Q3 = merged_tracks_df[var].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for the outliers\n",
        "    lower_bound = Q1 - 8 * IQR\n",
        "    upper_bound = Q3 + 8 * IQR\n",
        "\n",
        "    # Save this data to a CSV file\n",
        "    data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}.csv\", index=False)\n",
        "    sns.boxplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, color='lightgray')  # Boxplot\n",
        "    sns.stripplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "    ax.set_ylim([max(min(merged_tracks_df[var]), lower_bound), min(max(merged_tracks_df[var]), upper_bound)])\n",
        "\n",
        "    ax.set_title(f\"{var}\")\n",
        "    ax.set_xlabel('Condition')\n",
        "    ax.set_ylabel(var)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the layout to prevent overlap\n",
        "plt.show()\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gWwEfRJGhMA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculate the Spatial Coverage**\n",
        "\n",
        "Spatial coverage provides insight into the spatial extent covered by the object's movement, with higher values indicating that the object has covered a larger area or volume during its movement.\n",
        "\n",
        "\n",
        "To calculate the spatial coverage of a track in 2D or 3D space, we consider a series of points each with \\(x\\), \\(y\\), and optionally \\(z\\) coordinates, sorted by time. The spatial coverage, denoted as \\(S\\), represents the area (in 2D) or volume (in 3D) enclosed by the convex hull formed by the points in the track. It provides insight into the spatial extent covered by the moving object.\n",
        "\n",
        "#### In the implementation below we:\n",
        "1. **Check Dimensionality**:\n",
        "   - If the variance of the \\(z\\) coordinates is zero, implying all \\(z\\) coordinates are the same, the spatial coverage is calculated in 2D using only the \\(x\\) and \\(y\\) coordinates.\n",
        "   - If the \\(z\\) coordinates vary, the spatial coverage is calculated in 3D using the \\(x\\), \\(y\\), and \\(z\\) coordinates.\n",
        "\n",
        "2. **Form Convex Hull**:\n",
        "   - In 2D, a minimum of 3 non-collinear points is required to form a convex hull.\n",
        "   - In 3D, a minimum of 4 non-coplanar points is required to form a convex hull.\n",
        "   - If the required minimum points are not available, the spatial coverage is defined to be zero.\n",
        "\n",
        "3. **Calculate Spatial Coverage**:\n",
        "   - In 2D, the spatial coverage \\(S\\) is the area of the convex hull formed by the points in the track.\n",
        "   - In 3D, the spatial coverage \\(S\\) is the volume of the convex hull formed by the points in the track.\n",
        "\n",
        "#### Formula:\n",
        "- For 2D Spatial Coverage (Area of Convex Hull), if points are \\(P_1(x_1, y_1), P_2(x_2, y_2), \\ldots, P_n(x_n, y_n)\\):\n",
        "  $$ S_{2D} = \\text{Area of Convex Hull formed by } P_1, P_2, \\ldots, P_n $$\n",
        "\n",
        "- For 3D Spatial Coverage (Volume of Convex Hull), if points are \\(P_1(x_1, y_1, z_1), P_2(x_2, y_2, z_2), \\ldots, P_n(x_n, y_n, z_n)\\):\n",
        "  $$ S_{3D} = \\text{Volume of Convex Hull formed by } P_1, P_2, \\ldots, P_n $$\n",
        "\n"
      ],
      "metadata": {
        "id": "sYCKq5wWljf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Calculate the Spatial Coverage\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def calculate_spatial_coverage(group):\n",
        "    group = group.sort_values('POSITION_T')\n",
        "    coords = group[['POSITION_X', 'POSITION_Y', 'POSITION_Z']].values\n",
        "\n",
        "    # Check the variance of Z coordinates\n",
        "    z_variance = np.var(coords[:, 2])\n",
        "\n",
        "    if z_variance == 0:  # If variance of Z is 0, calculate 2D spatial coverage\n",
        "        if len(coords) < 3:  # Need at least 3 points for a 2D convex hull\n",
        "            return pd.Series({'Spatial_Coverage': 0})\n",
        "\n",
        "        try:\n",
        "            coords_2d = coords[:, :2]  # Use only X and Y coordinates\n",
        "            hull_2d = ConvexHull(coords_2d, qhull_options='QJ')  # 'QJ' joggles the input to avoid precision errors\n",
        "            spatial_coverage = hull_2d.volume  # Area of the convex hull in 2D\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating 2D spatial coverage: {e}\")\n",
        "            spatial_coverage = 0\n",
        "    else:  # If variance of Z is not 0, calculate 3D spatial coverage\n",
        "        if len(coords) < 4:  # Need at least 4 points for a 3D convex hull\n",
        "            return pd.Series({'Spatial_Coverage': 0})\n",
        "\n",
        "        try:\n",
        "            hull = ConvexHull(coords, qhull_options='QJ')  # 'QJ' joggles the input to avoid precision errors\n",
        "            spatial_coverage = hull.volume  # Volume of the convex hull in 3D\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating 3D spatial coverage: {e}\")\n",
        "            spatial_coverage = 0\n",
        "\n",
        "    return pd.Series({'Spatial_Coverage': spatial_coverage})\n",
        "\n",
        "# Assuming merged_spots_df is your DataFrame\n",
        "merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "\n",
        "# Calculate spatial coverage for each track\n",
        "df_spatial_coverage = merged_spots_df.groupby('Unique_ID').apply(calculate_spatial_coverage).reset_index()\n",
        "\n",
        "# Find the overlapping columns between the two DataFrames, excluding the merging key\n",
        "overlapping_columns = merged_tracks_df.columns.intersection(df_spatial_coverage.columns).drop('Unique_ID')\n",
        "\n",
        "# Drop the overlapping columns from the left DataFrame\n",
        "merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
        "\n",
        "# Merge the spatial coverage back into the original DataFrame\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, df_spatial_coverage, on='Unique_ID', how='left')\n",
        "\n",
        "# Save the DataFrame with the calculated spatial coverage\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A-46mr2inFhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot The Spatial Coverage\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# List of variables to plot\n",
        "variables_to_plot = [\"Spatial_Coverage\"]\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_pages = PdfPages(Results_Folder+'Boxplots.pdf')\n",
        "\n",
        "number_of_unique_conditions = merged_tracks_df['Condition'].nunique()  # Get the number of unique conditions\n",
        "width_per_condition = 3  # Width in inches per condition\n",
        "\n",
        "fig, axes = plt.subplots(len(variables_to_plot), 1, figsize=(number_of_unique_conditions * width_per_condition, 4))\n",
        "\n",
        "\n",
        "# Make sure axes is a list in case there's only one subplot\n",
        "if len(variables_to_plot) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, var in zip(axes, variables_to_plot):\n",
        "    # Extract the data for this variable\n",
        "    data_for_var = merged_tracks_df[['Condition', var]]\n",
        "\n",
        "        # Calculate the Interquartile Range (IQR) to identify outliers\n",
        "    Q1 = merged_tracks_df[var].quantile(0.25)\n",
        "    Q3 = merged_tracks_df[var].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for the outliers\n",
        "    lower_bound = Q1 - 8 * IQR\n",
        "    upper_bound = Q3 + 8 * IQR\n",
        "\n",
        "    # Save this data to a CSV file\n",
        "    data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}.csv\", index=False)\n",
        "    sns.boxplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, color='lightgray')  # Boxplot\n",
        "    sns.stripplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "    ax.set_ylim([max(min(merged_tracks_df[var]), lower_bound), min(max(merged_tracks_df[var]), upper_bound)])\n",
        "\n",
        "    ax.set_title(f\"{var}\")\n",
        "    ax.set_xlabel('Condition')\n",
        "    ax.set_ylabel(var)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the layout to prevent overlap\n",
        "plt.show()\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VL-0ABmtlxaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------\n",
        "\n",
        "# **Part 3: Plot track parameters**\n",
        "-------------------------------------------\n",
        "\n",
        "<font size = 4> In this section you can plot all the track parameters previously computed. Data and graphs are automatically saved in your result folder.\n"
      ],
      "metadata": {
        "id": "joRI14WVUPuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot track parameters\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# Create a list of potential variables from DataFrame columns\n",
        "all_columns = merged_tracks_df.columns.tolist()\n",
        "\n",
        "# Remove unwanted columns like 'condition' and 'repeat' from the list\n",
        "selectable_columns = [col for col in all_columns if col not in ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']]\n",
        "\n",
        "# Create checkboxes for selectable columns\n",
        "variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "\n",
        "# Arrange and display checkboxes in the notebook\n",
        "display(widgets.VBox([\n",
        "    widgets.Label('Variables to Plot:'),\n",
        "    widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n",
        "]))\n",
        "\n",
        "# Define the plotting function\n",
        "def plot_selected_vars(button):\n",
        "  print(\"Plotting in progress...\")\n",
        "\n",
        "  variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "\n",
        "  pdf_pages = PdfPages(f\"{Results_Folder}/boxplots.pdf\")\n",
        "\n",
        "# Determine the number of variables to plot\n",
        "  n_plots = len(variables_to_plot)\n",
        "\n",
        "# If no variables are selected, avoid creating a plot\n",
        "  if n_plots == 0:\n",
        "    print(\"No variables selected for plotting\")\n",
        "  else:\n",
        "    # Set the height of each subplot and figure width\n",
        "    subplot_height = 4  # Adjust as per your requirement\n",
        "    number_of_unique_conditions = merged_tracks_df['Condition'].nunique()  # Get the number of unique conditions\n",
        "    width_per_condition = 3  # Width in inches per condition\n",
        "\n",
        "    fig_width = number_of_unique_conditions * width_per_condition  # Adjust as per your requirement\n",
        "\n",
        "    # Calculate the total figure height\n",
        "    fig_height = n_plots * subplot_height\n",
        "\n",
        "    # Create subplots with dynamic figure size\n",
        "    fig, axes = plt.subplots(n_plots, 1, figsize=(fig_width, fig_height))\n",
        "\n",
        "    # Make axes iterable in case there's only one subplot\n",
        "    if n_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, var in zip(axes, variables_to_plot):\n",
        "        data_for_var = merged_tracks_df[['Condition', var]]\n",
        "\n",
        "        # Calculate the Interquartile Range (IQR) to identify outliers\n",
        "        Q1 = merged_tracks_df[var].quantile(0.25)\n",
        "        Q3 = merged_tracks_df[var].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for the outliers\n",
        "        lower_bound = Q1 - 8 * IQR\n",
        "        upper_bound = Q3 + 8 * IQR\n",
        "\n",
        "        data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}.csv\", index=False)\n",
        "        sns.boxplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, color='lightgray')\n",
        "        sns.stripplot(x='Condition', y=var, data=merged_tracks_df, ax=ax, hue='Repeat', dodge=True, jitter=True, alpha=0.2)\n",
        "        ax.set_ylim([max(min(merged_tracks_df[var]), lower_bound), min(max(merged_tracks_df[var]), upper_bound)])\n",
        "\n",
        "        ax.set_title(f\"{var}\")\n",
        "        ax.set_xlabel('Condition')\n",
        "        ax.set_ylabel(var)\n",
        "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "    pdf_pages.savefig(fig)\n",
        "    pdf_pages.close()\n",
        "    plt.show()\n",
        "\n",
        "# Create a button that will execute the plotting function when clicked\n",
        "button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n",
        "button.on_click(plot_selected_vars)\n",
        "display(button)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AdzJCNTyUTEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "# **Part 4: Explore your high-dimensional data (work in progress)**\n",
        "--------\n",
        "\n",
        "<font size = 4> The workflow provided below is inspired by [CellPlato](https://github.com/Michael-shannon/cellPLATO)"
      ],
      "metadata": {
        "id": "6tFTER9rg2Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1: Choose the track metrics to use for clustering**\n"
      ],
      "metadata": {
        "id": "_7bKh1LujZsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Choose the track metrics to use\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "selected_df = pd.DataFrame()\n",
        "\n",
        "# Filter out non-numeric columns\n",
        "numeric_df = merged_tracks_df.select_dtypes(include=['float64', 'int64'])  # Selecting only numeric columns\n",
        "\n",
        "# Create a list of column names\n",
        "column_names = numeric_df.columns.tolist()\n",
        "\n",
        "# Create a checkbox for each column\n",
        "checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in column_names]\n",
        "\n",
        "# Arrange checkboxes in a 2x grid\n",
        "grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n",
        "\n",
        "# Create a button to trigger the selection\n",
        "button = widgets.Button(description=\"Select the track parameters\", layout=widgets.Layout(width='400px'))\n",
        "\n",
        "# Define the button click event handler\n",
        "def on_button_click(b):\n",
        "    global selected_df  # Declare selected_df as global\n",
        "\n",
        "    # Get the selected columns from the checkboxes\n",
        "    selected_columns = [box.description for box in checkboxes if box.value]\n",
        "\n",
        "    # Extract the selected columns from the DataFrame\n",
        "    selected_df = numeric_df[selected_columns]\n",
        "\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n",
        "\n",
        "    if nan_columns:\n",
        "        warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n",
        "        for col in nan_columns:\n",
        "            selected_df = selected_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "# Set the button click event handler\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "# Display the grid of checkboxes and the button\n",
        "display(grid, button)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EP-kp5JkR3xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2: UMAP and HDBSCAN**\n",
        "\n",
        "<font size = 4> The given code performs UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction on the merged tracks dataframe, focusing on its numeric columns, and visualizes the result. In the provided UMAP code, the parameters `n_neighbors`, `min_dist`, and `n_components` are crucial for determining the structure and appearance of the resulting low-dimensional representation of the data.\n",
        "\n",
        "<font size = 4>`n_neighbors`: This parameter controls how UMAP balances local versus global structure in the data. It determines the size of the local neighborhood UMAP will look at when learning the manifold structure of the data.\n",
        "- A smaller value emphasizes the local structure of the data, potentially at the expense of the global structure.\n",
        "- A larger value allows UMAP to consider more distant neighbors, emphasizing more on the global structure of the data.\n",
        "- Typically, values in the range of 5 to 50 are chosen, depending on the density and scale of the data.\n",
        "\n",
        "<font size = 4>`min_dist`: This parameter controls how tightly UMAP is allowed to pack points together. It determines the minimum distance between points in the low-dimensional representation.\n",
        "- Setting it to a low value will allow points to be packed more closely, potentially revealing clusters in the data.\n",
        "- A higher value ensures that points are more spread out in the representation.\n",
        "- Values usually range between 0 and 1.\n",
        "\n",
        "<font size = 4>`n_dimension`: This parameter determines the number of dimensions in the low-dimensional space that the data will be reduced to.\n",
        "For visualization purposes, `n_dimension` is typically set to 2 or 3 to obtain 2D or 3D representations, respectively.\n"
      ],
      "metadata": {
        "id": "GLlDuCKmlxvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Perform UMAP\n",
        "import umap\n",
        "\n",
        "n_neighbors = 15  # @param {type: \"number\"}\n",
        "min_dist = 0.1  # @param {type: \"number\"}\n",
        "n_dimension = 3  # @param {type: \"slider\", min: 1, max: 3}\n",
        "\n",
        "# Initialize UMAP object with the specified settings\n",
        "reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_dimension, random_state=42)\n",
        "embedding = reducer.fit_transform(selected_df)\n",
        "\n",
        "# Create dynamic column names based on n_components\n",
        "column_names = [f'UMAP dimension {i}' for i in range(1, n_dimension + 1)]\n",
        "\n",
        "# Create a DataFrame with the UMAP results\n",
        "umap_df = pd.DataFrame(embedding, columns=column_names)\n",
        "\n",
        "# Concatenate the conditions (if available)\n",
        "if 'Condition' in merged_tracks_df.columns:\n",
        "    umap_df = pd.concat([umap_df, merged_tracks_df['Condition'].reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Visualize the UMAP projection\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# The plot will adjust automatically based on the n_components\n",
        "if n_dimension == 2:\n",
        "    sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=umap_df, palette='viridis', s=60)\n",
        "    plt.title('UMAP Projection of the Dataset')\n",
        "    plt.show()\n",
        "elif n_dimension == 1:\n",
        "    sns.stripplot(x=column_names[0], hue='Condition', data=umap_df, palette='viridis', jitter=0.05, size=6)\n",
        "    plt.title('UMAP Projection of the Dataset')\n",
        "    plt.show()\n",
        "else:\n",
        "    # umap_df should have columns like 'UMAP dimension 1', 'UMAP dimension 2', 'UMAP dimension 3', and 'condition'\n",
        "    import plotly.express as px\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    fig = px.scatter_3d(umap_df,\n",
        "                    x='UMAP dimension 1',\n",
        "                    y='UMAP dimension 2',\n",
        "                    z='UMAP dimension 3',\n",
        "                    color='Condition')\n",
        "\n",
        "    for trace in fig.data:\n",
        "      trace.marker.size = 2  # You can set this to any desired value\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h-HJIe9ug1bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **HDBSCAN**\n",
        "\n",
        "<font size = 4> The provided code employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify clusters within a dataset that has already undergone UMAP dimensionality reduction. HDBSCAN is utilized for its proficiency in determining the optimal number of clusters while managing varied densities within the data.\n",
        "\n",
        "In the provided HDBSCAN code, the parameters `min_samples`, `min_cluster_size`, and `metric` are crucial for determining the structure and appearance of the resulting clusters in the data.\n",
        "\n",
        "`min_samples`: This parameter primarily controls the degree to which the algorithm is willing to declare noise. It's the number of samples in a neighborhood for a point to be considered as a core point.\n",
        "- A smaller value of `min_samples` makes the algorithm more prone to declaring points as part of a cluster, potentially leading to larger clusters and fewer noise points.\n",
        "- A larger value makes the algorithm more conservative, resulting in more points declared as noise and smaller, more defined clusters.\n",
        "- The choice of `min_samples` typically depends on the density of the data; denser datasets may require a larger value.\n",
        "\n",
        "`min_cluster_size`: This parameter determines the smallest size grouping that you wish to consider a cluster.\n",
        "- A smaller value will allow the formation of smaller clusters, whereas a larger value will prevent small isolated groups of points from being declared as clusters.\n",
        "- The choice of `min_cluster_size` depends on the scale of the data and the desired level of granularity in the clustering.\n",
        "\n",
        "`metric`: This parameter is the metric used for distance computation between data points, and it affects the shape of the clusters.\n",
        "- The `euclidean` metric is a good starting point, and depending on the clustering results and the data type, it might be beneficial to experiment with different metrics.\n"
      ],
      "metadata": {
        "id": "hMx8DVRMmvKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Run to see more information about the available metrics\n",
        "print(\"\"\"\n",
        "Metric                   Description                                                               Suitable For\n",
        "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "Euclidean                Standard distance metric.                                                 Numerical data.\n",
        "Manhattan                Sum of absolute differences.                                              Numerical/Categorical data.\n",
        "Chebyshev                Maximum value of absolute differences.                                    Numerical data.\n",
        "Minkowski                Generalization of Euclidean and Manhattan distance.                       Numerical data.\n",
        "Bray-Curtis              Dissimilarity between sample sets.                                        Numerical data.\n",
        "Canberra                 Weighted version of Manhattan distance.                                   Numerical data.\n",
        "Mahalanobis              Distance between a point and a distribution.                              Numerical data.\n",
        "\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PGdYdL7hnrpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Identify clusters using HDBSCAN\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "clustering_data_source = 'umap'  # @param ['umap', 'raw']\n",
        "min_samples = 15  # @param {type: \"number\"}\n",
        "min_cluster_size = 50  # @param {type: \"number\"}\n",
        "metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'minkowski', 'braycurtis', 'canberra', 'mahalanobis']\n",
        "\n",
        "# Apply HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)  # You may need to tune these parameters\n",
        "\n",
        "if clustering_data_source == 'umap':\n",
        "  if n_dimension == 2:\n",
        "    clusterer.fit(umap_df[['UMAP dimension 1', 'UMAP dimension 2']])  # Use the UMAP results for clustering\n",
        "\n",
        "  if n_dimension == 3:\n",
        "    clusterer.fit(umap_df[['UMAP dimension 1', 'UMAP dimension 2', 'UMAP dimension 3']])  # Use the UMAP results for clustering\n",
        "\n",
        "else:\n",
        "  clusterer.fit(selected_df.select_dtypes(include=['number']))\n",
        "\n",
        "# Add the cluster labels to your UMAP DataFrame\n",
        "umap_df['Cluster'] = clusterer.labels_\n",
        "\n",
        "# Plotting the results\n",
        "if n_dimension == 2:\n",
        "\n",
        "  plt.figure(figsize=(12,10))\n",
        "  sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', hue='Cluster', palette='set2', data=umap_df, s=60)\n",
        "  plt.title('Clusters Identified by HDBSCAN')\n",
        "  plt.show()\n",
        "\n",
        "if n_dimension == 3:\n",
        "\n",
        "  fig = px.scatter_3d(umap_df,\n",
        "                    x='UMAP dimension 1',\n",
        "                    y='UMAP dimension 2',\n",
        "                    z='UMAP dimension 3',\n",
        "                    color='Cluster')\n",
        "\n",
        "  for trace in fig.data:\n",
        "    trace.marker.size = 2  # You can set this to any desired value\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1VV75MUjixkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px  # Importing plotly for 3D plots\n",
        "\n",
        "# @title ##Identify exemplar cells using HDBSCAN (not available)\n",
        "\n",
        "\n",
        "# Extracting exemplar points\n",
        "exemplars = []\n",
        "for exemplar in clusterer.exemplars_:\n",
        "    exemplars.extend(exemplar)\n",
        "\n",
        "# Flatten the exemplars list of lists into a single list\n",
        "flattened_exemplars = [index for sublist in exemplars for index in sublist]\n",
        "\n",
        "# Now pass the flattened list to iloc\n",
        "exemplar_df = umap_df.iloc[flattened_exemplars]\n",
        "\n",
        "# Plotting clusters and exemplar points\n",
        "if n_dimension == 1:\n",
        "    plt.figure(figsize=(12,10))\n",
        "    sns.stripplot(x='UMAP dimension 1', hue='Cluster', data=umap_df, palette='viridis', jitter=0.05, size=6)\n",
        "    sns.stripplot(x='UMAP dimension 1', color='red', label='Exemplars', data=exemplar_df, jitter=0.05, size=10, marker='X')\n",
        "    plt.title('Clusters and Exemplar Cells Identified by HDBSCAN')\n",
        "    plt.show()\n",
        "\n",
        "elif n_dimension == 2:\n",
        "    plt.figure(figsize=(12,10))\n",
        "    sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', hue='Cluster', palette='viridis', data=umap_df, s=60)\n",
        "    sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', color='red', label='Exemplars', data=exemplar_df, s=100, marker='X')\n",
        "    plt.title('Clusters and Exemplar Cells Identified by HDBSCAN')\n",
        "    plt.show()\n",
        "\n",
        "elif n_dimension == 3:\n",
        "    fig = px.scatter_3d(umap_df,\n",
        "                        x='UMAP dimension 1',\n",
        "                        y='UMAP dimension 2',\n",
        "                        z='UMAP dimension 3',\n",
        "                        color='Cluster',\n",
        "                        color_discrete_sequence=px.colors.qualitative.Vivid)\n",
        "\n",
        "    exemplar_fig = px.scatter_3d(exemplar_df,\n",
        "                                 x='UMAP dimension 1',\n",
        "                                 y='UMAP dimension 2',\n",
        "                                 z='UMAP dimension 3',\n",
        "                                 color='red')\n",
        "\n",
        "    for trace in fig.data:\n",
        "        trace.marker.size = 2\n",
        "\n",
        "    for trace in exemplar_fig.data:\n",
        "        trace.marker.size = 5\n",
        "        trace.marker.symbol = 'x'\n",
        "\n",
        "    fig.add_trace(exemplar_fig.data[0])\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_4f4_I16ji5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fingerprint**\n",
        "\n",
        "This section is designed to visualize the distribution of different clusters within each condition in a dataset, showing the 'fingerprint' of each cluster per condition."
      ],
      "metadata": {
        "id": "eBFfmTo5e-Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot the 'fingerprint' of each cluster per condition\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# Group by 'Condition' and 'Cluster' and calculate the size of each group\n",
        "cluster_counts = umap_df.groupby(['Condition', 'Cluster']).size().reset_index(name='counts')\n",
        "\n",
        "# Calculate the total number of points per condition\n",
        "total_counts = umap_df.groupby('Condition').size().reset_index(name='total_counts')\n",
        "\n",
        "# Merge the DataFrames on 'Condition' to calculate percentages\n",
        "percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n",
        "percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n",
        "\n",
        "# Save the percentage_df DataFrame as a CSV file\n",
        "percentage_df.to_csv(Results_Folder+'/UMAP_percentage_results.csv', index=False)\n",
        "\n",
        "# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n",
        "pivot_df = percentage_df.pivot(index='Condition', columns='Cluster', values='percentage')\n",
        "\n",
        "# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n",
        "pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_pages = PdfPages(Results_Folder+'/UMAP_Cluster_Fingerprint_Plot.pdf')\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n",
        "plt.title('Percentage in each cluster per Condition')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Condition')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B8S0uwynjht1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3: t-SNE and HDBSCAN**\n",
        "\n",
        "This section performs t-SNE (t-Distributed Stochastic Neighbor Embedding) on your track dataset to reduce its dimensionality and visualize it in 1, 2, or 3 dimensions.\n",
        "\n",
        "`perplexity`:\n",
        "Determines the balance between preserving the local and global structure of the data.\n",
        "\n",
        "*   Typical Values: Between 5 and 50.\n",
        "*   How to Choose: Higher values consider more neighbors for each point, which could reveal more global structure but might mix different clusters. Lower values focus more on preserving the local structure, potentially leading to more distinct clusters.\n",
        "\n",
        "`n_iter`:\n",
        "The number of iterations for the optimization.\n",
        "  \n",
        "*  Typical Values: Usually, a minimum of 250.\n",
        "*  How to Choose: If the results lack stability or the cost function hasn‚Äôt converged, consider increasing this value.\n",
        "\n",
        "`n_dimension`:\n",
        "The number of dimensions to reduce the data to (1, 2, or 3 for this code).\n",
        "\n",
        "*   How to Choose: Typically 2 or 3 for visualization purposes. Choose 1 for a linear representation.\n",
        "\n"
      ],
      "metadata": {
        "id": "QP6qwvH-hY-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Perform t-SNE\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "perplexity = 30  # @param {type: \"number\"}\n",
        "n_iter = 300  # @param {type: \"number\"}\n",
        "n_dimension = 3  # @param {type: \"slider\", min: 1, max: 3}\n",
        "\n",
        "scaled_df = StandardScaler().fit_transform(selected_df)  # Scaling the numeric features\n",
        "\n",
        "tsne = TSNE(n_components=n_dimension, perplexity=perplexity, n_iter=n_iter, random_state=42)\n",
        "tsne_results = tsne.fit_transform(scaled_df)\n",
        "\n",
        "# Adjust the DataFrame creation to handle 1, 2, or 3 dimensions\n",
        "dimension_columns = [f'Dimension {i}' for i in range(1, n_dimension + 1)]\n",
        "tsne_df = pd.DataFrame(data=tsne_results, columns=dimension_columns)\n",
        "tsne_df['Condition'] = merged_tracks_df['Condition'].reset_index(drop=True)  # Reset index to avoid misalignment\n",
        "\n",
        "# Plotting the results\n",
        "if n_dimension == 1:\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.stripplot(x='Dimension 1', hue='Condition', data=tsne_df, palette='viridis', jitter=0.05, size=6)\n",
        "    plt.title('t-SNE Plot')\n",
        "    plt.show()\n",
        "\n",
        "elif n_dimension == 2:\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.scatterplot(\n",
        "        x='Dimension 1', y='Dimension 2',\n",
        "        hue='Condition',\n",
        "        palette=sns.color_palette(\"hsv\", len(tsne_df['Condition'].unique())),\n",
        "        data=tsne_df,\n",
        "        legend=\"full\",\n",
        "        alpha=0.9\n",
        "    )\n",
        "    plt.title('t-SNE Plot')\n",
        "    plt.show()\n",
        "\n",
        "elif n_dimension == 3:\n",
        "    import plotly.express as px  # Ensure plotly is imported\n",
        "\n",
        "    fig = px.scatter_3d(tsne_df,\n",
        "                        x='Dimension 1',\n",
        "                        y='Dimension 2',\n",
        "                        z='Dimension 3',\n",
        "                        color='Condition',  # Use 'Condition' as color\n",
        "                        color_discrete_sequence=px.colors.qualitative.Vivid)  # Use a color sequence suitable for categorical data\n",
        "\n",
        "    for trace in fig.data:\n",
        "        trace.marker.size = 2  # You can set this to any desired value\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KelZFuntkDF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Identify clusters using HDBSCAN\n",
        "\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "clustering_data_source = 'tsne'  # @param ['tsne', 'raw']\n",
        "min_samples = 15  # @param {type: \"number\"}\n",
        "min_cluster_size = 50  # @param {type: \"number\"}\n",
        "metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'minkowski', 'braycurtis', 'canberra', 'mahalanobis']\n",
        "\n",
        "# Apply HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)\n",
        "\n",
        "if clustering_data_source == 'tsne':\n",
        "  clusterer.fit(tsne_results)\n",
        "else:\n",
        "  clusterer.fit(selected_df.select_dtypes(include=['number']))\n",
        "\n",
        "# Add cluster labels to the DataFrame\n",
        "tsne_df['Cluster'] = clusterer.labels_\n",
        "\n",
        "# Plotting the results\n",
        "if n_dimension == 2:\n",
        "  plt.figure(figsize=(12,10))\n",
        "  sns.scatterplot(x='Dimension 1', y='Dimension 2', hue='Cluster', palette='set2', data=tsne_df, s=60)\n",
        "  plt.title('Clusters Identified by HDBSCAN')\n",
        "  plt.show()\n",
        "\n",
        "elif n_dimension == 3:\n",
        "  import plotly.express as px  # Ensure plotly is imported\n",
        "  fig = px.scatter_3d(tsne_df,\n",
        "                      x='Dimension 1',\n",
        "                      y='Dimension 2',\n",
        "                      z='Dimension 3',\n",
        "                      color='Cluster')\n",
        "  for trace in fig.data:\n",
        "    trace.marker.size = 2\n",
        "  fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ypPWiZWCkZF5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# @title ##Plot the 'fingerprint' of each cluster per condition\n",
        "\n",
        "\n",
        "\n",
        "# Group by 'Condition' and 'Cluster' and calculate the size of each group\n",
        "cluster_counts = tsne_df.groupby(['Condition', 'Cluster']).size().reset_index(name='counts')\n",
        "\n",
        "# Calculate the total number of points per condition\n",
        "total_counts = tsne_df.groupby('Condition').size().reset_index(name='total_counts')\n",
        "\n",
        "# Merge the DataFrames on 'Condition' to calculate percentages\n",
        "percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n",
        "percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n",
        "\n",
        "# Save the percentage_df DataFrame as a CSV file\n",
        "percentage_df.to_csv(Results_Folder+'/tsne_percentage_results.csv', index=False)\n",
        "\n",
        "# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n",
        "pivot_df = percentage_df.pivot(index='Condition', columns='Cluster', values='percentage')\n",
        "\n",
        "# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n",
        "pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_pages = PdfPages(Results_Folder+'/tsne_Cluster_Fingerprint_Plot.pdf')\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n",
        "plt.title('Percentage in each cluster per Condition')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Condition')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cZJWY5rMkgmW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}