{"cells":[{"cell_type":"markdown","metadata":{"id":"xF4zYMmXULP7"},"source":["# **CellTracksColab - Distance to ROI**\n","---\n","<font size = 4> This notebook is specifically designed to analyze movement tracks in relation to designated Regions of Interest (ROIs). Its primary aim is to compute and analyze the distances between moving objects (tracks) and ROIs, which may also be dynamic. By evaluating these distances over time, the notebook provides insights into the spatial behavior of the tracked entities relative to the ROIs. The notebook integrates data processing, distance calculations, and statistical analysis to offer a comprehensive tool for researchers to study movement patterns, interactions, and the overall dynamics of moving entities in relation to critical areas of interest.\n","\n","---\n","\n","# **Part 0. Before getting started**\n","---\n","\n","<font size = 5>**Important notes**\n","\n","---\n","\n","## Data Requirements for Analysis\n","\n","<font size = 4>Be advised of one significant limitation inherent to this notebook.\n","\n","<font size = 4 color=\"red\">**This notebook only supports 2D + t datasets**</font>.\n","\n","---\n","# Prerequisites for Using This Notebook\n","\n","<font size = 4>To effectively utilize this notebook for analyzing track distances to Regions of Interest (ROIs), the following prerequisites are essential:\n","<font size = 4>\n","1. **DataFrames from CellTrackColab**:\n","   - Ensure you have the `spots` and `tracks` DataFrames compiled by CellTrackColab.\n","<font size = 4>\n","2. **ROI Images in TIF Format**:\n","   - The ROI images or movies should be in TIF (`.tif`) file format. These images should be mask or label images where the background has a pixel value of 0.\n","<font size = 4>\n","3. **Proper Naming of ROI Images**:\n","   - Adopt a consistent and unique naming convention for your ROIs. This naming should be reflected in both your data analysis and image files.\n","   - Follow the file naming format: `File_name_ROI_name.tif`. For example, if your tracking file is named 'sample' and the ROI is 'nuclei', the corresponding image file should be named 'sample_nuclei.tif'.\n","   - Place all ROI image files in the same folder for streamlined access and analysis.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JrkfFr7mgZmA"},"outputs":[],"source":["# @title #MIT License\n","\n","print(\"\"\"\n","**MIT License**\n","\n","Copyright (c) 2023 Guillaume Jacquemet\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE.\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"Y4-Ft-yNRVCc"},"source":["--------------------------------------------------------\n","# **Part 1: Prepare the session and load your data**\n","--------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"9h0prdayn0qG"},"source":["## **1.1. Install key dependencies**\n","---\n","<font size = 4>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rAP0ahCzn1V6"},"outputs":[],"source":["#@markdown ##Play to install\n","%pip -q install pandas scikit-learn\n","%pip -q install hdbscan\n","%pip -q install umap-learn\n","%pip -q install plotly\n","%pip -q install tqdm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"uiuJFVIsXOsl"},"outputs":[],"source":["#@markdown ##Play to load the dependancies\n","\n","import ipywidgets as widgets\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import numpy as np\n","import itertools\n","from matplotlib.gridspec import GridSpec\n","import requests\n","\n","!pip freeze > requirements.txt\n","\n","# Current version of the notebook the user is running\n","current_version = \"0.9.1\"\n","Notebook_name = 'Distance_to_ROI'\n","\n","# URL to the raw content of the version file in the repository\n","version_url = \"https://raw.githubusercontent.com/guijacquemet/CellTracksColab/main/Notebook/latest_version.txt\"\n","\n","# Function to define colors for formatting messages\n","class bcolors:\n","    WARNING = '\\033[91m'  # Red color for warning messages\n","    ENDC = '\\033[0m'      # Reset color to default\n","\n","# Check if this is the latest version of the notebook\n","try:\n","    All_notebook_versions = pd.read_csv(version_url, dtype=str)\n","    print('Notebook version: ' + current_version)\n","\n","    # Check if 'Version' column exists in the DataFrame\n","    if 'Version' in All_notebook_versions.columns:\n","        Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Notebook_name]['Version'].iloc[0]\n","        print('Latest notebook version: ' + Latest_Notebook_version)\n","\n","        if current_version == Latest_Notebook_version:\n","            print(\"This notebook is up-to-date.\")\n","        else:\n","            print(bcolors.WARNING + \"A new version of this notebook has been released. We recommend that you download it at https://github.com/guijacquemet/CellTracksColab\" + bcolors.ENDC)\n","    else:\n","        print(\"The 'Version' column is not present in the version file.\")\n","except requests.exceptions.RequestException as e:\n","    print(\"Unable to fetch the latest version information. Please check your internet connection.\")\n","except Exception as e:\n","    print(\"An error occurred:\", str(e))\n","\n","#----------------------- Key functions -----------------------------#\n","\n","# Function to calculate Cohen's d\n","def cohen_d(group1, group2):\n","    diff = group1.mean() - group2.mean()\n","    n1, n2 = len(group1), len(group2)\n","    var1 = group1.var()\n","    var2 = group2.var()\n","    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n","    d = diff / np.sqrt(pooled_var)\n","    return d\n","\n","\n","def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n","    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n","\n","    # Estimating the number of chunks based on the provided chunk size\n","    num_chunks = int(len(df) / chunk_size) + 1\n","\n","    # Create a tqdm instance for progress tracking\n","    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n","        # Open the file for writing\n","        with open(path, \"w\") as f:\n","            # Write the header once at the beginning\n","            df.head(0).to_csv(f, index=False)\n","\n","            for chunk in np.array_split(df, num_chunks):\n","                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n","                pbar.update(len(chunk))\n","\n","def check_for_nans(df, df_name):\n","    \"\"\"\n","    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n","\n","    Args:\n","    df (pd.DataFrame): DataFrame to be checked for NaN values.\n","    df_name (str): The name of the DataFrame as a string, used for printing.\n","    \"\"\"\n","    # Check if the DataFrame has any NaN values and print a warning if it does.\n","    nan_columns = df.columns[df.isna().any()].tolist()\n","\n","    if nan_columns:\n","        for col in nan_columns:\n","            nan_count = df[col].isna().sum()\n","            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n","    else:\n","        print(f\"No NaN values found in {df_name}.\")\n","\n","def save_parameters(params, file_path, param_type):\n","    # Convert params dictionary to a DataFrame for human readability\n","    new_params_df = pd.DataFrame(list(params.items()), columns=['Parameter', 'Value'])\n","    new_params_df['Type'] = param_type\n","\n","    if os.path.exists(file_path):\n","        # Read existing file\n","        existing_params_df = pd.read_csv(file_path)\n","\n","        # Merge the new parameters with the existing ones\n","        # Update existing parameters or append new ones\n","        updated_params_df = pd.merge(existing_params_df, new_params_df,\n","                                     on=['Type', 'Parameter'],\n","                                     how='outer',\n","                                     suffixes=('', '_new'))\n","\n","        # If there's a new value, update it, otherwise keep the old value\n","        updated_params_df['Value'] = updated_params_df['Value_new'].combine_first(updated_params_df['Value'])\n","\n","        # Drop the temporary new value column\n","        updated_params_df.drop(columns='Value_new', inplace=True)\n","    else:\n","        # Use new parameters DataFrame directly if file doesn't exist\n","        updated_params_df = new_params_df\n","\n","    # Save the updated DataFrame to CSV\n","    updated_params_df.to_csv(file_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"3Kzd_8GUnpbw"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GA1wCrkoV4i5"},"outputs":[],"source":["#@markdown ##Play the cell to connect your Google Drive to Colab\n","\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bsDAwkSOo1gV"},"source":["## **1.3. Load your CellTracksColab dataset**\n","---\n","\n","<font size=\"4\"> Before proceeding, please ensure that your data has been properly processed using CellTracksColab. Typically, your Track table should be named `merged_Tracks.csv`, and your Spot table should be named `merged_Spots.csv`.\n","\n","For the `Results_Folder` parameter, you can choose the same folder that already contains all the results associated with your dataset. Any results generated by this notebook will be saved in the `Distance_to_ROI` subfolder.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CQKXq3giI3nX"},"outputs":[],"source":["#@markdown ##Provide the path to your dataset:\n","\n","import os\n","import re\n","import glob\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import requests\n","import zipfile\n","\n","#@markdown ###You have existing dataframes, provide the path to your:\n","\n","Track_table = ''  # @param {type: \"string\"}\n","Spot_table = ''  # @param {type: \"string\"}\n","\n","#@markdown ###Provide the path to your Result folder\n","\n","Results_Folder = \"\"  # @param {type: \"string\"}\n","\n","if not Results_Folder:\n","    Results_Folder = '/content/Results'  # Default Results_Folder path if not defined\n","\n","if not os.path.exists(Results_Folder):\n","    os.makedirs(Results_Folder)  # Create Results_Folder if it doesn't exist\n","\n","# Print the location of the result folder\n","print(f\"Result folder is located at: {Results_Folder}\")\n","\n","def validate_tracks_df(df):\n","    \"\"\"Validate the tracks dataframe for necessary columns and data types.\"\"\"\n","    required_columns = ['TRACK_ID']\n","    for col in required_columns:\n","        if col not in df.columns:\n","            print(f\"Error: Column '{col}' missing in tracks dataframe.\")\n","            return False\n","\n","    # Additional data type checks or value ranges can be added here\n","    return True\n","\n","def validate_spots_df(df):\n","    \"\"\"Validate the spots dataframe for necessary columns and data types.\"\"\"\n","    required_columns = ['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T']\n","    for col in required_columns:\n","        if col not in df.columns:\n","            print(f\"Error: Column '{col}' missing in spots dataframe.\")\n","            return False\n","\n","    # Additional data type checks or value ranges can be added here\n","    return True\n","\n","def check_unique_id_match(df1, df2):\n","    df1_ids = set(df1['Unique_ID'])\n","    df2_ids = set(df2['Unique_ID'])\n","\n","    # Check if the IDs in the two dataframes match\n","    if df1_ids == df2_ids:\n","        print(\"The Unique_ID values in both dataframes match perfectly!\")\n","    else:\n","        missing_in_df1 = df2_ids - df1_ids\n","        missing_in_df2 = df1_ids - df2_ids\n","\n","        if missing_in_df1:\n","            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n","\n","        if missing_in_df2:\n","            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n","\n","# For existing dataframes\n","if Track_table:\n","    print(\"Loading track table file....\")\n","    merged_tracks_df = pd.read_csv(Track_table, low_memory=False)\n","    if not validate_tracks_df(merged_tracks_df):\n","        print(\"Error: Validation failed for loaded tracks dataframe.\")\n","\n","if Spot_table:\n","    print(\"Loading spot table file....\")\n","    merged_spots_df = pd.read_csv(Spot_table, low_memory=False)\n","    if not validate_spots_df(merged_spots_df):\n","        print(\"Error: Validation failed for loaded spots dataframe.\")\n","\n","def check_unique_id_match(df1, df2):\n","    df1_ids = set(df1['Unique_ID'])\n","    df2_ids = set(df2['Unique_ID'])\n","\n","    # Check if the IDs in the two dataframes match\n","    if df1_ids == df2_ids:\n","        print(\"The Unique_ID values in both dataframes match perfectly!\")\n","    else:\n","        missing_in_df1 = df2_ids - df1_ids\n","        missing_in_df2 = df1_ids - df2_ids\n","\n","        if missing_in_df1:\n","            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n","\n","        if missing_in_df2:\n","            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n","\n","check_unique_id_match(merged_spots_df, merged_tracks_df)\n","\n","check_for_nans(merged_spots_df, \"merged_spots_df\")\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","\n","# Define the metadata columns that are expected to have identical values for each filename\n","metadata_columns = ['Condition', 'experiment_nb', 'Repeat']\n","\n","def check_metadata(df, df_name=\"DataFrame\"):\n","    consistent_metadata = True\n","    for name, group in df.groupby('File_name'):\n","        for col in metadata_columns:\n","            if not group[col].nunique() == 1:\n","                consistent_metadata = False\n","                print(f\"Inconsistency found in {df_name} for file: {name} in column: {col}\")\n","                break  # Stop checking other columns for this group\n","        if not consistent_metadata:\n","            break  # Stop the entire process if any inconsistency is found\n","\n","    if consistent_metadata:\n","        print(f\"{df_name} has consistent metadata.\")\n","    else:\n","        print(f\"{df_name} has inconsistencies in the metadata. Please check the output for details.\")\n","    return\n","\n","check_metadata(merged_tracks_df, \"merged_tracks_df\")\n","check_metadata(merged_spots_df, \"merged_spots_df\")\n"]},{"cell_type":"markdown","metadata":{"id":"2eIekawNM9aS"},"source":["# **Part 2: Compute the distance to the nearest ROI**\n","\n","<font size = 4>\n","This process can be repeated for different types of ROIs, such as 'Nuclei', 'Junctions', etc.\n","Simply change the `ROI_name` parameter to correspond to each specific ROI type you wish to analyze. For instance, set `ROI_name` to 'Nuclei' for analyzing nuclei, and then change it to 'Junctions' for junctions. This flexibility allows you to perform separate analyses for each ROI type using the same notebook, ensuring a thorough and comprehensive examination of tracks in relation to diverse ROI types.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0kXOWRNb3pY8"},"source":["## **2.1. Input Parameters**\n","<font size=\"4\">\n","\n","**ROI Folder Path**: Specify the directory containing your Region of Interest (ROI) images. These images should be masks or label images where the background has a pixel value of 0, and objects of interest are represented by values greater than 0.\n","\n","**ROI Name**: Define the unique name for your region of interest (ROI). This name should align with the naming convention used in your image files. You can perform analyses on multiple ROIs one at a time by adjusting the `ROI_name` parameter for each specific ROI image. Image files should adhere to the following format: `File_name_ROI_name.tif`. For example, if your tracking file is named 'sample,' and the ROI is 'nuclei,' the corresponding image file should be named 'sample_nuclei.tif.'\n","\n","**Pixel Calibration**: Enter the calibration value that converts pixel distances into real-world units (e.g., micrometers). This crucial parameter ensures accurate measurements in physical dimensions.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"J1F43e7R3tcJ"},"outputs":[],"source":["ROI_folder = ''  # @param {type: \"string\"}\n","\n","ROI_name = ''# @param {type: \"string\"}\n","\n","Pixel_calibration = None# @param {type: \"number\"}\n","\n","if not os.path.exists(Results_Folder+\"/Distance_to_ROI\"):\n","    os.makedirs(Results_Folder+\"/Distance_to_ROI\")\n","\n","if not os.path.exists(Results_Folder+\"/Distance_to_ROI/\"+ROI_name):\n","    os.makedirs(Results_Folder+\"/Distance_to_ROI/\"+ROI_name)\n","\n","distance_column_name = f'DistanceTo{ROI_name}'\n","\n","if distance_column_name in merged_spots_df.columns:\n","    print(f\"The column '{distance_column_name}' already exists in the DataFrame.\")\n","    print(\"This indicates that the distance to ROI calculation might have been done previously.\")\n","    print(\"Computing the distance to ROI again may not required unless the data has changed.\")\n","\n","print(f\"Done.\")"]},{"cell_type":"markdown","metadata":{"id":"bWe7J-9uuDgX"},"source":["## **2.2. Check that your dataset matches your image files**\n","\n","1. **Checking and Correcting Coordinates**: This process assesses and corrects the coordinates in the dataset to ensure they fall within the bounds of the associated images or videos.\n","\n","2. **Zero Pixel Percentage**: For each file in the dataset, the code calculates the percentage of background (zero) pixels in the associated Region of Interest (ROI) image or video. This metric helps evaluate the dataset's quality and whether the coordinates need adjustment.\n","\n","3. **Correction Process**: If the coordinates are found outside the image or video bounds, the code corrects them, ensuring they are within acceptable limits. Any corrections made are reported in the console.\n","\n","4. **Supported Data Types**: The code can handle image and video data. It determines the type and dimensions of the data automatically.\n","\n","5. **Pixel Calibration**: The coordinates are adjusted for the specified pixel calibration, ensuring accurate positioning in real-world units.\n","\n","By running this code, you validate and, if necessary, correct the coordinates in your dataset to match the dimensions of your ROI images or videos.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CTgVIKysGL0w"},"outputs":[],"source":["\n","# @title ##Check that your dataset matches your image files\n","\n","from tqdm.notebook import tqdm\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","from skimage import io\n","import matplotlib.pyplot as plt\n","from tifffile import imread\n","from skimage.measure import label, regionprops, find_contours\n","from scipy.ndimage import distance_transform_edt\n","\n","def check_zero_pixel_percentage(ROI_img):\n","    zero_pixel_count = np.sum(ROI_img == 0)\n","    total_pixel_count = ROI_img.size\n","    zero_pixel_percentage = (zero_pixel_count / total_pixel_count) * 100\n","    return zero_pixel_percentage\n","\n","\n","def check_and_correct_coordinates(df, file_name, image_dir, ROI_name):\n","    \"\"\"\n","    Checks and corrects the coordinates in the DataFrame for a given file to ensure they are within the bounds\n","    of the associated image or video.\n","\n","    Parameters:\n","    df (DataFrame): DataFrame containing the spots' data.\n","    file_name (str): The name of the file to check and correct.\n","    image_dir (str): Directory where the images or videos are stored.\n","    ROI_name (str): Suffix or identifier for the image or video file name.\n","\n","    Returns:\n","    DataFrame: Updated DataFrame with corrected coordinates.\n","    \"\"\"\n","    # Load the image or video\n","    ROI_img_path = f\"{image_dir}/{file_name}_{ROI_name}.tif\"\n","    try:\n","        ROI_img = io.imread(ROI_img_path)\n","    except FileNotFoundError:\n","        print(f\"Image or video for {file_name} not found.\")\n","        return df\n","    # Check the percentage of zero pixels\n","    zero_percentage = check_zero_pixel_percentage(ROI_img)\n","    print(f\"File {file_name}: Percentage of background pixels is {zero_percentage:.2f}%.\")\n","\n","    # Determine if it's an image or a video\n","    if ROI_img.ndim == 3:  # Video\n","        max_x, max_y = ROI_img.shape[2] - 1, ROI_img.shape[1] - 1\n","    elif ROI_img.ndim == 2:  # Image\n","        max_x, max_y = ROI_img.shape[1] - 1, ROI_img.shape[0] - 1\n","    else:\n","        print(f\"Unsupported number of dimensions ({ROI_img.ndim}) in the file {file_name}.\")\n","        return df\n","\n","    # Apply pixel calibration\n","    max_x, max_y = max_x * Pixel_calibration, max_y * Pixel_calibration\n","\n","    # Filter dataframe for the current file\n","    file_df = df[df['File_name'] == file_name]\n","\n","    # Correct each coordinate with tqdm for progress\n","    for idx in tqdm(file_df.index, desc=f\"Processing {file_name}\"):\n","        x, y = int(df.at[idx, 'POSITION_X']), int(df.at[idx, 'POSITION_Y'])\n","        corrected_x = max(0, min(x, max_x))\n","        corrected_y = max(0, min(y, max_y))\n","        if corrected_x != x or corrected_y != y:\n","            print(f\"Corrected coordinates for index {idx} from (x={x}, y={y}) to (x={corrected_x}, y={corrected_y})\")\n","        df.at[idx, 'POSITION_X'] = corrected_x\n","        df.at[idx, 'POSITION_Y'] = corrected_y\n","\n","    return df\n","\n","# Apply the function to each file in the DataFrame\n","for file_name in tqdm(merged_spots_df['File_name'].unique(), desc=\"Checking and correcting coordinates\"):\n","    merged_spots_df = check_and_correct_coordinates(merged_spots_df, file_name, ROI_folder, ROI_name)\n"]},{"cell_type":"markdown","metadata":{"id":"52STmnv43d45"},"source":["## **2.3. Visualise your tracks**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AE881uJW5ukQ"},"outputs":[],"source":["# @title ##Run the cell and choose the file you want to inspect\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","import matplotlib.pyplot as plt\n","\n","if not os.path.exists(Results_Folder+\"/Tracks\"):\n","    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = merged_spots_df['File_name'].unique()\n","\n","# Create a Dropdown widget with the filenames\n","filename_dropdown = widgets.Dropdown(\n","    options=filenames,\n","    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n","    description='File Name:',\n",")\n","\n","def plot_coordinates(filename):\n","    if filename:\n","        # Filter the DataFrame based on the selected filename\n","        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n","\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Coordinates for {filename}')\n","        plt.show()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","# Link the Dropdown widget to the plotting function\n","interact(plot_coordinates, filename=filename_dropdown)\n"]},{"cell_type":"markdown","metadata":{"id":"nC_pKRUuyn7A"},"source":["## **2.4. Compute the distance to the nearest ROI**\n","\n","By running this code, you compute and record the distances from each data point to the nearest ROI pixel, facilitating further analysis and interpretation of your dataset.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"QQMWWHTGCcC5"},"outputs":[],"source":["# @title ##Compute the distance to the nearest ROI\n","\n","\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","from skimage import io\n","import matplotlib.pyplot as plt\n","from tifffile import imread\n","from skimage.measure import label, regionprops, find_contours\n","from scipy.ndimage import distance_transform_edt\n","\n","\n","def compute_distances_using_distance_transform(df, image_dir):\n","    \"\"\"\n","    Compute distances to the nearest labeled pixel for each spot using the distance transform method.\n","    Automatically detects if the file is a single image or a video sequence and checks if the frame\n","    number corresponds to the actual number of frames in the video.\n","\n","    Parameters:\n","    df (DataFrame): The dataframe containing the spots' data.\n","    image_dir (str): The directory where the ROI images or videos are stored.\n","    \"\"\"\n","    for file_name in tqdm(df['File_name'].unique(), desc=\"Processing files\"):\n","        # Paths to the label images or video\n","        ROI_img_path = f\"{image_dir}/{file_name}_{ROI_name}.tif\"\n","\n","        try:\n","            ROI_img = io.imread(ROI_img_path)\n","            # Ensure the image dimensions are 3 or below\n","            if ROI_img.ndim > 3:\n","                raise ValueError(f\"Image file {file_name} has more than 3 dimensions, which is not supported.\")\n","\n","            # Determine if the file is a video by checking if it has more than two dimensions\n","            is_video = ROI_img.ndim == 3\n","\n","            # Verify that the 'FRAME' number in the dataframe does not exceed the number of frames in the video\n","            if is_video and 'FRAME' in df.columns:\n","                file_df = df[df['File_name'] == file_name]\n","            # Compute max_frame_num for the current file_name\n","                max_frame_num = file_df['FRAME'].max()\n","                num_frames = ROI_img.shape[0]\n","                if max_frame_num > num_frames:\n","                    print(f\"Error: max_frame_num ({max_frame_num}) exceeds num_frames ({num_frames}) in file {file_name}.\")\n","                    raise ValueError(f\"DataFrame contains 'FRAME' numbers that exceed the number of frames in the video for file {file_name}.\")\n","                for frame_idx in range(num_frames):\n","                    # Process each frame with matching spots\n","                    process_frame(ROI_img[frame_idx], df, file_name, frame_idx)\n","            else:\n","                # Process a single image\n","                process_frame(ROI_img, df, file_name)\n","\n","        except FileNotFoundError:\n","            print(f\"Error: Image for {file_name} not found. Skipping...\")\n","            continue\n","        except ValueError as e:\n","            print(e)\n","            break\n","\n","    return df\n","\n","def process_frame(ROI_img, df, file_name, frame_idx=None):\n","    \"\"\"\n","    Process a single frame or image and update the dataframe with distance values.\n","\n","    Parameters:\n","    ROI_img (ndarray): The ROI image or a single frame from a video.\n","    df (DataFrame): The dataframe to update.\n","    file_name (str): The name of the file being processed.\n","    frame_idx (int, optional): The index of the frame in the video.\n","    \"\"\"\n","    # Compute distance transform\n","    distance_transform_ROI = distance_transform_edt(ROI_img == 0) * Pixel_calibration\n","\n","    # Filter dataframe for the current file and frame\n","    file_df = df[df['File_name'] == file_name]\n","    if frame_idx is not None:\n","        file_df = file_df[file_df['FRAME'] == frame_idx]\n","\n","    for idx, row in tqdm(file_df.iterrows(), total=file_df.shape[0], desc=f\"Processing coordinates for {file_name}\", leave=False):\n","        y, x = int(row['POSITION_Y'] / Pixel_calibration), int(row['POSITION_X'] / Pixel_calibration)\n","                # Check if x and y are within the bounds of the image\n","\n","        if 0 <= x < distance_transform_ROI.shape[1] and 0 <= y < distance_transform_ROI.shape[0]:\n","            df.loc[df.index[idx], f'DistanceTo{ROI_name}'] = distance_transform_ROI[y, x]\n","        else:\n","            print(f\"Warning: Coordinates (x={x}, y={y}) out of bounds for {file_name}\")\n","\n","compute_distances_using_distance_transform(merged_spots_df, ROI_folder)\n","\n","save_dataframe_with_progress(merged_spots_df, Results_Folder + '/' + 'merged_Spots.csv', desc=\"Saving Spots\")"]},{"cell_type":"markdown","metadata":{"id":"6-EKHi8a8RHa"},"source":["## **2.5. Check for missing values**\n","\n","By running this code, you can quickly identify which filenames in your dataset contain data points with missing distance values. This information is valuable for data quality assessment. This likely occurs because ROI images are missing.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"usMBpDDq2nfq"},"outputs":[],"source":["\n","# @title ##Check for NaN\n","\n","\n","def print_filenames_with_nan_distances(spots_df, ROI_name):\n","    nan_filenames = []\n","\n","    grouped_spots = spots_df.groupby('Unique_ID')\n","\n","    for unique_id, group in grouped_spots:\n","        if group[f'DistanceTo{ROI_name}'].isna().any():\n","            # Store filenames associated with NaN distances\n","            nan_filenames.extend(group['File_name'].unique())\n","\n","    # Print unique filenames with NaN distances\n","    unique_nan_filenames = set(nan_filenames)\n","    print(f\"Filenames with NaN distances: {unique_nan_filenames}\")\n","\n","# Usage\n","print_filenames_with_nan_distances(merged_spots_df, ROI_name)\n"]},{"cell_type":"markdown","metadata":{"id":"QmNQ2vSZ8cNF"},"source":["## **2.6. Visual validation**\n","\n","1. **Filename Dropdown**: Select a specific filename from the dropdown list. This corresponds to the tracked data associated with the ROI you want to analyze.\n","\n","2. **Display Option**: Choose between 'All' or 'Specific number' to control the number of points displayed on the plot.\n","\n","3. **Number of Points**: If you select 'Specific number' in the display option, enter the desired number of points to display.\n","\n","4. **Save as PDF**: Check this box if you want to save the generated plot as a PDF file.\n","\n","5. **Visualize Distances Button**: Click this button to visualize the distances measured for the selected ROI and filename.\n","\n","The resulting plot will display the ROI image with distances represented by red circles around the points of interest. Each circle's radius corresponds to the distance measurement in real-world units (e.g., micrometers), taking into account the specified pixel calibration.\n","\n","If the 'Save as PDF' option is checked, the plot will be saved as a PDF file in the designated results folder.\n","\n","Use this interface to visually inspect and validate the correctness of the measured distances for your ROI data.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pZ2CLaOlMWOb"},"outputs":[],"source":["# @title ##Run to check visually that the distances measured are correct.\n","from ipywidgets import Button, interactive, IntSlider, widgets, fixed\n","from ipywidgets import Output\n","from IPython.display import clear_output\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tifffile import imread\n","import matplotlib.backends.backend_pdf\n","\n","\n","error_output = Output()\n","\n","def display_error_message(message):\n","    with error_output:\n","        clear_output(wait=False)\n","        print(f\"Error: {message}\")\n","\n","filename_dropdown = widgets.Dropdown(\n","    options=merged_spots_df['File_name'].unique(),\n","    description='Filename:',\n","    disabled=False\n",")\n","\n","# Additional widgets for user input\n","display_option = widgets.Dropdown(\n","    options=['All', 'Specific number'],\n","    value='All',\n","    description='Display:',\n","    disabled=False\n",")\n","\n","number_of_points = widgets.BoundedIntText(\n","    value=5,\n","    min=1,\n","    max=1,  # Adjust max as per your data range\n","    step=1,\n","    description='Number of points:',\n","    disabled=False\n",")\n","\n","# Checkbox for saving plot as PDF\n","save_as_pdf_checkbox = widgets.Checkbox(\n","    value=False,\n","    description='Save as PDF',\n","    disabled=False\n",")\n","\n","# Display these widgets immediately\n","display(filename_dropdown)\n","\n","def update_display(frame_number, ROI_img, data_for_frame, filename, display_mode, point_count, save_pdf):\n","    plt.figure(figsize=(8, 8))\n","\n","    frame = ROI_img.copy()\n","    if frame.ndim == 3:  # If the image is a video\n","        frame = frame[frame_number]\n","\n","    coords_for_frame = data_for_frame[data_for_frame['POSITION_T'] == frame_number]\n","    if coords_for_frame.empty:\n","        print(f\"No data available for frame {frame_number} in file {filename}.\")\n","    else:\n","        plt.imshow(frame, cmap='gray')\n","\n","        plt.xlim(0, frame.shape[1])\n","        plt.ylim(frame.shape[0], 0)\n","\n","        if display_mode == 'All':\n","            points_to_display = coords_for_frame\n","        else:\n","            points_to_display = coords_for_frame.head(point_count)\n","\n","        for idx, row in points_to_display.iterrows():\n","            x, y = int(row['POSITION_X']/Pixel_calibration), int(row['POSITION_Y']/Pixel_calibration)\n","            distance_to_edge = row[f'DistanceTo{ROI_name}']/Pixel_calibration\n","            circle_edge = plt.Circle((x, y), distance_to_edge, color='red', fill=False, linewidth=1)\n","            plt.gca().add_patch(circle_edge)\n","\n","        plt.scatter(points_to_display['POSITION_X']/Pixel_calibration, points_to_display['POSITION_Y']/Pixel_calibration, c='yellow', s=50, zorder=3)\n","\n","\n","    plt.title(f\"Frame {frame_number} for {filename}\")\n","\n","    if save_pdf:\n","        pdf_filename = f\"{Results_Folder}/Distance_to_ROI/{ROI_name}/{filename}_frame_{frame_number}.pdf\"\n","        plt.savefig(pdf_filename, format='pdf')\n","        print(f\"Plot saved as '{pdf_filename}'\")\n","    plt.show()\n","\n","def visualize_precomputed_distances_for_filename(filename):\n","    ROI_img_path = f\"{ROI_folder}/{filename}_{ROI_name}.tif\"\n","        # Calculate the maximum number of points for the selected file\n","    max_points_for_file = merged_spots_df[merged_spots_df['File_name'] == filename].groupby('FRAME').size().max()\n","\n","    # Update the maximum value for the number_of_points widget\n","    number_of_points.max = max_points_for_file\n","\n","    try:\n","        ROI_img = imread(ROI_img_path)\n","    except FileNotFoundError:\n","        display_error_message(f\"Image for {filename} not found.\")\n","        return\n","\n","    data_for_frame = merged_spots_df[merged_spots_df['File_name'] == filename]\n","\n","    max_frame = data_for_frame['FRAME'].max()\n","    frame_slider = widgets.IntSlider(min=0, max=max_frame, description='Frame')\n","\n","    # Modified call to include new widgets\n","    w = interactive(update_display,\n","                    frame_number=frame_slider,\n","                    ROI_img=fixed(ROI_img),\n","                    data_for_frame=fixed(data_for_frame),\n","                    filename=fixed(filename),\n","                    display_mode=display_option,\n","                    point_count=number_of_points,\n","                    save_pdf=save_as_pdf_checkbox)\n","\n","    display(w)\n","\n","# Button to trigger visualization\n","plot_button_filename = Button(description=\"Visualize Distances\", button_style='info')\n","\n","# Function to handle button click for filename visualization\n","def on_plot_button_filename_click(b):\n","    filename = filename_dropdown.value\n","    print(\"In progress...\")\n","    # Clear the previous output\n","    clear_output()\n","\n","    # Call the visualization function without redisplaying the widgets\n","    visualize_precomputed_distances_for_filename(filename)\n","\n","# Bind the function to the button click event\n","plot_button_filename.on_click(on_plot_button_filename_click)\n","\n","# Display the button and error output\n","display(plot_button_filename)\n","display(error_output)\n"]},{"cell_type":"markdown","metadata":{"id":"63pi0QAu-n3q"},"source":["## **2.5. Compute general track metrics associated with the ROI**\n","\n","\n","1. **MaxDistance_{ROI_name}**:\n","   - The maximum distance of the track from the ROI during the tracking period.\n","   - Indicates the farthest point reached relative to the ROI.\n","\n","2. **MinDistance_{ROI_name}**:\n","   - The minimum distance of the track from the ROI during the tracking period.\n","   - Represents the closest approach to the ROI.\n","\n","3. **StartDistance_{ROI_name}** and **EndDistance_{ROI_name}**:\n","   - Distances from the ROI at the start and end of the tracking period, respectively.\n","   - Useful for understanding initial and final positioning relative to the ROI.\n","\n","4. **MedianDistance_{ROI_name}**:\n","   - The median of all recorded distances to the ROI.\n","   - Provides a central tendency measure, less affected by outliers than the mean.\n","\n","5. **StdDevDistance_{ROI_name}**:\n","   - Standard deviation of the distances.\n","   - Indicates the variability or consistency of the track's distance from the ROI.\n","\n","6. **DirectionMovement_{ROI_name}**:\n","   - Calculated as `EndDistance - StartDistance`.\n","   - A positive value indicates moving away from the ROI over time, and a negative value suggests moving closer.\n","\n","7. **AvgRateChange_{ROI_name}**:\n","   - Average rate of change in distance per frame.\n","   - Helps assess the speed of movement towards or away from the ROI.\n","\n","8. **PercentageChange_{ROI_name}**:\n","   - Percentage change in distance from the start to the end of the track.\n","   - Normalizes the movement relative to the initial distance.\n","\n","9. **TrendSlope_{ROI_name}**:\n","   - Slope of a linear regression line fitted to the distance values over time.\n","   - Indicates the general trend of movement (increasing or decreasing distance).\n","\n","## Interpretation\n","- These metrics are calculated considering the distance to the closest ROI at each time point. If the ROI is moving, the metrics reflect the relative motion between the track and the ROI.\n","- The closest ROI to a track at each time point may change if there are multiple ROIs. This factor is inherently considered in the distance calculations.\n","- It is essential to consider the movement of both the track and the ROI when interpreting these metrics. For example, a decreasing distance over time could mean the track is moving towards the ROI, the ROI is moving towards the track, or both.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XUF-3iPXMdTl"},"outputs":[],"source":["# @title ##Run to compute the metrics.\n","\n","\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","import numpy as np\n","from scipy.stats import linregress\n","\n","def get_distances_and_metrics(track_df, spots_df, ROI_name):\n","    results = []\n","\n","    grouped_spots = spots_df.groupby('Unique_ID')\n","\n","    for _, track in tqdm(track_df.iterrows(), total=track_df.shape[0], desc=\"Processing Tracks\"):\n","        unique_id = track['Unique_ID']\n","\n","        if unique_id in grouped_spots.groups:\n","            track_spots = grouped_spots.get_group(unique_id)\n","            distances = track_spots[f'DistanceTo{ROI_name}']\n","\n","            if distances.empty or distances.isna().all():\n","                max_distance = min_distance = start_distance = end_distance = median_distance = average_rate_of_change = percentage_change = direction_of_movement = np.nan\n","            # Basic metrics\n","\n","            else:\n","              max_distance = distances.max(skipna=True)\n","              min_distance = distances.min(skipna=True)\n","              start_distance = distances.iloc[0] if not distances.empty else np.nan\n","              end_distance = distances.iloc[-1] if not distances.empty else np.nan\n","              median_distance = distances.median(skipna=True)\n","              std_dev_distance = distances.std(skipna=True)\n","\n","              # Advanced metrics\n","              direction_of_movement = end_distance - start_distance\n","              average_rate_of_change = direction_of_movement / len(distances) if len(distances) > 0 else np.nan\n","              percentage_change = (direction_of_movement / start_distance * 100) if start_distance != 0 else np.nan\n","\n","              # Linear regression to determine trend\n","              slope, _, _, _, _ = linregress(range(len(distances)), distances) if not distances.empty else (np.nan,)*5\n","\n","            results.append({\n","                'Unique_ID': unique_id,\n","                f'MaxDistance_{ROI_name}': max_distance,\n","                f'MinDistance_{ROI_name}': min_distance,\n","                f'StartDistance_{ROI_name}': start_distance,\n","                f'EndDistance_{ROI_name}': end_distance,\n","                f'MedianDistance_{ROI_name}': median_distance,\n","                f'StdDevDistance_{ROI_name}': std_dev_distance,\n","                f'DirectionMovement_{ROI_name}': direction_of_movement,\n","                f'AvgRateChange_{ROI_name}': average_rate_of_change,\n","                f'PercentageChange_{ROI_name}': percentage_change,\n","                f'TrendSlope_{ROI_name}': slope\n","            })\n","\n","    return pd.DataFrame(results)\n","\n","# Using the function with your DataFrames\n","distances_metrics_df = get_distances_and_metrics(merged_tracks_df, merged_spots_df, ROI_name)\n","\n","# Merging Process\n","overlapping_columns = merged_tracks_df.columns.intersection(distances_metrics_df.columns).drop('Unique_ID')\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","merged_tracks_df = pd.merge(merged_tracks_df, distances_metrics_df, on='Unique_ID', how='left')\n","\n","# Save the updated DataFrame\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n"]},{"cell_type":"markdown","metadata":{"id":"FJHi3-zp3JTd"},"source":["--------\n","# **Part 3. Quality Control**\n","--------\n","\n","      \n","\n"]},{"cell_type":"markdown","metadata":{"id":"dqu5RWoh8TNc"},"source":["## **3.1. Assess if your dataset is balanced**\n","---\n","\n","In cell tracking and similar biological analyses, the balance of the dataset is important, particularly in ensuring that each biological repeat carries equal weight. Here's why this balance is essential:\n","\n","### Accurate Representation of Biological Variability\n","\n","- **Capturing True Biological Variation**: Biological repeats are crucial for capturing the natural variability inherent in biological systems. Equal weighting ensures that this variability is accurately represented.\n","- **Reducing Sampling Bias**: By balancing the dataset, we avoid overemphasizing the characteristics of any single repeat, which might not be representative of the broader biological context.\n","\n","If your data is too imbalanced, it may be useful to ensure that this does not shift your results.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"dlqqoWZlzoeL"},"outputs":[],"source":["import pandas as pd\n","\n","# @title ##Check the number of track per condition per repeats\n","\n","\n","if not os.path.exists(f\"{Results_Folder}/QC\"):\n","    os.makedirs(f\"{Results_Folder}/QC\")\n","\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import os\n","\n","def count_tracks_by_condition_and_repeat(df, Results_Folder, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID'):\n","    \"\"\"\n","    Counts the number of unique tracks for each combination of condition and repeat in the given DataFrame and\n","    saves a stacked histogram plot as a PDF in the QC folder with annotations for each stack.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    Results_Folder (str): The base folder where the results will be saved.\n","    condition_col (str): The name of the column representing the condition. Default is 'Condition'.\n","    repeat_col (str): The name of the column representing the repeat. Default is 'Repeat'.\n","    track_id_col (str): The name of the column representing the track ID. Default is 'Unique_ID'.\n","    \"\"\"\n","    track_counts = df.groupby([condition_col, repeat_col])[track_id_col].nunique()\n","    track_counts_df = track_counts.reset_index()\n","    track_counts_df.rename(columns={track_id_col: 'Number_of_Tracks'}, inplace=True)\n","\n","    # Pivot the data for plotting\n","    pivot_df = track_counts_df.pivot(index=condition_col, columns=repeat_col, values='Number_of_Tracks').fillna(0)\n","\n","    # Plotting\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel('Number of Tracks')\n","    ax.set_title('Stacked Histogram of Track Counts per Condition and Repeat')\n","    ax.legend(title=repeat_col)\n","    ax.grid(axis='y', linestyle='--')\n","\n","    # Hide horizontal grid lines\n","    ax.yaxis.grid(False)\n","\n","    # Add number annotations on each stack\n","    for bar in bars.patches:\n","        ax.text(bar.get_x() + bar.get_width() / 2,\n","                bar.get_y() + bar.get_height() / 2,\n","                int(bar.get_height()),\n","                ha='center', va='center', color='black', fontweight='bold', fontsize=8)\n","\n","    # Save the plot as a PDF\n","    pdf_file = os.path.join(Results_Folder, 'Track_Counts_Histogram.pdf')\n","    plt.savefig(pdf_file, bbox_inches='tight')\n","    print(f\"Saved histogram to {pdf_file}\")\n","\n","    plt.show()\n","\n","    return track_counts_df\n","\n","result_df = count_tracks_by_condition_and_repeat(merged_tracks_df, f\"{Results_Folder}/QC\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oQbwNuLG737Q"},"source":["## **3.2. Compute Similarity Metrics between Field of Views (FOV) and between Conditions and Repeats**\n","---\n","\n","<font size = 4>**Purpose**:\n","\n","<font size = 4>This section provides a set of tools to compute and visualize similarities between different field of views (FOV) based on selected track parameters. By leveraging hierarchical clustering, the resulting dendrogram offers a clear visualization of how different FOV, conditions, or repeats relate to one another. This tool is essential for:\n","\n","<font size = 4>1. **Quality Control**:\n","    - Ensuring that FOVs from the same condition or experimental setup are more similar to each other than to FOVs from different conditions.\n","    - Confirming that repeats of the same experiment yield consistent results and cluster together.\n","    \n","<font size = 4>2. **Data Integrity**:\n","    - Identifying potential outliers or anomalies in the dataset.\n","    - Assessing the overall consistency of the experiment and ensuring reproducibility.\n","\n","<font size = 4>**How to Use**:\n","\n","<font size = 4>1. **Track Parameters Selection**:\n","    - A list of checkboxes allows users to select which track parameters they want to consider for similarity calculations. By default, all parameters are selected. Users can deselect parameters that they believe might not contribute significantly to the similarity.\n","\n","<font size = 4>2. **Similarity Metric**:\n","    - Users can choose a similarity metric from a dropdown list. Options include cosine, euclidean, cityblock, jaccard, and correlation. The choice of similarity metric can influence the clustering results, so users might need to experiment with different metrics to see which one provides the most meaningful results.\n","\n","<font size = 4>3. **Linkage Method**:\n","    - Determines how the distance between clusters is calculated in the hierarchical clustering process. Different linkage methods can produce different dendrograms, so users might want to try various methods.\n","\n","<font size = 4>4. **Visualization**:\n","    - Once the parameters are selected, users can click on the \"Select the track parameters and visualize similarity\" button. This will compute the hierarchical clustering and display two dendrograms:\n","        - One dendrogram displays similarities between individual FOVs.\n","        - Another dendrogram aggregates the data based on conditions and repeats, providing a higher-level view of the similarities.\n","      \n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bojHIRXv5bnN"},"outputs":[],"source":["# @title ##Compute similarity metrics between FOV and between conditions and repeats\n","\n","import pandas as pd\n","import numpy as np\n","from scipy.spatial.distance import cosine\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import ipywidgets as widgets\n","from sklearn.metrics import pairwise_distances\n","from scipy.spatial.distance import pdist\n","\n","# Check and create \"QC\" folder\n","if not os.path.exists(f\"{Results_Folder}/QC\"):\n","    os.makedirs(f\"{Results_Folder}/QC\")\n","\n","# Columns to exclude\n","excluded_columns = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","\n","selected_df = pd.DataFrame()\n","\n","# Filter out non-numeric columns but keep 'File_name'\n","numeric_df = merged_tracks_df.select_dtypes(include=['float64', 'int64']).copy()\n","numeric_df['File_name'] = merged_tracks_df['File_name']\n","\n","# Create a list of column names excluding 'File_name'\n","column_names = [col for col in numeric_df.columns if col not in excluded_columns]\n","\n","# Create a checkbox for each column\n","checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in column_names]\n","\n","# Dropdown for similarity metrics\n","similarity_dropdown = widgets.Dropdown(\n","    options=['cosine', 'euclidean', 'cityblock', 'jaccard', 'correlation'],\n","    value='cosine',\n","    description='Similarity Metric:'\n",")\n","\n","# Dropdown for linkage methods\n","linkage_dropdown = widgets.Dropdown(\n","    options=['single', 'complete', 'average', 'ward'],\n","    value='single',\n","    description='Linkage Method:'\n",")\n","\n","# Arrange checkboxes in a 2x grid\n","grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n","\n","# Create a button to trigger the selection and visualization\n","button = widgets.Button(description=\"Select the track parameters and visualize similarity\", layout=widgets.Layout(width='400px'), button_style='info')\n","\n","# Define the button click event handler\n","def on_button_click(b):\n","    global selected_df  # Declare selected_df as global\n","\n","    # Get the selected columns from the checkboxes\n","    selected_columns = [box.description for box in checkboxes if box.value]\n","    selected_columns.append('File_name')  # Always include 'File_name'\n","\n","    # Extract the selected columns from the DataFrame\n","    selected_df = numeric_df[selected_columns]\n","\n","    # Check and print the percentage of NaNs for each selected column\n","    for column in selected_columns:\n","        if selected_df[column].isna().any():\n","            nan_percentage = selected_df[column].isna().mean() * 100\n","            print(\"Warning: NaN values found in the selected data.\")\n","            print(f\"{column}: {nan_percentage:.2f}%\")\n","            any_nan = True\n","            print(\"Proceeding to handle NaN values.\")\n","            selected_df = selected_df.dropna()\n","\n","    if not any_nan:\n","        print(\"No NaN values found in the selected columns.\")\n","\n","    # Aggregate the data by filename\n","    aggregated_by_filename = selected_df.groupby('File_name').mean(numeric_only=True)\n","\n","    # Aggregate the data by condition and repeat\n","    aggregated_by_condition_repeat = merged_tracks_df.groupby(['Condition', 'Repeat'])[selected_columns].mean(numeric_only=True)\n","\n","    # Compute condensed distance matrices\n","    distance_matrix_filename = pdist(aggregated_by_filename, metric=similarity_dropdown.value)\n","    distance_matrix_condition_repeat = pdist(aggregated_by_condition_repeat, metric=similarity_dropdown.value)\n","\n","    # Perform hierarchical clustering\n","    linked_filename = linkage(distance_matrix_filename, method=linkage_dropdown.value)\n","    linked_condition_repeat = linkage(distance_matrix_condition_repeat, method=linkage_dropdown.value)\n","\n","    annotation_text = f\"Similarity Method: {similarity_dropdown.value}, Linkage Method: {linkage_dropdown.value}\"\n","\n","        # Prepare the parameters dictionary\n","    similarity_params = {\n","        'Similarity Metric': similarity_dropdown.value,\n","        'Linkage Method': linkage_dropdown.value,\n","        'Selected Columns': ', '.join(selected_columns)\n","    }\n","\n","    # Save the parameters\n","    params_file_path = os.path.join(Results_Folder, \"QC/analysis_parameters.csv\")\n","    save_parameters(similarity_params, params_file_path, 'Similarity Metrics')\n","\n","    # Plot the dendrograms one under the other\n","    plt.figure(figsize=(10, 10))\n","\n","    # Dendrogram for individual filenames\n","    plt.subplot(2, 1, 1)\n","    dendrogram(linked_filename, labels=aggregated_by_filename.index, orientation='top', distance_sort='descending', leaf_rotation=90)\n","    plt.title(f'Dendrogram of Field of view Similarities\\n{annotation_text}')\n","\n","    # Dendrogram for aggregated data based on condition and repeat\n","    plt.subplot(2, 1, 2)\n","    dendrogram(linked_condition_repeat, labels=aggregated_by_condition_repeat.index, orientation='top', distance_sort='descending', leaf_rotation=90)\n","    plt.title(f'Dendrogram of Aggregated Similarities by Condition and Repeat\\n{annotation_text}')\n","\n","    plt.tight_layout()\n","\n","    # Save the dendrogram to a PDF\n","    pdf_pages = PdfPages(f\"{Results_Folder}/QC/Dendrogram_Similarities.pdf\")\n","\n","    # Save the current figure to the PDF\n","    pdf_pages.savefig()\n","\n","    # Close the PdfPages object to finalize the document\n","    pdf_pages.close()\n","\n","    plt.show()\n","\n","# Set the button click event handler\n","button.on_click(on_button_click)\n","\n","# Display the widgets\n","display(grid, similarity_dropdown, linkage_dropdown, button)\n"]},{"cell_type":"markdown","metadata":{"id":"joRI14WVUPuM"},"source":["-------------------------------------------\n","\n","# **Part 4. Plot track parameters**\n","-------------------------------------------\n","\n","<font size = 4> In this section you can plot all the track parameters previously computed. Data and graphs are automatically saved in your result folder.\n","\n","<b>Note on Units:</b> The parameters plotted are in the unit of measurement you used when tracking your data.\n","</font>\n","\n","<font size=\"4\" color=\"red\">\n","<b>Results Storage:</b>\n","Results generated by in this section are saved  in the sub-folder named `track_parameters_plots` within your `Results_Folder`.\n"]},{"cell_type":"markdown","metadata":{"id":"hIa_MlR2Ktu-"},"source":["##**Statistical analyses**\n","### Cohen's d (Effect Size):\n","<font size = 4>Cohen's d measures the size of the difference between two groups, normalized by their pooled standard deviation. Values can be interpreted as small (0 to 0.2), medium (0.2 to 0.5), or large (0.5 and above) effects. It helps quantify how significant the observed difference is, beyond just being statistically significant.\n","\n","### Randomization Test:\n","<font size = 4>This non-parametric test evaluates if observed differences between conditions could have arisen by random chance. It shuffles condition labels multiple times, recalculating the Cohen's d each time. The resulting p-value, which indicates the likelihood of observing the actual difference by chance, provides evidence against the null hypothesis: a smaller p-value implies stronger evidence against the null.\n","\n","### Bonferroni Correction:\n","<font size = 4>Given multiple comparisons, the Bonferroni Correction adjusts significance thresholds to mitigate the risk of false positives. By dividing the standard significance level (alpha) by the number of tests, it ensures that only robust findings are considered significant. However, it's worth noting that this method can be conservative, sometimes overlooking genuine effects."]},{"cell_type":"markdown","metadata":{"id":"RdAeBwtVaRCv"},"source":["## **4.1. Plot your entire dataset**\n","--------"]},{"cell_type":"code","source":["import os\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import pandas as pd\n","from scipy.stats import zscore\n","\n","# @title ##Plot track normalized track parameters based on conditions as an heatmap (entire dataset)\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/track_parameters_plots\"\n","Conditions = 'Condition'\n","df_to_plot = merged_tracks_df\n","\n","# Check and create necessary directories\n","folders = [\"pdf\", \"csv\"]\n","for folder in folders:\n","    dir_path = os.path.join(base_folder, folder)\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","\n","def heatmap_comparison(df, Results_Folder, Conditions, variables_per_page=40):\n","    # Get all the selectable columns\n","    variables_to_plot = get_selectable_columns(df)\n","\n","    # Drop rows where all elements are NaNs in the variables_to_plot columns\n","    df = df.dropna()\n","\n","    # Compute median for each variable across Conditions\n","    median_values = df.groupby(Conditions)[variables_to_plot].median().transpose()\n","\n","    # Normalize the median values using Z-score\n","    normalized_values = median_values.apply(zscore, axis=1)\n","\n","    # Number of pages\n","    total_variables = len(variables_to_plot)\n","    num_pages = int(np.ceil(total_variables / variables_per_page))\n","\n","    # Initialize an empty DataFrame to store all pages' data\n","    all_pages_data = pd.DataFrame()\n","\n","    # Create a PDF file to save the heatmaps\n","    with PdfPages(f\"{Results_Folder}/Heatmaps_Normalized_Median_Values_by_Condition.pdf\") as pdf:\n","        for page in range(num_pages):\n","            start = page * variables_per_page\n","            end = min(start + variables_per_page, total_variables)\n","            page_data = normalized_values.iloc[start:end]\n","\n","            # Append this page's data to the all_pages_data DataFrame\n","            all_pages_data = pd.concat([all_pages_data, page_data])\n","\n","            plt.figure(figsize=(16, 10))\n","            sns.heatmap(page_data, cmap='coolwarm', annot=True, linewidths=.1)\n","            plt.title(f\"Z-score Normalized Median Values of Variables by Condition (Page {page + 1})\")\n","            plt.tight_layout()\n","\n","            pdf.savefig()  # saves the current figure into a pdf page\n","            plt.show()\n","            plt.close()\n","\n","    # Save all pages data to a single CSV file\n","    all_pages_data.to_csv(f\"{Results_Folder}/Normalized_Median_Values_by_Condition.csv\")\n","\n","    print(f\"Heatmaps saved to {Results_Folder}/Heatmaps_Normalized_Median_Values_by_Condition.pdf\")\n","    print(f\"All data saved to {Results_Folder}/Normalized_Median_Values_by_Condition.csv\")\n","\n","# Example usage\n","heatmap_comparison(merged_tracks_df, base_folder, Conditions)\n"],"metadata":{"cellView":"form","id":"9S95OXvF0ODf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"tklU69qJRndj"},"outputs":[],"source":["# @title ##Plot track parameters (entire dataset)\n","\n","import ipywidgets as widgets\n","from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","import pandas as pd\n","import os\n","from matplotlib.backends.backend_pdf import PdfPages\n","from matplotlib.ticker import FixedLocator\n","\n","\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/track_parameters_plots\"\n","Conditions = 'Condition'\n","df_to_plot = merged_tracks_df\n","\n","# Check and create necessary directories\n","folders = [\"pdf\", \"csv\"]\n","for folder in folders:\n","    dir_path = os.path.join(base_folder, folder)\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def create_condition_selector(df, column_name):\n","    conditions = df[column_name].unique()\n","    condition_selector = SelectMultiple(\n","        options=conditions,\n","        description='Conditions:',\n","        disabled=False,\n","        layout=Layout(width='100%')  # Adjusting the layout width\n","    )\n","    return condition_selector\n","\n","def display_condition_selection(df, column_name):\n","    condition_selector = create_condition_selector(df, column_name)\n","\n","    condition_accordion = Accordion(children=[VBox([condition_selector])])\n","    condition_accordion.set_title(0, 'Select Conditions')\n","    display(condition_accordion)\n","    return condition_selector\n","\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Conditions, Results_Folder, condition_selector):\n","\n","    plt.clf()  # Clear the current figure before creating a new plot\n","    print(\"Plotting in progress...\")\n","\n","  # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","  # Get selected conditions\n","    selected_conditions = condition_selector.value\n","    n_selected_conditions = len(selected_conditions)\n","\n","    if n_selected_conditions == 0:\n","        print(\"No conditions selected for plotting\")\n","        return\n","\n","# Use only selected and ordered conditions\n","    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n","\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df[Conditions].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = df[df[Conditions] == cond1][var]\n","        group2 = df[df[Conditions] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = abs(cohen_d(new_group1, new_group2))\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = df[[Conditions, var, 'Repeat', 'File_name' ]]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = df[var].quantile(0.25)\n","      Q3 = df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      tick_labels = ax_box.get_xticklabels()\n","      tick_locations = ax_box.get_xticks()\n","      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n","      ax_box.set_xticklabels(tick_labels, rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","condition_selector = display_condition_selection(df_to_plot, Conditions)\n","selectable_columns = get_selectable_columns(df_to_plot)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, base_folder, condition_selector))\n","display(button)"]},{"cell_type":"markdown","metadata":{"id":"l9PtrXYM0mKY"},"source":["## **4.2. Plot a balanced dataset**\n","--------"]},{"cell_type":"markdown","metadata":{"id":"3S0qiuWGaYv4"},"source":["## **4.2.1. Downsample your dataset to ensure that it is balanced**\n","--------\n","\n","### Downsampling and Balancing Dataset\n","\n","This section of the notebook is dedicated to addressing imbalances in the dataset, which is crucial for ensuring the accuracy and reliability of the analysis. The cell bellow will downsample the dataset to balance the number of tracks across different conditions and repeats. It allows for reproducibility by including a `random_seed` parameter, which is set to 42 by default but can be adjusted as needed.\n","\n","All results from this section will be saved in the Balanced Dataset Directory created in your `Results_Folder`.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IVQAzHo6N8PG"},"outputs":[],"source":["# @title ##Run this cell to downsample and balance your dataset\n","\n","random_seed = 42  # @param {type: \"number\"}\n","\n","if not os.path.exists(f\"{Results_Folder}/Balanced_dataset\"):\n","    os.makedirs(f\"{Results_Folder}/Balanced_dataset\")\n","\n","\n","def count_tracks_by_condition_and_repeat(df, Results_Folder, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID'):\n","    \"\"\"\n","    Counts the number of unique tracks for each combination of condition and repeat in the given DataFrame and\n","    saves a stacked histogram plot as a PDF in the QC folder with annotations for each stack.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    Results_Folder (str): The base folder where the results will be saved.\n","    condition_col (str): The name of the column representing the condition. Default is 'Condition'.\n","    repeat_col (str): The name of the column representing the repeat. Default is 'Repeat'.\n","    track_id_col (str): The name of the column representing the track ID. Default is 'Unique_ID'.\n","    \"\"\"\n","    track_counts = df.groupby([condition_col, repeat_col])[track_id_col].nunique()\n","    track_counts_df = track_counts.reset_index()\n","    track_counts_df.rename(columns={track_id_col: 'Number_of_Tracks'}, inplace=True)\n","\n","    # Pivot the data for plotting\n","    pivot_df = track_counts_df.pivot(index=condition_col, columns=repeat_col, values='Number_of_Tracks').fillna(0)\n","\n","    # Plotting\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel('Number of Tracks')\n","    ax.set_title('Stacked Histogram of Track Counts per Condition and Repeat')\n","    ax.legend(title=repeat_col)\n","    ax.grid(axis='y', linestyle='--')\n","\n","    # Hide horizontal grid lines\n","    ax.yaxis.grid(False)\n","\n","    # Add number annotations on each stack\n","    for bar in bars.patches:\n","        ax.text(bar.get_x() + bar.get_width() / 2,\n","                bar.get_y() + bar.get_height() / 2,\n","                int(bar.get_height()),\n","                ha='center', va='center', color='black', fontweight='bold', fontsize=8)\n","\n","    # Save the plot as a PDF\n","    pdf_file = os.path.join(Results_Folder, 'Track_Counts_Histogram.pdf')\n","    plt.savefig(pdf_file, bbox_inches='tight')\n","    print(f\"Saved histogram to {pdf_file}\")\n","\n","    plt.show()\n","\n","    return track_counts_df\n","\n","def balance_dataset(df, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID', random_seed=None):\n","    \"\"\"\n","    Balances the dataset by downsampling tracks for each condition and repeat combination.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    condition_col (str): The name of the column representing the condition.\n","    repeat_col (str): The name of the column representing the repeat.\n","    track_id_col (str): The name of the column representing the track ID.\n","    random_seed (int, optional): The seed for the random number generator. Default is None.\n","\n","    Returns:\n","    pandas.DataFrame: A new DataFrame with balanced track counts.\n","    \"\"\"\n","    # Group by condition and repeat, and find the minimum track count\n","    min_track_count = df.groupby([condition_col, repeat_col])[track_id_col].nunique().min()\n","\n","    # Function to sample min_track_count tracks from each group\n","    def sample_tracks(group):\n","        return group.sample(n=min_track_count, random_state=random_seed)\n","\n","    # Apply sampling to each group and concatenate the results\n","    balanced_merged_tracks_df = df.groupby([condition_col, repeat_col]).apply(sample_tracks).reset_index(drop=True)\n","\n","    return balanced_merged_tracks_df\n","\n","balanced_merged_tracks_df = balance_dataset(merged_tracks_df, random_seed=random_seed)\n","result_df = count_tracks_by_condition_and_repeat(balanced_merged_tracks_df, f\"{Results_Folder}/Balanced_dataset\")\n","\n","check_for_nans(balanced_merged_tracks_df, \"balanced_merged_tracks_df\")\n","save_dataframe_with_progress(balanced_merged_tracks_df, Results_Folder + '/Balanced_dataset/merged_Tracks_balanced_dataset.csv')\n"]},{"cell_type":"markdown","metadata":{"id":"tzAsrJURz4E6"},"source":["## **4.2.2. Check if the downsampling has affected data distribution**\n","--------\n","\n","This section of the notebook generates a heatmap visualizing the Kolmogorov-Smirnov (KS) p-values for each numerical column in the dataset, comparing the distributions before and after downsampling. This heatmap serves as a tool for assessing the impact of downsampling on data quality, guiding decisions on whether the downsampled dataset is suitable for further analysis.\n","\n","#### Purpose of the Heatmap\n","- **KS Test:** The KS test is used to determine if two samples are drawn from the same distribution. In this context, it compares the distribution of each numerical column in the original dataset (`merged_tracks_df`) with its counterpart in the downsampled dataset (`balanced_merged_tracks_df`).\n","- **P-Value Interpretation:** The p-value indicates the probability that the two samples come from the same distribution. A higher p-value suggests a greater likelihood that the distributions are similar.\n","\n","#### Interpreting the Heatmap\n","- **Color Coding:** The heatmap uses a color gradient (from viridis) to represent the range of p-values. Darker colors indicate higher p-values.\n","- **P-Value Thresholds:**\n","  - **High P-Values (Lighter Areas):** Indicate that the downsampling process likely did not significantly alter the distribution of that numerical column for the specific condition-repeat group.\n","  - **Low P-Values (Darker Areas):** Suggest that the downsampling process may have affected the distribution significantly.\n","- **Varying P-Values:** Variations in color across different columns and rows help identify which specific numerical columns and condition-repeat groups are most affected by the downsampling.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"LUGDFw62QCbd"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy.stats import ks_2samp\n","\n","# @title ##Check if your downsampling has affected your data distribution\n","\n","def calculate_ks_p_value(df1, df2, column):\n","    \"\"\"\n","    Calculate the KS p-value for a given column between two dataframes.\n","\n","    Parameters:\n","    df1 (pandas.DataFrame): Original DataFrame.\n","    df2 (pandas.DataFrame): DataFrame after downsampling.\n","    column (str): Column name to compare.\n","\n","    Returns:\n","    float: KS p-value.\n","    \"\"\"\n","    return ks_2samp(df1[column].dropna(), df2[column].dropna())[1]\n","\n","# Identify numerical columns\n","numerical_columns = merged_tracks_df.select_dtypes(include=['int64', 'float64']).columns\n","\n","# Initialize a DataFrame to store KS p-values\n","ks_p_values = pd.DataFrame(columns=numerical_columns)\n","\n","# Iterate over each group and numerical column\n","for group, group_df in merged_tracks_df.groupby(['Condition', 'Repeat']):\n","    group_p_values = []\n","    balanced_group_df = balanced_merged_tracks_df[(balanced_merged_tracks_df['Condition'] == group[0]) & (balanced_merged_tracks_df['Repeat'] == group[1])]\n","    for column in numerical_columns:\n","        p_value = calculate_ks_p_value(group_df, balanced_group_df, column)\n","        group_p_values.append(p_value)\n","    ks_p_values.loc[f'Condition: {group[0]}, Repeat: {group[1]}'] = group_p_values\n","\n","# Maximum number of columns per heatmap\n","max_columns_per_heatmap = 20\n","\n","# Total number of columns\n","total_columns = len(ks_p_values.columns)\n","\n","# Calculate the number of heatmaps needed\n","num_heatmaps = -(-total_columns // max_columns_per_heatmap)  # Ceiling division\n","\n","# File path for the PDF\n","pdf_filepath = Results_Folder+'/Balanced_dataset/p-Value Heatmap.pdf'\n","\n","# Create a PDF file\n","with PdfPages(pdf_filepath) as pdf:\n","    # Loop through each subset of columns and create a heatmap\n","    for i in range(num_heatmaps):\n","        start_col = i * max_columns_per_heatmap\n","        end_col = min(start_col + max_columns_per_heatmap, total_columns)\n","\n","        # Subset of columns for this heatmap\n","        subset_columns = ks_p_values.columns[start_col:end_col]\n","\n","        # Create the heatmap for the subset of columns\n","        plt.figure(figsize=(12, 8))\n","        sns.heatmap(ks_p_values[subset_columns], cmap='viridis', vmax=0.5, vmin=0)\n","        plt.title(f'Kolmogorov-Smirnov P-Value Heatmap (Columns {start_col+1} to {end_col})')\n","        plt.xlabel('Numerical Columns')\n","        plt.ylabel('Condition-Repeat Groups')\n","        plt.tight_layout()\n","\n","        # Save the current figure to the PDF\n","        pdf.savefig()\n","        plt.show()\n","        plt.close()\n","\n","print(f\"Saved all heatmaps to {pdf_filepath}\")\n","\n","# Save the p-values to a CSV file\n","ks_p_values.to_csv(Results_Folder + '/Balanced_dataset/ks_p_values.csv')\n","print(\"Saved KS p-values to ks_p_values.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"kgoO61WY06ZK"},"source":["## **4.2.3. Plot your balanced dataset**\n","--------"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1olzHCkHWJRy"},"outputs":[],"source":["# @title ##Plot track parameters (balanced dataset)\n","\n","import ipywidgets as widgets\n","from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","import pandas as pd\n","import os\n","from matplotlib.ticker import FixedLocator\n","\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/Balanced_dataset/track_parameters_plots\"\n","Conditions = 'Condition'\n","df_to_plot = balanced_merged_tracks_df\n","\n","# Check and create necessary directories\n","folders = [\"pdf\", \"csv\"]\n","for folder in folders:\n","    dir_path = os.path.join(base_folder, folder)\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def create_condition_selector(df, column_name):\n","    conditions = df[column_name].unique()\n","    condition_selector = SelectMultiple(\n","        options=conditions,\n","        description='Conditions:',\n","        disabled=False,\n","        layout=Layout(width='100%')  # Adjusting the layout width\n","    )\n","    return condition_selector\n","\n","def display_condition_selection(df, column_name):\n","    condition_selector = create_condition_selector(df, column_name)\n","\n","    condition_accordion = Accordion(children=[VBox([condition_selector])])\n","    condition_accordion.set_title(0, 'Select Conditions')\n","    display(condition_accordion)\n","    return condition_selector\n","\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Conditions, Results_Folder, condition_selector):\n","\n","    plt.clf()  # Clear the current figure before creating a new plot\n","    print(\"Plotting in progress...\")\n","\n","  # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","  # Get selected conditions\n","    selected_conditions = condition_selector.value\n","    n_selected_conditions = len(selected_conditions)\n","\n","    if n_selected_conditions == 0:\n","        print(\"No conditions selected for plotting\")\n","        return\n","\n","# Use only selected and ordered conditions\n","    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n","\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df[Conditions].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = df[df[Conditions] == cond1][var]\n","        group2 = df[df[Conditions] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = abs(cohen_d(new_group1, new_group2))\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = df[[Conditions, var, 'Repeat', 'File_name' ]]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = df[var].quantile(0.25)\n","      Q3 = df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      tick_labels = ax_box.get_xticklabels()\n","      tick_locations = ax_box.get_xticks()\n","      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n","      ax_box.set_xticklabels(tick_labels, rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","condition_selector = display_condition_selection(df_to_plot, Conditions)\n","selectable_columns = get_selectable_columns(df_to_plot)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, base_folder, condition_selector))\n","display(button)"]},{"cell_type":"markdown","metadata":{"id":"8axJUzmpHSK_"},"source":["# **Part 5. Classify your tracks by distance and plot your dataset**\n","--------\n","<font size = 4>\n","<b>Note on Units:</b> The parameters plotted are in the unit of measurement you used when tracking your data.\n","</font>\n","\n","\n","<font size=\"4\" color=\"red\">\n","<b>Results Storage:</b> The results generated in this section are saved in the sub-folder named `Distance_to_ROI` located within your `Results_Folder`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IgkycikN1FZb"},"outputs":[],"source":["# @title ##Check the distribution of your dataset\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","ROI_name = 'edge'  #@param {type:\"string\"}\n","\n","def plot_tracks_vs_distance(dataframe, ROI_name):\n","    sns.set(style=\"whitegrid\")\n","    plt.figure(figsize=(20, 6))\n","\n","    # Creating a histogram with a bin size of 10\n","    ax = sns.histplot(data=dataframe, x=f'MaxDistance_{ROI_name}', bins=range(0, int(dataframe[f'MaxDistance_{ROI_name}'].max()) + 10, 10), kde=False)\n","\n","    plt.title(f'Number of Tracks vs Max Distance to {ROI_name}')\n","    plt.xlabel(f'Distance to {ROI_name}')\n","    plt.ylabel('Number of Tracks')\n","\n","    # Set x-ticks and rotate the labels for better readability\n","    plt.xticks(range(0, int(dataframe[f'MaxDistance_{ROI_name}'].max()) + 10, 10), rotation=90, ha='right')\n","\n","    plt.tight_layout()  # Adjust the layout to accommodate label sizes\n","    plt.show()\n","\n","# Example usage\n","plot_tracks_vs_distance(merged_tracks_df, ROI_name)  # Replace 'ROI_name' with the actual ROI name\n"]},{"cell_type":"markdown","metadata":{"id":"gau0_u24hRSt"},"source":["## **5.1. Classify your tracks by distance**\n","--------\n","This section classify tracks based on distance from a specified Region of Interest (ROI).\n","\n","## Input Parameters\n","1. `distance_threshold` (type: number): The distance in units (as per your data) that defines the threshold for classification. Tracks within this distance (based on the Max distance computed previously) from the ROI are classified as 'Close,' and those beyond it as 'Far.'\n","2. `ROI_name` (type: string): The name of the Region of Interest. This is used to label the classification columns in the DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ZEJsk7bG1uCY"},"outputs":[],"source":["# Imports\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","import numpy as np\n","\n","# Function to classify tracks by distance\n","def classify_tracks_by_distance(dataframe, distance_threshold, ROI_name):\n","    classification_column = f'Track_Classification_{ROI_name}'\n","    dataframe[classification_column] = dataframe.apply(\n","        lambda row: f'Close_{ROI_name}' if row[f'MaxDistance_{ROI_name}'] <= distance_threshold else f'Far_{ROI_name}', axis=1)\n","    dataframe[f'Track_Classification_Condition_{ROI_name}'] = dataframe['Condition']+'_'+dataframe[classification_column]\n","    return dataframe\n","\n","# Google Colab form fields\n","distance_threshold = 75  #@param {type:\"number\"}\n","ROI_name = 'edge'  #@param {type:\"string\"}\n","\n","# Classifying tracks\n","classified_tracks_df = classify_tracks_by_distance(merged_tracks_df, distance_threshold, ROI_name)\n","\n","# Displaying and saving results\n","close_tracks_count = len(classified_tracks_df[classified_tracks_df[f'Track_Classification_{ROI_name}'] == f'Close_{ROI_name}'])\n","far_tracks_count = len(classified_tracks_df) - close_tracks_count\n","\n","print(f\"Classification of tracks based on distance to {ROI_name}:\")\n","print(f\"Close to {ROI_name}: {close_tracks_count}\")\n","print(f\"Far from {ROI_name}: {far_tracks_count}\")\n","\n","# Save the updated DataFrame\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')"]},{"cell_type":"markdown","metadata":{"id":"uoVXsGpXhBTJ"},"source":["## **5.2. Plot your entire dataset**\n","--------"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CMb6xnNOWduq"},"outputs":[],"source":["# @title ##Plot track parameters (entire dataset)\n","\n","import ipywidgets as widgets\n","from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","import pandas as pd\n","import os\n","from matplotlib.ticker import FixedLocator\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/Distance_to_ROI/{ROI_name}/Classified_track_parameters_plots\"\n","Conditions = f'Track_Classification_Condition_{ROI_name}'\n","df_to_plot = merged_tracks_df\n","\n","# Check and create necessary directories\n","folders = [\"pdf\", \"csv\"]\n","for folder in folders:\n","    dir_path = os.path.join(base_folder, folder)\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def create_condition_selector(df, column_name):\n","    conditions = df[column_name].unique()\n","    condition_selector = SelectMultiple(\n","        options=conditions,\n","        description='Conditions:',\n","        disabled=False,\n","        layout=Layout(width='100%')  # Adjusting the layout width\n","    )\n","    return condition_selector\n","\n","def display_condition_selection(df, column_name):\n","    condition_selector = create_condition_selector(df, column_name)\n","\n","    condition_accordion = Accordion(children=[VBox([condition_selector])])\n","    condition_accordion.set_title(0, 'Select Conditions')\n","    display(condition_accordion)\n","    return condition_selector\n","\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Conditions, Results_Folder, condition_selector):\n","\n","    plt.clf()  # Clear the current figure before creating a new plot\n","    print(\"Plotting in progress...\")\n","\n","  # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","  # Get selected conditions\n","    selected_conditions = condition_selector.value\n","    n_selected_conditions = len(selected_conditions)\n","\n","    if n_selected_conditions == 0:\n","        print(\"No conditions selected for plotting\")\n","        return\n","\n","# Use only selected and ordered conditions\n","    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n","\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df[Conditions].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = df[df[Conditions] == cond1][var]\n","        group2 = df[df[Conditions] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = abs(cohen_d(new_group1, new_group2))\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = df[[Conditions, var, 'Repeat', 'File_name' ]]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = df[var].quantile(0.25)\n","      Q3 = df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      tick_labels = ax_box.get_xticklabels()\n","      tick_locations = ax_box.get_xticks()\n","      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n","      ax_box.set_xticklabels(tick_labels, rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","condition_selector = display_condition_selection(df_to_plot, Conditions)\n","selectable_columns = get_selectable_columns(df_to_plot)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, base_folder, condition_selector))\n","display(button)"]},{"cell_type":"markdown","metadata":{"id":"sTDzk60pmFtq"},"source":["## **5.3. Plot a balanced dataset**\n","--------"]},{"cell_type":"markdown","metadata":{"id":"2NxW2WUQmFt1"},"source":["## **5.3.1. Downsample your dataset to ensure that it is balanced**\n","--------\n","\n","### Downsampling and Balancing Dataset\n","\n","This section of the notebook is dedicated to addressing imbalances in the dataset, which is crucial for ensuring the accuracy and reliability of the analysis. The cell bellow will downsample the dataset to balance the number of tracks across different conditions and repeats. It allows for reproducibility by including a `random_seed` parameter, which is set to 42 by default but can be adjusted as needed.\n","\n","\n","<font size=\"4\" color=\"red\">\n","<b>Results Storage:</b> The results generated in this section are saved in the sub-folder named `Distance_to_ROI` located within your `Results_Folder`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-l-sX6vxmFt1"},"outputs":[],"source":["# @title ##Run this cell to downsample and balance your dataset\n","\n","random_seed = 42  # @param {type: \"number\"}\n","\n","if not os.path.exists(f\"{Results_Folder}/Distance_to_ROI/{ROI_name}/Classified_Balanced_dataset\"):\n","    os.makedirs(f\"{Results_Folder}/Distance_to_ROI/{ROI_name}/Classified_Balanced_dataset\")\n","\n","\n","def count_tracks_by_condition_and_repeat(df, Results_Folder, condition_col=f'Track_Classification_Condition_{ROI_name}', repeat_col='Repeat', track_id_col='Unique_ID'):\n","    \"\"\"\n","    Counts the number of unique tracks for each combination of condition and repeat in the given DataFrame and\n","    saves a stacked histogram plot as a PDF in the QC folder with annotations for each stack.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    Results_Folder (str): The base folder where the results will be saved.\n","    condition_col (str): The name of the column representing the condition. Default is 'Condition'.\n","    repeat_col (str): The name of the column representing the repeat. Default is 'Repeat'.\n","    track_id_col (str): The name of the column representing the track ID. Default is 'Unique_ID'.\n","    \"\"\"\n","    track_counts = df.groupby([condition_col, repeat_col])[track_id_col].nunique()\n","    track_counts_df = track_counts.reset_index()\n","    track_counts_df.rename(columns={track_id_col: 'Number_of_Tracks'}, inplace=True)\n","\n","    # Pivot the data for plotting\n","    pivot_df = track_counts_df.pivot(index=condition_col, columns=repeat_col, values='Number_of_Tracks').fillna(0)\n","\n","    # Plotting\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel('Number of Tracks')\n","    ax.set_title('Stacked Histogram of Track Counts per Condition and Repeat')\n","    ax.legend(title=repeat_col)\n","    ax.grid(axis='y', linestyle='--')\n","\n","    # Hide horizontal grid lines\n","    ax.yaxis.grid(False)\n","\n","    # Add number annotations on each stack\n","    for bar in bars.patches:\n","        ax.text(bar.get_x() + bar.get_width() / 2,\n","                bar.get_y() + bar.get_height() / 2,\n","                int(bar.get_height()),\n","                ha='center', va='center', color='black', fontweight='bold', fontsize=8)\n","\n","    # Save the plot as a PDF\n","    pdf_file = os.path.join(Results_Folder, 'Track_Counts_Histogram.pdf')\n","    plt.savefig(pdf_file, bbox_inches='tight')\n","    print(f\"Saved histogram to {pdf_file}\")\n","\n","    plt.show()\n","\n","    return track_counts_df\n","\n","\n","def balance_dataset(df, condition_col=f'Track_Classification_Condition_{ROI_name}', repeat_col='Repeat', track_id_col='Unique_ID', random_seed=None):\n","    \"\"\"\n","    Balances the dataset by downsampling tracks for each condition and repeat combination.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    condition_col (str): The name of the column representing the condition.\n","    repeat_col (str): The name of the column representing the repeat.\n","    track_id_col (str): The name of the column representing the track ID.\n","    random_seed (int, optional): The seed for the random number generator. Default is None.\n","\n","    Returns:\n","    pandas.DataFrame: A new DataFrame with balanced track counts.\n","    \"\"\"\n","    # Group by condition and repeat, and find the minimum track count\n","    min_track_count = df.groupby([condition_col, repeat_col])[track_id_col].nunique().min()\n","\n","    # Function to sample min_track_count tracks from each group\n","    def sample_tracks(group):\n","        return group.sample(n=min_track_count, random_state=random_seed)\n","\n","    # Apply sampling to each group and concatenate the results\n","    balanced_merged_tracks_df = df.groupby([condition_col, repeat_col]).apply(sample_tracks).reset_index(drop=True)\n","\n","    return balanced_merged_tracks_df\n","\n","balanced_merged_tracks_df = balance_dataset(merged_tracks_df, random_seed=random_seed)\n","result_df = count_tracks_by_condition_and_repeat(balanced_merged_tracks_df, f\"{Results_Folder}/Distance_to_ROI/{ROI_name}/Classified_Balanced_dataset\")\n","\n","check_for_nans(balanced_merged_tracks_df, \"balanced_merged_tracks_df\")\n","save_dataframe_with_progress(balanced_merged_tracks_df,f\"{Results_Folder}/Distance_to_ROI/{ROI_name}/Classified_Balanced_dataset/merged_Tracks_balanced_dataset.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"k7CPB7fnmFt1"},"source":["## **5.3.2. Check if the downsampling has affected data distribution**\n","--------\n","\n","This section of the notebook generates a heatmap visualizing the Kolmogorov-Smirnov (KS) p-values for each numerical column in the dataset, comparing the distributions before and after downsampling. This heatmap serves as a tool for assessing the impact of downsampling on data quality, guiding decisions on whether the downsampled dataset is suitable for further analysis.\n","\n","#### Purpose of the Heatmap\n","- **KS Test:** The KS test is used to determine if two samples are drawn from the same distribution. In this context, it compares the distribution of each numerical column in the original dataset (`merged_tracks_df`) with its counterpart in the downsampled dataset (`balanced_merged_tracks_df`).\n","- **P-Value Interpretation:** The p-value indicates the probability that the two samples come from the same distribution. A higher p-value suggests a greater likelihood that the distributions are similar.\n","\n","#### Interpreting the Heatmap\n","- **Color Coding:** The heatmap uses a color gradient (from viridis) to represent the range of p-values. Darker colors indicate higher p-values.\n","- **P-Value Thresholds:**\n","  - **High P-Values (Lighter Areas):** Indicate that the downsampling process likely did not significantly alter the distribution of that numerical column for the specific condition-repeat group.\n","  - **Low P-Values (Darker Areas):** Suggest that the downsampling process may have affected the distribution significantly.\n","- **Varying P-Values:** Variations in color across different columns and rows help identify which specific numerical columns and condition-repeat groups are most affected by the downsampling.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"UP0dkjLJmFt2"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy.stats import ks_2samp\n","\n","# @title ##Check if your downsampling has affected your data distribution\n","\n","def calculate_ks_p_value(df1, df2, column):\n","    \"\"\"\n","    Calculate the KS p-value for a given column between two dataframes.\n","\n","    Parameters:\n","    df1 (pandas.DataFrame): Original DataFrame.\n","    df2 (pandas.DataFrame): DataFrame after downsampling.\n","    column (str): Column name to compare.\n","\n","    Returns:\n","    float: KS p-value.\n","    \"\"\"\n","    return ks_2samp(df1[column].dropna(), df2[column].dropna())[1]\n","\n","# Identify numerical columns\n","numerical_columns = merged_tracks_df.select_dtypes(include=['int64', 'float64']).columns\n","\n","# Initialize a DataFrame to store KS p-values\n","ks_p_values = pd.DataFrame(columns=numerical_columns)\n","\n","# Iterate over each group and numerical column\n","for group, group_df in merged_tracks_df.groupby([f'Track_Classification_Condition_{ROI_name}', 'Repeat']):\n","    group_p_values = []\n","    balanced_group_df = balanced_merged_tracks_df[(balanced_merged_tracks_df[f'Track_Classification_Condition_{ROI_name}'] == group[0]) & (balanced_merged_tracks_df['Repeat'] == group[1])]\n","    for column in numerical_columns:\n","        p_value = calculate_ks_p_value(group_df, balanced_group_df, column)\n","        group_p_values.append(p_value)\n","    ks_p_values.loc[f'{group[0]}, R: {group[1]}'] = group_p_values\n","\n","# Maximum number of columns per heatmap\n","max_columns_per_heatmap = 20\n","\n","# Total number of columns\n","total_columns = len(ks_p_values.columns)\n","\n","# Calculate the number of heatmaps needed\n","num_heatmaps = -(-total_columns // max_columns_per_heatmap)  # Ceiling division\n","\n","# File path for the PDF\n","pdf_filepath = f\"{Results_Folder}/Distance_to_ROI/{ROI_name}/Classified_Balanced_dataset/KS_p_values_heatmaps.pdf\"\n","\n","# Create a PDF file\n","with PdfPages(pdf_filepath) as pdf:\n","    # Loop through each subset of columns and create a heatmap\n","    for i in range(num_heatmaps):\n","        start_col = i * max_columns_per_heatmap\n","        end_col = min(start_col + max_columns_per_heatmap, total_columns)\n","\n","        # Subset of columns for this heatmap\n","        subset_columns = ks_p_values.columns[start_col:end_col]\n","\n","        # Create the heatmap for the subset of columns\n","        plt.figure(figsize=(12, 8))\n","        sns.heatmap(ks_p_values[subset_columns], cmap='viridis', vmax=0.5, vmin=0)\n","        plt.title(f'Kolmogorov-Smirnov P-Value Heatmap (Columns {start_col+1} to {end_col})')\n","        plt.xlabel('Numerical Columns')\n","        plt.ylabel('Condition-Repeat Groups')\n","        plt.tight_layout()\n","\n","        # Save the current figure to the PDF\n","        pdf.savefig()\n","        plt.show()\n","        plt.close()\n","\n","print(f\"Saved all heatmaps to {pdf_filepath}\")\n","\n","# Save the p-values to a CSV file\n","ks_p_values.to_csv(f\"{Results_Folder}/Distance_to_ROI/{ROI_name}/Classified_Balanced_dataset/ks_p_values.csv\")\n","print(\"Saved KS p-values to ks_p_values.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"3LsBKWnkmFt2"},"source":["## **5.3.3. Plot your balanced dataset**\n","--------"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9CLEe0JAW5QD"},"outputs":[],"source":["# @title ##Plot track parameters (balanced dataset)\n","\n","import ipywidgets as widgets\n","from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","import pandas as pd\n","import os\n","from matplotlib.ticker import FixedLocator\n","\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/Distance_to_ROI/{ROI_name}/Classified_Balanced_dataset/Classified_track_parameters_plots\"\n","Conditions = f'Track_Classification_Condition_{ROI_name}'\n","df_to_plot = balanced_merged_tracks_df\n","\n","# Check and create necessary directories\n","folders = [\"pdf\", \"csv\"]\n","for folder in folders:\n","    dir_path = os.path.join(base_folder, folder)\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def create_condition_selector(df, column_name):\n","    conditions = df[column_name].unique()\n","    condition_selector = SelectMultiple(\n","        options=conditions,\n","        description='Conditions:',\n","        disabled=False,\n","        layout=Layout(width='100%')  # Adjusting the layout width\n","    )\n","    return condition_selector\n","\n","def display_condition_selection(df, column_name):\n","    condition_selector = create_condition_selector(df, column_name)\n","\n","    condition_accordion = Accordion(children=[VBox([condition_selector])])\n","    condition_accordion.set_title(0, 'Select Conditions')\n","    display(condition_accordion)\n","    return condition_selector\n","\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Conditions, Results_Folder, condition_selector):\n","\n","    plt.clf()  # Clear the current figure before creating a new plot\n","    print(\"Plotting in progress...\")\n","\n","  # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","  # Get selected conditions\n","    selected_conditions = condition_selector.value\n","    n_selected_conditions = len(selected_conditions)\n","\n","    if n_selected_conditions == 0:\n","        print(\"No conditions selected for plotting\")\n","        return\n","\n","# Use only selected and ordered conditions\n","    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n","\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df[Conditions].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = df[df[Conditions] == cond1][var]\n","        group2 = df[df[Conditions] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = abs(cohen_d(new_group1, new_group2))\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = df[[Conditions, var, 'Repeat', 'File_name' ]]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = df[var].quantile(0.25)\n","      Q3 = df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      tick_labels = ax_box.get_xticklabels()\n","      tick_locations = ax_box.get_xticks()\n","      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n","      ax_box.set_xticklabels(tick_labels, rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","condition_selector = display_condition_selection(df_to_plot, Conditions)\n","selectable_columns = get_selectable_columns(df_to_plot)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, base_folder, condition_selector))\n","display(button)"]},{"cell_type":"markdown","metadata":{"id":"IMfsr-DFsVN4"},"source":["# **Part 6: Version log**\n","---\n","<font size = 4>While I strive to provide accurate and helpful information, please be aware that:\n","  - This notebook may contain bugs.\n","  - Features are currently limited and will be expanded in future releases.\n","\n","<font size = 4>We encourage users to report any issues or suggestions for improvement. Please check the [repository](https://github.com/guijacquemet/CellTracksColab) regularly for updates and the latest version of this notebook.\n","\n","\n","<font size = 4>**Version 0.9.1**\n","  - Added the PIP freeze option to save a requirement text\n","  - Added the heatmap visualisation of track parameters\n","  - Heatmaps can now be displayed on multiple pages\n","  - Fix userwarning message during plotting (all box plots)\n","\n","<font size = 4>**Version 0.9**\n","  - Improved plotting strategy. Specific conditions can be chosen\n","  - absolute cohen d values are now shown\n","  - In the QC the heatmap is automatically divided in subplot when too many columns are in the df\n","\n","<font size = 4>**Version 0.8**\n","  - Settings are now saved\n","  - Order of the section has been modified to help streamline biological discoveries\n","  - New section added to quality Control to check if the dataset is balanced\n","\n","<font size = 4>**Version 0.7**\n","  - First release of this notebook\n","\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":["2eIekawNM9aS","FJHi3-zp3JTd","joRI14WVUPuM","8axJUzmpHSK_"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}